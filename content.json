{"pages":[{"title":"About this blog","text":"Japanese Ver. Data Science를 좋아하는 학생입니다.특히나, Causal Inference분야에 관심이 많습니다.R, python을 사용하고 있습니다.다양한 사람들과 소통하며 공부하기를 희망하고 있습니다. ・블로그 운영의 목적 스스로의 복습용 메모 주로 머신러닝의 응용처로써의 통계적 인과추론(Causal Inference)에 대해 학습한 내용을 정리합니다. 함께 공부하는 사람에 대한 내용 공유 한국과 일본의 학습자들과 공유하기 위해 모든 블로그 포스트는 한국어 버젼과 일본어 버젼 두가지로 작성하고 있습니다. 부족한 부분이 많습니다. 오타나 잘못된 내용을 발견하셨다면 댓글로 알려주세요.","link":"/About-this-blog/index.html"},{"title":"About this blog","text":"Korean Ver. Data Scienceが大好きな学生です。特に、Causal Inferenceの分野に興味を持っています。R, pythonを使っています。色んな方とコミュニケーションを取りながら学習することを願っています。 ・ブログの目的 自分自身の復習用のメモ 主に機械学習の応用先としての統計的因果推論(Causal Inference)について勉強した内容をまとめます。 他の学習者への内容共有 日本と韓国の学習者と共有するために全てのポストは、日本語バージョンと韓国語バージョンの二通りで作成しています。 まだまだ足りないところが沢山です。 誤字及び間違った内容が見つかりましたら、いつでもコメントください。","link":"/About-this-blog-jap/index.html"}],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/11/13/hello-world/"},{"title":"Bayesian Network","text":"도대체 베이지안 네트워크가 뭐야? 베이지안 네트워크(BN; Bayesian Network) 란? 확률 변수(RV; Random variables)들 사이의 조건부 독립 등의 관계를 보임으로써, RV의 full joint distribution등을 간결하게 표현할 수 있는 그래프 표기법 (Graphical Notation) 이다. &lt; !– more –&gt; 여기서 그래프(Graph) 란, 수학에서 차트(Chart)와 대조되어 정의된 node와 edge의 집합 edge가 방향이 지정되어 있으면 directed, 그렇지 않으면 undirected 그래프의 모든 edge가 directed일 때 directed graph directed edge에서, 시작되는 쪽의 노드를 parent node 라고 하고 반대쪽은 child node라고 한다 복수의 연결된 directed edge의 방향이 같은 경우 이를 directed path라고 하고, directed path의 첫 번째 노드는 경로상의 모든 노드들의 ancestor node이고, 반대로 나머지 노드들은 첫번째 노드의 descendant node이다. directed path의 시작점과 끝점이 일치할 경우 이를 cyclic이라 하고, 그렇지 않은 경우 acyclic라고 한다. 베이지안 네트워크(BN) 의 Syntax Network 는 Node와 이들을 연결시키는 Edge로 구성된다 방향성 비순환 그래프(DAG; Directed Acyclic Graph) 가 되어야 한다 개별 Node들은 RV인 $X$에 대해 $\\bf P(X | Paranets(X))$를 의미한다. 개별 Edge들은 부모가 자식에게 주는 직접적인 영향(Direct Influence) 을 의미한다. 먼저 확률에 대한 간단한 복습부터 베이지안 네트워크 라는 것은 결국 확률변수(RV) 간의 관계 를 표현한 것이다. 확률 이라는 것은 상대적인 빈도 이다. 독립성 (Independence) $P(A|B) = P(A)$ $\\Leftrightarrow P(A,B) = P(A)P(B)$ $\\Leftrightarrow P(B|A) = P(B)$; A와 B가 독립이면, B는 A와 독립이다. 사건B가 발생했다는 정보는 사건A가 발생할 확률에 추가적인 정보를 제공하지 못한다. 이는, 밑에 서술하는 Conditional Independence 와 대립되는 의미로 Marginal Independence 라고 할 수 있다. 조건부 독립 (Conditional Independence) $P(A|B,C) = P(A|C)$ 사건C가 주어졌을 때 두 사건 A와 B가 독립인 경우, 이것은 C라는 조건하에서 조건부 독립 이다. 조건부 확률 (Conditional Probability) $P(A= true|B=true)$ “Probablity of A given B” B가 주어졌을 때, A의 확률 결합 확률 (joint Probability) $P(A= true, B=true)$ “the probability of A=true and B=true” A=true와 B=true가 동시에 만족할 확률 조건부 확률과 결합 확률의 관계 는 일반적으로, $P(X|Y) =\\cfrac{P(X,Y)}{P(Y)}$ 총 확률 법칙 (Law of Total Probability) “Summing out” or “Marginalization” $P(A) = \\sum_kP(A,B_k) = \\sum_kP(A|B_k)P(B_k)$ $P(A) = \\sum_kP(A,B_k)$ 는 $B_1,B_2,…,B_n$이 각각 상호배반적인 집합이고 이들의 합집합이 전체집합이 되므로 성립 (marginalize) $\\sum_kP(A,B_k) = \\sum_kP(A|B_k)P(B_k)$는 조건부확률과 결합확률의 관계를 이용하면 유도가능 이로 인한 이점은, $P(A)$를 직접 구하는 것보다, $P(A|B_k)$와 같은 조건부확률을 구해서 합치는 것이 일반적으로 더 수월하다는 것이다. 혹은 결합확률을 알고 있을 때, 여러가지 확률을 계산 할 수 있다. 예를들어, 결합확률인 $P(a,b,c,d)$를 알고 있을 때, $P(c|b)$는 이렇게 표현할 수 있다 $P(c|b) = \\sum_a \\sum_d P(a,c,d|b) = \\cfrac{1}{P(b)}\\sum_a \\sum_d {\\bf P(a,b,c,d)}$ 그러나 joint의 경우에는 parameter의 수가 exponential하게 늘어나게 된다! (Chain Rule의 필요성) 확률의 연쇄법칙 (Chain Rule for probability) 모든 joint distribution에 대해, 결합확률과 조건부확률의 관계에 의해 언제나 이하와 같이 표현할 수 있다. $P(a,b,c,…,z) = P(a|b,c,…,z)P(b,c,….,z)$ 이것을 반복적으로 하면, $P(a,b,c,…,z) = P(a|b,c,…,z)P(b|c,…,z)P(c|d,…,z)…P(z)$로 표현 가능하다. (Factorization) 곱 분해 법칙 (Rule of product decomposition) 그래프로 표현된 경우, 그래프가 acyclic graph 라면 그래프에 속한 RV의 결합분포(joint distribution)는 family의 모든 조건부 분포 $P(Child|Parent)$의 곱으로 표현 할 수 있다. $P(x_1,x_2,…,x_n) = \\prod _iP(x_i|Parents(x_i))$ Parents는 직접적으로 연결되어 영향을 받는 변수만을 의미! 예를 들어, $X\\rightarrow Y \\rightarrow Z$ 인 그래프에서 $P(X=x, Y=y, Z=z)$를 구하는 것을 생각해보자 원래는 가능한 모든 조합의 $(x, y, z)$에 해당하는 확률 테이블을 만들어야 한다 그러나, 이 법칙을 이용하면 $P(X=x, Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y)$로 간결하게 표현 가능 이처럼 고차원을 저차원으로 만들어 차원의 저주(curse of dimensionality) 에서도 비교적 자유로워 질 수 있다. BN에 등장하는 전형적인 논리 구조 (Typical Local Structures)","link":"/2020/11/14/%E3%80%90%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E3%80%91doc2vec%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B-dmpv-DBOW%E3%82%82%E8%A7%A3%E8%AA%AC/"}],"tags":[{"name":"causal","slug":"causal","link":"/tags/causal/"},{"name":"KMOOC","slug":"KMOOC","link":"/tags/KMOOC/"}],"categories":[{"name":"KOR","slug":"KOR","link":"/categories/KOR/"},{"name":"통계적인과추론","slug":"KOR/통계적인과추론","link":"/categories/KOR/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/"},{"name":"ML","slug":"KOR/통계적인과추론/ML","link":"/categories/KOR/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/ML/"}]}