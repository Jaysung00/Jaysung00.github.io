<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jay Sung&#39;s DS blog</title>
    <link>https://jaysung00.github.io/</link>
    
    <atom:link href="https://jaysung00.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Mon, 30 Nov 2020 07:13:26 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>【Causal Inference 입문편 ①】인과추론의 목적과 RCT에 관하여</title>
      <link>https://jaysung00.github.io/2020/11/30/CI1/</link>
      <guid>https://jaysung00.github.io/2020/11/30/CI1/</guid>
      <pubDate>Mon, 30 Nov 2020 02:39:29 GMT</pubDate>
      
      <description>&lt;hr&gt;</description>
      
      
      
      <content:encoded><![CDATA[<hr><a id="more"></a><h1><span id="인과추론causal-inference으로-뭘-할-수-있는데">* 인과추론(Causal Inference)으로 뭘 할 수 있는데?</span></h1><hr><blockquote><p>"A사 맥주를 공중파 CF에 내보냈을때, 해당 아이스크림의 매상은 얼마나 올랐을까?"</p></blockquote><blockquote><p>"전 사원 대상 Python 연수 프로그램을 설치 했을때, 사원 들의 일의 능률은 얼마나 올랐을까?"</p></blockquote><p>이와 같은 질문들은 비즈니스에서 일상적으로 흔히 나올 수 있는 질문들이다.</p><p>하지만 이에 대해 <strong>깊은 고찰 없이 단순하게 효과를 정의하고 평가함</strong> 으로써, 우리는 수많은 <strong>바이어스</strong> 를 만들어 내고 있다.</p><p>공중파 CF의 효과를 계산하기 위해 단순히 CF 전후의 매상의 차이를 계산해서, CF와 관계없이 시기적으로 날씨가 더워져서 오른 맥주의 매상까지도 <strong>CF의 효과</strong>로써 평가해버린다.</p><p>또한 Python 연수를 신청한 사람들은 그렇지 않은 사원들보다 원래부터 우수한 사람이 많았을 수 있다. 원래부터 일의 능률이 높은 연수자그룹과 비연수자 그룹의 능률을 단순 비교해서 원래의 차이까지도 <strong>python 연수의 효과</strong>로 평가해버리기 십상이다.</p><p>이처럼 인과추론의 목적을 철저하게 비즈니스적 관점에서 보자면 <strong>어떠한 시책의 정확한 효과측정</strong> 을 위한 이론 &amp; 기술 분야라고 할 수 있다.</p><p><br></p><hr><h1><span id="추론inference-의-신뢰성의-3단계">* 추론(Inference) 의 신뢰성의 3단계</span></h1><hr><h3><span id="level-1-실험experimental-레벨">Level 1. 실험(Experimental) 레벨</span></h3><ul><li><p><strong>RCT (Randomized Controlled Trial; 무작위화 비교 실험)</strong></p></li><li><p>3가지 기본요건</p><p>(1). <strong>비교</strong> : <code>Control Group</code>과 <code>Treatment Group</code> 의 비교를 통해 독립변수가 종속변수에 영향을 미쳤는지 확인하는 과정</p><p>(2). <strong>조작</strong> : 시간적으로 독립변수가 먼저 발생하고 그 후에 뒤따라 종속변수가 발생함을 입증하기 위해, 임의로 독립변수를 의도적인 시기에 발생하도록하고 이에 뒤따른 종속변수의 변화를 측정하도록 시간적 순서를 조작하는 것 <em>(인과성의 선후관계)</em></p><p>(3). <strong>통제</strong> : 허위적 관계가 아닌 것을 입증하기 위해, 독립변수를 제외한 종속변수에 영향을 미칠 수 있는 여러 변수들이 종속변수에 영향을 미치지 못하도록 상황을 의도적으로 통제하는 것</p></li></ul><h3><span id="level-2-준실험quasi-experimental-레벨">Level 2. 준실험(Quasi Experimental) 레벨</span></h3><ul><li><p><code>Level 1</code>의 실험설계는 인과관계를 명확히 구명할 수 있지만, 인위적 통제가 어렵거나 윤리적 문제등으로 인해 (<em>특히 비즈니스의 경우 제한된 예산 등에 의해</em>) 실제 활용이 매우 어렵다. 이에 따라 비록 실험 설계에는 미치지 못하지만, 그 대안적인 방법으로 활용되는 방법이다.</p></li><li><p>대표적인 방법</p><p>(1). <strong>시계열 설계(time-series design)</strong> : 비교집단을 별도로 설정하기 곤란한 경우에 <em>하나의 집단</em> 을 선택해서, 독립변수 도입의 전후상태를 비교하는 방법이다. 외적요인에 대한 통제가 어렵기 때문에 (각 기간마다 외부의 영향이 다르다), 위험이 있을 수 있다. 이를 개선하기 위해서는 같은 조사를 여러 집단에서 되풀이하여 실시하여 같은 결과를 얻을 수 있는지 확인할 필요가 있다.</p><p>(2). <strong>비동일 통제집단 설계(nonequivalent control group design)</strong> : 비동일 통제집단 설계는 실험설계의 통제집단 전후비교와 유사하지만 <em>비교집단을 무작위로 선정하지 않는다</em> 는 차이가 있다. 비동일 통제집단 설계는 무작위배치 이외의 방법(매칭, 기존집단의 선정 등)으로 <code>Control Group</code> 및 <code>Treatment Group</code>을 선정한다.</p></li></ul><h3><span id="level-3-관찰observation-레벨">Level 3. 관찰(Observation) 레벨</span></h3><ul><li><p><em>독립변수를 조작할 수 없고, 연구대상을 무작위화할 수 없는 경우</em> 이다. 어느 한 시점에서 독립변수와 종속변수 모두를 측정해서 상관관계를 파악하는데에 그친다.</p></li><li><p>선후관계가 파악되지 않았고, 무작위화를 통해 동일한 집단에서 비교하지 못했으므로 부적절한 해석을 하게 될 위험을 가지고 있다.</p></li><li><p><strong>확증편향</strong> <span class="math inline">\(^{[*1]}\)</span>(confirmation bias) 이나 <strong>사후해석편향</strong> <span class="math inline">\(^{[*2]}\)</span>(hindsight bias)에 영향을 받기 쉽다. 예를 들어, 시책 담당자가 좋은 결과만을 보고 싶다고 하면 집계의 방법을 유리하게 설정해서 유리한 결과가 나오도록 하는 것이 얼마든지 가능하므로 주의가 필요하다.</p><ul><li><p><code>Level 3</code>은 <code>Level 1</code>&amp; <code>Level 2</code>를 한 후에 추가적으로 검토하는 용도.</p></li><li><p>또한, 집계의 방법을 미리 정해놓는 것을 통해, 자의적으로 변경해서 입맛에 맞는 해석을 하지 않는 것이 중요하다.</p></li></ul></li></ul><p><br></p><h4><span id="여기서-기억해야-할-것-lv1-rightarrow-lv2-rightarrow-lv3-의-순서로-시책의-효과를-검토해가는-것이-중요하다">【여기서 기억해야 할 것】 Lv1 <span class="math inline">\(\rightarrow\)</span> Lv2 <span class="math inline">\(\rightarrow\)</span> Lv3 의 순서로 시책의 효과를 검토해가는 것이 중요하다!!</span></h4><p><br></p><p><span style="font-size: 85%;"> <span class="math inline">\(^{[*1]}:\)</span> 원하는 정보를 선택적으로 모으는 등의 가지고 있는 신념을 확인하려는 경향성. </span></p><p><span style="font-size: 85%;"> <span class="math inline">\(^{[*2]}:\)</span> 어떤 사건이 발생한 후, 사전에 그런 일이 일어날 것으로 예상했었다는 식으로 문제를 처리하는 것. 실제로는 벌어진 사건에 대해 전혀 대비를 하지 못하고, 그 원인을 냉정하게 규명해야 함에도 불구하고 "충분히 예측했던 일"이라며 자기 확신에 빠지는 것.</span></p><hr><h1><span id="potential-outcome-framework">* Potential Outcome Framework</span></h1><hr><ul><li><p><strong>처치(Treatment) 혹은 개입(Intervention)이 이뤄졌는지 여부</strong></p><p><span class="math inline">\(\begin{equation}Z_i= \left \{\begin{array}{l}1　(Treated) \\0　(Untreated)\end{array}\right.\end{equation}\)</span><br><br></p></li><li><p><strong>종속변수(DV; Dependent Variable) 혹은 목적변수(Criterion Variable)</strong> ; 개입을 받은 경우와 받지 않은 경우 두가지로 나타낼 수 있다.<br><em>(실제로는 어느 한쪽만 관찰가능하지만)</em></p><p><span class="math inline">\(\begin{equation}Y_i= \left \{\begin{array}{l}Y_i^{(1)}　(Z_i = 1) \\Y_i^{(0)}　(Z_i = 0)\end{array}\right.\end{equation}\)</span></p><p><span class="math inline">\(\Rightarrow Y_i = Y_i^{(0)}(1- Z_i) + Y_i^{(1)}Z_i\)</span></p></li></ul><p><br></p><ul><li><p>이와 같이, 샘플 <span class="math inline">\(i\)</span> 에 대하여 개입을 받은 경우의 결과 <span class="math inline">\(Y^{(1)}\)</span> 와 받지 않은 경우의 결과 <span class="math inline">\(Y^{(0)}\)</span> 간의 차이가 개입의 진정한 <strong>처치효과(TE; Treatment Effect)</strong> 라고 가정하는 것을 <strong>Potential Outcome Framework</strong> 라고 한다.</p><p><span class="math inline">\(\bf \tau_{TE} = Y^{(1)}-Y^{(0)}\)</span></p></li></ul><p><br></p><ul><li><p>모든 샘플 <span class="math inline">\(i\)</span> 에 대해 각각의 처치효과를 구하는 것은 까다롭기 떄문에, 그룹간의 비교로써 <strong>평균처치효과(ATE; Average Treatment Effect)</strong> 를 다루는 경우도 많다.</p><p><span class="math inline">\(\bf \tau_{ATE}= E[Y^{(1)}]-E[Y^{(0)}]\)</span></p></li></ul><p><br></p><hr><h1><span id="인과추론의-기초-rct">* 인과추론의 기초, RCT</span></h1><hr><blockquote><p>RCT의 특징</p></blockquote><ul><li><p>비즈니스의 관점에서는 <strong>AB테스트</strong> 라고 할 수 있다.</p></li><li><p>RCT (Randomized Controlled Trial; 무작위화 비교 실험)를 통해 <code>Control Group</code>과 <code>Treatment Group</code>을 무작위하게 나눔으로써 <strong>두 그룹간의 동질성</strong> 을 기대할 수 있다.</p></li><li><p>즉 RCT에서는 이론상, <span class="math inline">\(ATU = ATT = ATE\)</span>을 기대할 수 있다.</p><ul><li><p><span class="math inline">\(ATU\)</span> <em>(Average Treatment Effect on the Untreated)</em> <span class="math inline">\(= E[Y^{(1)}|Z=0] - E[Y^{(0)}|Z=0]\)</span></p></li><li><p><span class="math inline">\(ATT\)</span> <em>(Average Treatment Effect on the Treated)</em> <span class="math inline">\(= E[Y^{(1)}|Z=1] - E[Y^{(0)}|Z=1]\)</span></p></li><li><p><span class="math inline">\(\bf ATE\)</span> <strong>(Average Treatment Effect)</strong> <span class="math inline">\(\bf = E[Y^{(1)}] - E[Y^{(0)}]\)</span></p></li></ul></li></ul><p><br></p><p><img src="https://i.imgur.com/waDPtS0.png" width="600px"></p><p><span class="math inline">\(^{[*]}\)</span> 위의 표에서 <code>Control Group</code>의 <span class="math inline">\(Y_i^{(1)}\)</span>과 <code>Treatment Group</code>의 <span class="math inline">\(Y_i^{(0)}\)</span>은 실제로 관찰 불가능한 반사실적 <strong>Potential Outcome</strong> 이다.</p><p><br></p><blockquote><p>RCT의 의의</p></blockquote><ul><li><p><strong>선택바이어스(Selection Bias)</strong> 의 제거</p></li><li><p>조작변수 이외의 다른 변수들을 통제하지 못한 채 <code>Control Group</code>과 <code>Treatment Group</code> 선택하게 되면, 그룹간의 동질성을 확보하지 못하여 <strong>교란변수(confounding factor)</strong> 에 의해 효과가 왜곡 될 수있다. 이러한 것을 <strong>선택 바이어스</strong> 라고 한다.</p></li><li><p>RCT는 완전 무작위로 처치그룹을 선택하기 때문에 <strong>선택 바이어스</strong> 에서 자유로워질 수 있다.</p></li></ul><p><br></p><blockquote><p>RCT의 어려운 점</p></blockquote><p>(1). 비용이 많이 든다.</p><p>(2). noncompliance 문제</p><ul><li>RCT에서 무작위로 그룹을 배분해도 거기에 따르지 않는 사람이 생겨서 나타나는 문제</li></ul><p>(3). <em>(특히 AB테스트에서)</em> 다른 RCT를 같은 대상자에 겹쳐서 실행하게 될 경우, 그에 따른 바이어스가 생길 수 있다.</p><ul><li>통계적으로 처리하기가 상당히 복잡해진다.</li></ul><p><br></p><hr><h2><span id="reference">* Reference</span></h2><p>해당 포스트는 <a href="https://www.youtube.com/watch?v=u8hsTkLg2xc&amp;t=159s">유튜브 채널「データの科学のメソドロジー」의 山田典一님의 강의</a>를 정리 &amp; 추가한 내용임을 밝힙니다.</p><p>추가 내용 참조</p><p><a href="https://www.amazon.co.jp/dp/4297111179/ref=cm_sw_r_tw_dp_U_x_-LQxEbJ8JDZ1N">効果検証入門〜正しい比較のための因果推論/計量経済学の基礎 （安井翔太）</a></p><p><a href="https://www.rieti.go.jp/jp/publications/dp/19j003.pdf">RCTをめぐる3つの問題とその解法（山口一男）</a></p><p><a href="http://blog.daum.net/sangrimza/15612241">http://blog.daum.net/sangrimza/15612241</a></p><p><a href="https://healthpolicyhealthecon.com/2015/02/23/experiment-and-quasi-experiment-1/">https://healthpolicyhealthecon.com/2015/02/23/experiment-and-quasi-experiment-1/</a></p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/KOR/">KOR</category>
      
      <category domain="https://jaysung00.github.io/categories/KOR/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/">통계적인과추론</category>
      
      <category domain="https://jaysung00.github.io/categories/KOR/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/ML/">ML</category>
      
      
      
      <comments>https://jaysung00.github.io/2020/11/30/CI1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【 도대체 베이지안 네트워크가 뭐야? ①】</title>
      <link>https://jaysung00.github.io/2020/11/14/BN1/</link>
      <guid>https://jaysung00.github.io/2020/11/14/BN1/</guid>
      <pubDate>Sat, 14 Nov 2020 14:15:12 GMT</pubDate>
      
      <description>&lt;hr /&gt;</description>
      
      
      
      <content:encoded><![CDATA[<hr><a id="more"></a><h1><span id="베이지안-네트워크bn-bayesian-network-란">* 베이지안 네트워크(BN; Bayesian Network) 란?</span></h1><hr><ul><li>확률 변수(RV; Random variables)들 사이의 조건부 독립 등의 관계를 보임으로써, RV의 full joint distribution등을 간결하게 표현할 수 있는 <strong>그래프 표기법 (Graphical Notation)</strong> 이다.</li></ul><p><br></p><ul><li><p>여기서 <strong>그래프(Graph)</strong> 란, 수학에서 차트(Chart)와 대조되어 정의된 <code>node</code>와 <code>edge</code>의 집합</p><ul><li><p><code>edge</code>가 방향이 지정되어 있으면 <code>directed</code>, 그렇지 않으면 <code>undirected</code></p></li><li><p>그래프의 모든 <code>edge</code>가 <code>directed</code>일 때 <code>directed graph</code></p></li><li><p><code>directed edge</code>에서, 시작되는 쪽의 노드를 <code>parent node</code> 라고 하고 반대쪽은 <code>child node</code>라고 한다</p></li><li><p>복수의 연결된 <code>directed edge</code>의 방향이 같은 경우 이를 <code>directed path</code>라고 하고, <code>directed path</code>의 첫 번째 노드는 경로상의 모든 노드들의 <code>ancestor node</code>이고, 반대로 나머지 노드들은 첫번째 노드의 <code>descendant node</code>이다.</p></li><li><p><code>directed path</code>의 시작점과 끝점이 일치할 경우 이를 <code>cyclic</code>이라 하고, 그렇지 않은 경우 <code>acyclic</code>라고 한다.</p></li></ul></li></ul><p><br></p><ul><li><p><strong>베이지안 네트워크(BN)</strong> 의 <strong>Syntax</strong></p><ul><li><p><code>Network</code> 는 <code>Node</code>와 이들을 연결시키는 <code>Edge</code>로 구성된다</p></li><li><p><code>방향성 비순환 그래프(DAG; Directed Acyclic Graph)</code> 가 되어야 한다</p></li><li><p>개별 <code>Node</code>들은 RV인 <span class="math inline">\(X\)</span>에 대해 <span class="math inline">\(\bf P(X | Paranets(X))\)</span>를 의미한다.</p></li><li><p>개별 <code>Edge</code>들은 부모가 자식에게 주는 <strong>직접적인 영향(Direct Influence)</strong> 을 의미한다.</p></li></ul></li></ul><p><br></p><hr><h1><span id="먼저-확률에-대한-간단한-복습부터">* 먼저 확률에 대한 간단한 복습부터</span></h1><hr><ul><li><strong>베이지안 네트워크</strong> 라는 것은 결국 <em>확률변수(RV) 간의 관계</em> 를 표현한 것이다.<br></li><li><strong>확률</strong> 이라는 것은 <em>상대적인 빈도</em> 이다.<br><br></li></ul><blockquote><p>독립성 (Independence)</p></blockquote><ul><li><p><span class="math inline">\(P(A|B) = P(A)\)</span></p><p><span class="math inline">\(\Leftrightarrow P(A,B) = P(A)P(B)\)</span></p><p><span class="math inline">\(\Leftrightarrow P(B|A) = P(B)\)</span>; A와 B가 독립이면, B는 A와 독립이다.</p><ul><li><p>사건B가 발생했다는 정보는 사건A가 발생할 확률에 추가적인 정보를 제공하지 못한다.</p></li><li><p>이는, 밑에 서술하는 Conditional Independence 와 대립되는 의미로 Marginal Independence 라고 할 수 있다.</p></li></ul></li></ul><p><br></p><blockquote><p>조건부 독립 (Conditional Independence)</p></blockquote><ul><li><p><span class="math inline">\(P(A|B,C) = P(A|C)\)</span></p><ul><li>사건C가 주어졌을 때 두 사건 A와 B가 독립인 경우, 이것은 C라는 조건하에서 <em>조건부 독립</em> 이다.</li></ul></li></ul><p><br></p><blockquote><p>조건부 확률 (Conditional Probability)</p></blockquote><ul><li><p><span class="math inline">\(P(A= true|B=true)\)</span></p><ul><li><p>"Probablity of A given B"</p></li><li><p>B가 주어졌을 때, A의 확률<br><br></p></li></ul></li></ul><blockquote><p>결합 확률 (joint Probability)</p></blockquote><ul><li><p><span class="math inline">\(P(A= true, B=true)\)</span></p><ul><li><p>"the probability of A=true <strong>and</strong> B=true"</p></li><li><p>A=true와 B=true가 동시에 만족할 확률</p></li><li><p><strong>조건부 확률과 결합 확률의 관계</strong> 는 일반적으로, <span class="math inline">\(P(X|Y) =\cfrac{P(X,Y)}{P(Y)}\)</span><br><br></p></li></ul></li></ul><blockquote><p>총 확률 법칙 (Law of Total Probability)</p></blockquote><ul><li><p>"Summing out" or "Marginalization"</p></li><li><p><span class="math inline">\(P(A) = \sum_kP(A,B_k) = \sum_kP(A|B_k)P(B_k)\)</span></p><ul><li><p><span class="math inline">\(P(A) = \sum_kP(A,B_k)\)</span> 는 <span class="math inline">\(B_1,B_2,...,B_n\)</span>이 각각 상호배반적인 집합이고 이들의 합집합이 전체집합이 되므로 성립 (marginalize)</p></li><li><p><span class="math inline">\(\sum_kP(A,B_k) = \sum_kP(A|B_k)P(B_k)\)</span>는 조건부확률과 결합확률의 관계를 이용하면 유도가능<br><br></p></li></ul></li><li><p>이로 인한 이점은, <span class="math inline">\(P(A)\)</span>를 직접 구하는 것보다, <span class="math inline">\(P(A|B_k)\)</span>와 같은 조건부확률을 구해서 합치는 것이 일반적으로 더 수월하다는 것이다.</p></li><li><p>혹은 결합확률을 알고 있을 때, 여러가지 확률을 계산 할 수 있다.</p><ul><li><p>예를들어, 결합확률인 <span class="math inline">\(P(a,b,c,d)\)</span>를 알고 있을 때, <span class="math inline">\(P(c|b)\)</span>는 이렇게 표현할 수 있다</p></li><li><p><span class="math inline">\(P(c|b) = \sum_a \sum_d P(a,c,d|b) = \cfrac{1}{P(b)}\sum_a \sum_d {\bf P(a,b,c,d)}\)</span></p></li><li><p>그러나 joint의 경우에는 parameter의 수가 exponential하게 늘어나게 된다! (Chain Rule의 필요성)</p></li></ul></li></ul><p><br></p><blockquote><p>확률의 연쇄법칙 (Chain Rule for probability)</p></blockquote><ul><li><p>모든 joint distribution에 대해, 결합확률과 조건부확률의 관계에 의해 언제나 이하와 같이 표현할 수 있다.</p></li><li><p><span class="math inline">\(P(a,b,c,...,z) = P(a|b,c,...,z)P(b,c,....,z)\)</span></p></li><li><p>이것을 반복적으로 하면, <span class="math inline">\(P(a,b,c,...,z) = P(a|b,c,...,z)P(b|c,...,z)P(c|d,...,z)...P(z)\)</span>로 표현 가능하다. (Factorization)</p></li></ul><p><br></p><blockquote><p>곱 분해 법칙 (Rule of product decomposition)</p></blockquote><ul><li><p>Bayesian Network에서는 그래프에 속한 RV의 결합분포(joint distribution)는 <code>family</code>의 모든 조건부 분포 <span class="math inline">\(P(Child|Parent)\)</span>의 곱<span class="math inline">\(^{[*1]}\)</span>으로 표현 할 수 있다. <em>(시리즈의 다음 포스트의 Factorization of Bayes Network 내용 참조)</em></p></li><li><p><span class="math inline">\(P(x_1,x_2,...,x_n) = \prod _iP(x_i|Parents(x_i))\)</span></p><ul><li><p>Parents는 직접적으로 연결되어 영향을 받는 변수만을 의미!</p></li><li><p>예를 들어, <span class="math inline">\(X\rightarrow Y \rightarrow Z\)</span> 인 그래프에서 <span class="math inline">\(P(X=x, Y=y, Z=z)\)</span>를 구하는 것을 생각해보자</p></li><li><p>원래는 가능한 모든 조합의 <span class="math inline">\((x, y, z)\)</span>에 해당하는 확률 테이블을 만들어야 한다</p></li><li><p>그러나, 이 법칙을 이용하면 <span class="math inline">\(P(X=x, Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y)\)</span>로 간결하게 표현 가능</p></li><li><p>이처럼 고차원을 저차원으로 만들어 <em>차원의 저주(curse of dimensionality)</em> 에서도 비교적 자유로워 질 수 있다.</p></li></ul></li></ul><p><br></p><p>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*1]}:\)</span> 이렇게 정의되는 원래는 뒤에서 기술하는 베이지안 네트워크의 Typical Local Structures Rules와 관련 되어있다. </p><hr><h1><span id="베이지안-네트워크의-rules-of-typical-local-structures">* 베이지안 네트워크의 Rules of Typical Local Structures</span></h1><hr><p><br></p><blockquote><p>Rule 1. 사슬 혹은 폭포형 (Chain or Cascading)</p></blockquote><p><img src="https://i.imgur.com/IF5m1WL.png"></p><ul><li><p>변수<span class="math inline">\(X\)</span>와 변수<span class="math inline">\(Y\)</span>의 사이에 하나의 방향성 경로 만 있고 변수<span class="math inline">\(Z\)</span>가 해당 경로를 가로막고 있는 경우, <strong><span class="math inline">\(Z\)</span>가 조건부로 주어졌을때 두 변수 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>는 조건부 독립</strong> 이다.</p></li><li><p><span class="math inline">\(X \perp Y|Z\)</span><br><span class="math inline">\(\Leftrightarrow P(Y|X,Z) = P(Y|Z)\)</span></p></li></ul><p><br></p><blockquote><p>Rule 2. 분기 혹은 공통부모형 (Fork or Common parent)</p></blockquote><p><img src="https://i.imgur.com/mIdGQWD.png"></p><ul><li><p>변수 <span class="math inline">\(Z\)</span>가 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>의 공통 원인이고 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>사이에 단 하나의 경로가 있는 경우, <strong><span class="math inline">\(Z\)</span>의 조건이 주어졌을 때 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>는 조건부 독립</strong> 이다.</p></li><li><p><span class="math inline">\(X \perp Y | Z\)</span><br><span class="math inline">\(\Leftrightarrow P(X,Y|Z) = P(X|Z)P(Y|Z)\)</span></p></li></ul><p><br></p><blockquote><p>Rule 3. 충돌부 혹은 V-구조 (Collider or V-structure)</p></blockquote><p><img src="https://i.imgur.com/9HLO4Ad.png"></p><ul><li><p>변수 <span class="math inline">\(Z\)</span>가 두 변수 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span> 사이의 충돌 노드이고 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span> 사이에 단 <em>하나의 경로</em> 만 있을 경우, <strong><span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>는 비조건부 독립(underconditionally independent)</strong> 이다. 그러나 <strong><span class="math inline">\(Z\)</span> 또는 <span class="math inline">\(Z\)</span>의 <code>descendant</code>을 조건부로 하였을 때 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>는 종속적일 가능성</strong> 이 있다.</p></li><li><p><span class="math inline">\(\sim (X \perp Y|Z)\)</span><br><span class="math inline">\(\Leftrightarrow P(X,Y,Z)=P(X)P(Y)P(Z|X,Y)\)</span></p></li><li><p>즉 <span class="math inline">\(Z\)</span>가 not given 일 때는 독립이지만, 반대로 <span class="math inline">\(Z\)</span>가 given으로 주어지면 <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>가 종속적이 될 가능성이 생겨버린다.</p></li></ul><p><br></p><hr><h1><span id="bayes-ball-algorithm">* Bayes Ball Algorithm</span></h1><hr><ul><li><p>목적 ; <span class="math inline">\(X \perp Y | Z\)</span> (<span class="math inline">\(Z\)</span>가 given일 때 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>가 독립) 이 성립하는지 여부를 판정하기 위한 알고리즘</p></li><li><p><span class="math inline">\(X\)</span>에서 공이 출발한다고 가정했을 때 <span class="math inline">\(Y\)</span>까지 공이 도달하는지 확인하는 방법</p></li><li><p>여기서 공은 <code>Information</code>을 의미하고 화살표는 공의 움직임을 의미한다. 노드 간이 직접적인 edge로 연결되어 있지 않더라도 공이 굴러가서 도달할 수 있다면 <code>Indirect influence</code>가 존재하기때문에 두 변수는 <code>depedent</code>하다는 것을 의미한다.</p></li></ul><p><br></p><blockquote><p>Rule 1의 경우</p></blockquote><p>(1). <span class="math inline">\(Z\)</span>가 given이 아닐 때, 공은 지나갈 수 있다. (<span class="math inline">\(X, Y\)</span>는 종속)<br><img src="https://i.imgur.com/A5X39bt.png" width="298px"></p><p>(2). <span class="math inline">\(Z\)</span>가 <strong>given</strong> 일 때, 공은 지나갈 수 없다. (<span class="math inline">\(X \perp Y|Z\)</span>)<br><img src="https://i.imgur.com/k6dl20u.png" width="300px"></p><p><br></p><blockquote><p>Rule 2의 경우</p></blockquote><p>(1). <span class="math inline">\(Z\)</span>가 given이 아닐 때, 공은 지나갈 수 있다. (<span class="math inline">\(X, Y\)</span>는 종속)<br><img src="https://i.imgur.com/8mPvc3A.png" width="300px"></p><p>(2). <span class="math inline">\(Z\)</span>가 <strong>given</strong> 일 때, 공은 지나갈 수 없다. (<span class="math inline">\(X \perp Y|Z\)</span>)</p><p><img src="https://i.imgur.com/hssut55.png" width="300px"></p><p><br></p><blockquote><p>Rule 3의 경우</p></blockquote><p>(1). <span class="math inline">\(Z\)</span>가 <strong>given이 아닐 때, 공은 지나갈 수 없다.</strong> (<span class="math inline">\(\bf X \perp Y\)</span>)<br><img src="https://i.imgur.com/yhO2p9I.png" width="300px"></p><p>(2). <span class="math inline">\(X_C\)</span>가 <strong>given</strong> 일 때, 반대로 path가 생겨서 공이 지나갈 수 있게 된다. (<span class="math inline">\(X, Y\)</span>는 <strong>종속</strong> <span class="math inline">\(|Z\)</span>)</p><p><img src="https://i.imgur.com/Y6SAkrl.png" width="300px"></p><p><br></p><blockquote><p>Bayes Ball Algorithm 연습</p></blockquote><p><img src="https://i.imgur.com/7He2cq7.png" width="350px"></p><p><br></p><ul><li><p><strong>문제 1.</strong> <span class="math inline">\(X_1\perp X_4|X_2\)</span></p><p>두가지 경로로 공을 굴릴 수 있다.</p><p>(1). <span class="math inline">\(X_1 \rightarrow {\bf X_2}(given) \rightarrow X_4\)</span> 의 경로는 <span class="math inline">\(X_2\)</span>가 사슬의 given으로 막혀있으므로 지나갈 수 없다.</p><p>(2). <span class="math inline">\(X_1 \rightarrow X_3 \rightarrow X_5 \rightarrow X_6 \leftarrow {\bf X_2}(given) \rightarrow X_4\)</span> 의 경로는 <span class="math inline">\(X_6\)</span>가 충돌부의 not given으로 막혀있으므로 지나갈 수 없다.</p><p>따라서 어떠한 경로로도 볼은 지나갈수 없으므로 <strong><span class="math inline">\(X_2\)</span>가 given일 때 <span class="math inline">\(X_1\)</span>와 <span class="math inline">\(X_4\)</span>는 독립</strong> 이다.</p></li></ul><p><br></p><ul><li><p><strong>문제 2.</strong> <span class="math inline">\(X_2\perp X_5|X_1\)</span></p><p>두가지 경로로 공을 굴릴 수 있다.</p><p>(1). <span class="math inline">\(X_2 \rightarrow X_6 \leftarrow X_5\)</span> 의 경로는 <span class="math inline">\(X_6\)</span>가 충돌부의 not given으로 막혀있으므로 지나갈 수 없다.</p><p>(2). <span class="math inline">\(X_2 \leftarrow {\bf X_1}(given) \rightarrow X_3 \rightarrow X_5\)</span> 의 경로는 <span class="math inline">\(X_1\)</span>가 분기의 given으로 막혀있으므로 지나갈 수 없다.</p><p>따라서 어떠한 경로로도 볼은 지나갈수 없으므로 <strong><span class="math inline">\(X_1\)</span>가 given일 때 <span class="math inline">\(X_2\)</span>와 <span class="math inline">\(X_5\)</span>는 독립</strong> 이다.</p></li></ul><p><br></p><ul><li><p><strong>문제 3.</strong> <span class="math inline">\(X_1\perp X_6|\{X_2, X_3\}\)</span></p><p>두가지 경로로 공을 굴릴 수 있다.</p><p>(1). <span class="math inline">\(X_1 \rightarrow {\bf X_2}(given) \rightarrow X_6\)</span> 의 경로는 <span class="math inline">\(X_2\)</span>가 사슬의 given으로 막혀있으므로 지나갈 수 없다.</p><p>(2). <span class="math inline">\(X_1 \rightarrow {\bf X_3}(given) \rightarrow X_5 \rightarrow X_6\)</span> 의 경로는 <span class="math inline">\(X_3\)</span>가 사슬의 given으로 막혀있으므로 지나갈 수 없다.</p><p>따라서 어떠한 경로로도 볼은 지나갈수 없으므로 <strong><span class="math inline">\(\{X_2, X_3\}\)</span>가 given일 때 <span class="math inline">\(X_1\)</span>와 <span class="math inline">\(X_6\)</span>는 독립</strong> 이다.</p></li></ul><p><br></p><ul><li><p><strong>문제 4.</strong> <span class="math inline">\(X_2\perp X_3|\{X_1, X_6\}\)</span></p><p>두가지 경로로 공을 굴릴 수 있다.</p><p>(1). <span class="math inline">\(X_2 \leftarrow {\bf X_1}(given) \rightarrow X_3\)</span> 의 경로는 <span class="math inline">\(X_1\)</span>가 분기의 given으로 막혀있으므로 지나갈 수 없다.</p><p>(2). <span class="math inline">\(X_2 \rightarrow {\bf X_6}(given) \leftarrow X_5 \leftarrow X_3\)</span> 의 경로는 <span class="math inline">\(X_6\)</span>가 충돌부의 given으로 뚫려있으므로 지나갈 수 있다.</p><p>따라서 두번째 경로로 볼은 지나갈 수 있으므로 <strong><span class="math inline">\(\{X_1, X_6\}\)</span>가 given일 때 <span class="math inline">\(X_2\)</span>와 <span class="math inline">\(X_3\)</span>는 독립이 성립하지 않는다.</strong></p></li></ul><p><br></p><hr><h1><span id="d-seperation의-정의">* <span class="math inline">\(d\)</span>-Seperation의 정의</span></h1><hr><ul><li><p><span class="math inline">\(d\)</span>는 방향성(directly)을 의미한다.</p></li><li><p>Bayesian Ball Algorithm으로 <span class="math inline">\(d\)</span>-Seperation을 확인할 수 있다.</p></li><li><p>정리하자면, 경로p가 조건부집합 <span class="math inline">\(\{W\}\)</span>에 의해 <span class="math inline">\(d\)</span>-Seperate된다는 명제는 이하와 필요충분조건이다.</p><ol type="1"><li><p>경로p는 조건부집합 <span class="math inline">\(\{W\}\)</span>에 속하는 중간노드 <span class="math inline">\(Z\)</span> 의 사슬 <span class="math inline">\(X \rightarrow Z \rightarrow Y\)</span> 또는 분기 <span class="math inline">\(X \leftarrow Z \rightarrow Y\)</span> 를 포함한다.</p></li><li><p>경로p는 조건부집합 <span class="math inline">\(\{W\}\)</span>에 속하지 않는 중간노드 <span class="math inline">\(Z&#39;\)</span> 의 충돌부 <span class="math inline">\(X \rightarrow Z&#39; \leftarrow Y\)</span> 를 포함한다.</p></li></ol></li></ul><p><br></p><hr><h2><span id="reference">* Reference</span></h2><p>해당 포스트는 <a href="https://www.edwith.org/machinelearning2__17/joinLectures/9782">Edwith에 개설된 문일철 교수님의 인공지능 및 기계학습 개론 II 강의</a>를 정리 &amp; 추가한 내용임을 밝힙니다.</p><p>추가 내용 참조</p><p><a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;mallGb=KOR&amp;barcode=9791125102236">의학 및 사회과학 연구를 위한 통계적 인과추론 （Judea Pearl, Madelyn Glymour, Nicholas P. Jewell）</a></p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/KOR/">KOR</category>
      
      <category domain="https://jaysung00.github.io/categories/KOR/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/">통계적인과추론</category>
      
      <category domain="https://jaysung00.github.io/categories/KOR/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/ML/">ML</category>
      
      
      <category domain="https://jaysung00.github.io/tags/causal/">causal</category>
      
      <category domain="https://jaysung00.github.io/tags/KMOOC/">KMOOC</category>
      
      
      <comments>https://jaysung00.github.io/2020/11/14/BN1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【 도대체 베이지안 네트워크가 뭐야? ②】</title>
      <link>https://jaysung00.github.io/2020/11/14/BN2/</link>
      <guid>https://jaysung00.github.io/2020/11/14/BN2/</guid>
      <pubDate>Sat, 14 Nov 2020 14:15:12 GMT</pubDate>
      
      <description>&lt;hr /&gt;</description>
      
      
      
      <content:encoded><![CDATA[<hr><a id="more"></a><h1><span id="factorization-of-bayes-network">* Factorization of Bayes Network</span></h1><hr><ul><li><p>그래프에 속한 RV의 결합분포(joint distribution)는 <code>family</code>의 모든 조건부 분포 <span class="math inline">\(P(Child|Parent)\)</span>의 곱으로 표현 할 수 있다.</p></li><li><p><span class="math inline">\(P(X_1,X_2,...,X_n) = \prod _iP(X_i|Parents(X_i))\)</span></p></li><li><p><code>곱 분해 법칙 (Rule of product decomposition)</code></p></li><li><p>확률의 연쇄법칙 <span class="math inline">\(P(a,b,c,...,z) = P(a|b,c,...,z)P(b,c,....,z)\)</span>에서 사슬과 분기의 Rule에 따르면 부모노드가 given이면 이상은 조상노드는 전부 독립이게 되므로 성립. (충돌부는 부모노드가 아니다.)</p></li><li><p>즉, Bayes Network의 정보를 통해 joint distribution를 계산할 때 parameter의 갯수를 줄일 수 있다.</p></li></ul><p><br></p><p><span class="math inline">\([ 예시 ]\)</span><br><img src="https://i.imgur.com/tl4t2Iz.png" width="350px"></p><p><br></p><ul><li><span class="math inline">\(P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8)\)</span>를 구한다고 하자.</li></ul><ol type="1"><li><p><strong>확률의 연쇄법칙 (Chain Rule for probability)</strong> 에 의해 아무런 Bayesian Network의 정보가 없다고 하더라도 <span class="math inline">\(P(X_1,X_2,X_3,...,X_8) = P(X_1|X_2,X_3,...,X_8)P(X_2|X_3,...,X_8)P(X_3|X_4,...,X_8)...P(X_8)\)</span> 로 Factorize 할 수 있다.</p></li><li><p><strong>곱 분해 법칙 (Rule of product decomposition)</strong> 에 의해 Bayesian Network의 정보를 활용하면 <span class="math inline">\(P(X_1,X_2,X_3,...,X_8) = P(X_1)P(X_2)P(X_3|X_1)P(X_4|X_2)P(X_5|X_2)P(X_6|X_3,X_4)P(X_7|X_6)P(X_8|X_5,X_6)\)</span> 로 훨씬 작은 parameter만으로 Factorize 가능하다.</p></li></ol><p><br></p><hr><h1><span id="plate-notation">* Plate Notation</span></h1><hr><blockquote><p><span class="math inline">\(\begin{align} P(D|\theta) &amp;= P(X_1,...,X_N|\mu,\sigma) \\ &amp;= \prod_i^N P(X_i|\mu,\sigma) \end{align}\)</span></p></blockquote><p><img src="https://i.imgur.com/QqfT9En.png" width="700px"></p><ul><li>이처럼 여러 개의 독립적인 RV들에 대해 위와 같이 <strong>Plate Notation</strong> 로 표현하는 것이 가능하다.</li></ul><p><br></p><hr><h1><span id="베이지안-네트워크에서의-확률추론">* 베이지안 네트워크에서의 확률추론</span></h1><hr><ul><li><p>BN에 있는 모든 random variables ;<br><span class="math inline">\(X = \{X1 ... X_N\}\)</span></p></li><li><p>주어진 증거 변수 (given evidence variables) ;<br><span class="math inline">\(X_V =\{X_{k+1}...X_N\}\)</span><br><span class="math inline">\(x_V\)</span>는 evidence values</p></li><li><p>명시적으로 다루지는 않지만 관계가 있어서 감안할 필요가 있는 변수 (hidden variables) ;<br><span class="math inline">\(X_H = X-X_V = \{X_1...X_k\}\)</span></p></li><li><p>hidden variables ; <span class="math inline">\(X_H = \{Y,Z\}\)</span></p><ul><li><span class="math inline">\(Y\)</span> : query variable (interested hidden variables)<br></li><li><span class="math inline">\(Z\)</span> : uninterested hidden variables</li></ul></li></ul><p><br></p><h3><span id="1-1-주변확률-marginal-probability">1-1 주변확률 (Marginal Probability)</span></h3><blockquote><p>증거 변수 <span class="math inline">\(X_V\)</span> 의 <strong>주변확률 (Marginal Probability)</strong> <span class="math inline">\(P(x_V)\)</span> 는?</p></blockquote><p>      <span class="math inline">\(\begin{align} P(x_V) &amp;=\sum_{X_H}P(X)=\sum_{X_H}P(X_H,X_V) \space\space\space\space\space\space\dots(1)\\ &amp;= \sum_{x_1}...\sum_{x_k}P(x_1...x_k,x_V)\space\space\space\space\space\space\space\space\space\dots(2) \end{align}\)</span></p><ul><li><p>(1). 모든 변수에 대해 <strong>full joint</strong> 된 것을 <span class="math inline">\(X_H\)</span>로 marginalize out한 것이라고 생각한다.</p></li><li><p>(2). 각각의 Hidden variable에 대해 marginalize out한 것이라고 생각한다.</p></li></ul><p><br></p><h3><span id="1-2-조건부-확률-conditional-probability">1-2. 조건부 확률 (Conditional Probability)</span></h3><blockquote><p>주어진 증거(evidence)의 집합<span class="math inline">\(x_V\)</span>이 있을때, <strong>query variable(주어지지 않았지만 관심있는 변수)</strong> 의 <strong>조건부 확률</strong> <span class="math inline">\(P(Y|x_V)\)</span>은?</p></blockquote><p>      <span class="math inline">\(\begin{align} P(Y|X_V) &amp;= \sum_ZP(Y,Z = z|x_V) \space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\dots(1)\\ &amp;= \sum_Z\cfrac{P(Y,Z,x_V)}{P(x_V)} = \sum_Z\alpha P(X) \space\space\space\space\space\space\space\space\space\space\dots(2)\\ &amp;= \sum_Z \cfrac{P(Y,Z,x_V)}{\sum_{y,z}P(Y=y, Z=z, x_V)}\space\space\space\space\space\space\space\space\space\dots(3) \end{align}\)</span><br><br></p><ul><li><p>(1). <span class="math inline">\(Z\)</span>를 joint로 넣어주면서 <span class="math inline">\(Z\)</span>에 대해 marginalize out 한다.</p></li><li><p>(2). 조건부 확률의 정의를 이용해, <span class="math inline">\(x_V\)</span>를 포함한 <strong>full joint</strong> 를 <span class="math inline">\(P(x_V)\)</span> (Marginal Probability)로 나눈다.<br>(<span class="math inline">\(\cfrac{1}{P(x_V)} = \alpha\)</span>라는 정규화 상수(normalization constant)의 곱으로 생각할수도 있다.)</p></li><li><p>(3). 분모의 주변확률은 Inference Question1처럼 <strong>full joint</strong> 를 모든 Hidden variable에 대해 marginalize 해서 구할 수 있다.</p></li></ul><p><br></p><hr><h1><span id="변수제거-알고리즘-variable-eliminatation-algorithm">* 변수제거 알고리즘 (Variable Eliminatation Algorithm)</span></h1><hr><p><img src="https://i.imgur.com/suXGsdJ.png" width="750px"></p><blockquote><p>위와 같이 주어진 상황에서 변수제거 알고리즘으로 <span class="math inline">\(P(J=j)\)</span>를 구해보자</p></blockquote><h3><span id="step1">* Step1</span></h3><ul><li>위의 준비를 통해 베이지안 네트워크 상에서 관심있는 확률의 추론을 위해서, <strong>full joint</strong> 를 구하고 uninterested hidden variable에 대해 <strong>Marginalize</strong> 한다.</li></ul><p><br></p><ul><li><span class="math inline">\(\sum_{A,E,B,M} P(J= j,A,E,B,M)\)</span></li></ul><p><br></p><h3><span id="step2">* Step2</span></h3><ul><li><p><strong>full joint</strong> 를 Bayesina Network의 정보를 이용해 <em>곱분해 법칙</em>으로 바꿔 쓴다.</p></li><li><p>분해한 곱의 나열순서는 <strong>topological order</strong> <span class="math inline">\(^{[*1]}\)</span> 를 따른다.</p></li><li><p><strong>topological order</strong> ; B, E, A, J, M</p></li></ul><p><br></p><ul><li><span class="math inline">\(\sum_{B,E,A,M} P(B)P(E)P(A|B,E)P(J=j|A)P(M|A)\)</span></li></ul><p><br></p><p>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*1]}:\)</span> 들어오는 화살표가 없는 노드 부터 하나씩 선택하며 지우는 것을 반복할때 결정되는 순서 </p><h3><span id="step3">* Step3</span></h3><ul><li><p>순서를 유지한 채 각 <span class="math inline">\(\sum\)</span>가 관련없는 것을 밖으로 빼낸다.</p></li><li><p>제거할 변수의 순서는 뒤에서부터 정해진다.</p></li></ul><p><br></p><ul><li><span class="math inline">\(\space\space\space\sum_B P(B)\sum_EP(E)\sum_AP(A|B,E)P(J=j|A)\sum_MP(M|A)\)</span></li></ul><p><br></p><h3><span id="step4">* Step4</span></h3><ul><li>뒤에서 부터 <strong>function notation</strong>으로 바꿔주면서 변수를 지워나간다.</li></ul><p><br></p><ol type="1"><li><span class="math inline">\(\space\space\space\sum_B P(B)\sum_EP(E)\sum_AP(A|B,E)P(J=j|A) \underline {\sum_MP(M|A)}\)</span></li></ol><ul><li><p><span class="math inline">\(=\sum_B P(B)\sum_EP(E)\sum_AP(A|B,E)P(J=j|A) \underline {\bf f_1(A)}\)</span></p><ul><li>밑줄친 부분은 J와 <span class="math inline">\(d\)</span>-seperate이기 때문에 고려할 필요가 없다. 즉, A의 값과 상관없이 <span class="math inline">\(f_1(A)\)</span>는 1을 갖는다.</li></ul></li></ul><p><img src="https://i.imgur.com/EnN5hP3.png" width="220px"></p><p><br></p><ol start="2" type="1"><li><span class="math inline">\(=\sum_B P(B)\sum_EP(E)\underline{\sum_AP(A|B,E)P(J=j|A)}\)</span></li></ol><ul><li><p><span class="math inline">\(=\sum_B P(B)\sum_EP(E)\underline {\bf f_2(E,B)}\)</span></p><ul><li><span class="math inline">\(f_2(E,B)\)</span>는 이하와 같다.</li></ul></li></ul><p><img src="https://i.imgur.com/BOhvHDZ.png" width="750px"></p><p><br></p><ol start="3" type="1"><li><span class="math inline">\(=\sum_B P(B)\underline {\sum_EP(E)f_2(B,E)}\)</span></li></ol><ul><li><p><span class="math inline">\(=\sum_B P(B) \underline {\bf f_3(B)}\)</span></p><ul><li><span class="math inline">\(f_3(B)\)</span>는 이하와 같다.</li></ul></li></ul><p><img src="https://i.imgur.com/A3u5hM9.png" width="750px"></p><p><br></p><ol start="4" type="1"><li><span class="math inline">\(= \sum_BP(B)f_3(B)\)</span></li></ol><ul><li><p><span class="math inline">\(= P(B=b)f_3(B=b) + P(B= \sim b)f_3(B=\sim b)\)</span></p></li><li><p><span class="math inline">\(=0.001 * 0.849017 + 0.999 * 0.0513413 \fallingdotseq 0.052139\)</span></p></li><li><p>따라서, <span class="math inline">\(P(J=j) = 0.052139\)</span> 가 된다.</p></li></ul><p><br></p><hr><h2><span id="reference">* Reference</span></h2><p>해당 포스트는 <a href="https://www.edwith.org/machinelearning2__17/joinLectures/9782">Edwith에 개설된 문일철 교수님의 인공지능 및 기계학습 개론 II 강의</a>를 정리 &amp; 추가한 내용임을 밝힙니다.</p><p>추가 내용 참조 <a href="https://www.youtube.com/watch?v=TZnEJ4wvLPY">https://www.youtube.com/watch?v=TZnEJ4wvLPY</a></p><p><a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;mallGb=KOR&amp;barcode=9791125102236">의학 및 사회과학 연구를 위한 통계적 인과추론 （Judea Pearl, Madelyn Glymour, Nicholas P. Jewell）</a></p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/KOR/">KOR</category>
      
      <category domain="https://jaysung00.github.io/categories/KOR/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/">통계적인과추론</category>
      
      <category domain="https://jaysung00.github.io/categories/KOR/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/ML/">ML</category>
      
      
      <category domain="https://jaysung00.github.io/tags/causal/">causal</category>
      
      <category domain="https://jaysung00.github.io/tags/KMOOC/">KMOOC</category>
      
      
      <comments>https://jaysung00.github.io/2020/11/14/BN2/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
