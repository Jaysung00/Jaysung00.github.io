<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jay Sung&#39;s DS blog</title>
    <link>https://jaysung00.github.io/</link>
    
    <atom:link href="https://jaysung00.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Thu, 17 Dec 2020 11:44:00 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>jupyter</title>
      <link>https://jaysung00.github.io/2020/12/17/jupyter/</link>
      <guid>https://jaysung00.github.io/2020/12/17/jupyter/</guid>
      <pubDate>Thu, 17 Dec 2020 11:19:15 GMT</pubDate>
      
        
        
      <description>&lt;h1&gt;&lt;span id=&quot;feature-selection-for-uplift-modeling&quot;&gt;Feature Selection for Uplift Modeling&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;This notebook includes two section</description>
        
      
      
      
      <content:encoded><![CDATA[<h1><span id="feature-selection-for-uplift-modeling">Feature Selection for Uplift Modeling</span></h1><p>This notebook includes two sections:<br>- <strong>Feature selection</strong>: demonstrate how to use Filter methods to select the most important numeric features - <strong>Performance evaluation</strong>: evaluate the AUUC performance with top features dataset</p><p><em>(Paper reference: <a href="https://arxiv.org/abs/2005.03447">Zhao, Zhenyu, et al. "Feature Selection Methods for Uplift Modeling." arXiv preprint arXiv:2005.03447 (2020).</a>)</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> causalml.dataset <span class="keyword">import</span> make_uplift_classification</span><br></pre></td></tr></table></figure><pre><code>The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.</code></pre><h4><span id="import-filterselect-class-for-filter-methods">Import FilterSelect class for Filter methods</span></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> causalml.feature_selection.filters <span class="keyword">import</span> FilterSelect</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> causalml.inference.tree <span class="keyword">import</span> UpliftRandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> causalml.inference.meta <span class="keyword">import</span> BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor</span><br><span class="line"><span class="keyword">from</span> causalml.metrics <span class="keyword">import</span> plot_gain, auuc_score</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(<span class="string">&#x27;causalml&#x27;</span>)</span><br><span class="line">logging.basicConfig(level=logging.INFO)</span><br></pre></td></tr></table></figure><h3><span id="generate-dataset">Generate dataset</span></h3><p>Generate synthetic data using the built-in function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define parameters for simulation</span></span><br><span class="line"></span><br><span class="line">y_name = <span class="string">&#x27;conversion&#x27;</span></span><br><span class="line">treatment_group_keys = [<span class="string">&#x27;control&#x27;</span>, <span class="string">&#x27;treatment1&#x27;</span>]</span><br><span class="line">n = <span class="number">100000</span></span><br><span class="line">n_classification_features = <span class="number">50</span></span><br><span class="line">n_classification_informative = <span class="number">10</span></span><br><span class="line">n_classification_repeated = <span class="number">0</span></span><br><span class="line">n_uplift_increase_dict = &#123;<span class="string">&#x27;treatment1&#x27;</span>: <span class="number">8</span>&#125;</span><br><span class="line">n_uplift_decrease_dict = &#123;<span class="string">&#x27;treatment1&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">delta_uplift_increase_dict = &#123;<span class="string">&#x27;treatment1&#x27;</span>: <span class="number">0.1</span>&#125;</span><br><span class="line">delta_uplift_decrease_dict = &#123;<span class="string">&#x27;treatment1&#x27;</span>: <span class="number">-0.1</span>&#125;</span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">20200808</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df, X_names = make_uplift_classification(</span><br><span class="line">    treatment_name=treatment_group_keys,</span><br><span class="line">    y_name=y_name,</span><br><span class="line">    n_samples=n,</span><br><span class="line">    n_classification_features=n_classification_features,</span><br><span class="line">    n_classification_informative=n_classification_informative,</span><br><span class="line">    n_classification_repeated=n_classification_repeated,</span><br><span class="line">    n_uplift_increase_dict=n_uplift_increase_dict,</span><br><span class="line">    n_uplift_decrease_dict=n_uplift_decrease_dict,</span><br><span class="line">    delta_uplift_increase_dict = delta_uplift_increase_dict, </span><br><span class="line">    delta_uplift_decrease_dict = delta_uplift_decrease_dict,</span><br><span class="line">    random_seed=random_seed</span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>INFO:numexpr.utils:NumExpr defaulting to 4 threads.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>treatment_group_key</th><th>x1_informative</th><th>x2_informative</th><th>x3_informative</th><th>x4_informative</th><th>x5_informative</th><th>x6_informative</th><th>x7_informative</th><th>x8_informative</th><th>x9_informative</th><th>...</th><th>x56_uplift_increase</th><th>x57_uplift_increase</th><th>x58_uplift_increase</th><th>x59_increase_mix</th><th>x60_uplift_decrease</th><th>x61_uplift_decrease</th><th>x62_uplift_decrease</th><th>x63_uplift_decrease</th><th>conversion</th><th>treatment_effect</th></tr></thead><tbody><tr><th>0</th><td>control</td><td>0.653960</td><td>-0.217603</td><td>1.856916</td><td>-0.075662</td><td>0.080971</td><td>-0.338374</td><td>-1.011470</td><td>0.528000</td><td>0.115418</td><td>...</td><td>1.533832</td><td>-2.183001</td><td>1.839608</td><td>0.755302</td><td>1.835047</td><td>-0.458431</td><td>-1.927525</td><td>2.765331</td><td>0</td><td>0</td></tr><tr><th>1</th><td>control</td><td>3.439658</td><td>0.477855</td><td>-0.377658</td><td>-1.317121</td><td>0.861815</td><td>-0.393180</td><td>0.503727</td><td>2.323846</td><td>1.229948</td><td>...</td><td>-1.192333</td><td>-1.581815</td><td>2.423700</td><td>2.396904</td><td>0.296043</td><td>-1.961940</td><td>-1.444725</td><td>1.469213</td><td>1</td><td>0</td></tr><tr><th>2</th><td>treatment1</td><td>0.130907</td><td>-0.333536</td><td>0.474847</td><td>-0.352067</td><td>-0.024502</td><td>1.437105</td><td>0.566178</td><td>-0.232508</td><td>0.866236</td><td>...</td><td>-0.301982</td><td>-0.933816</td><td>0.475274</td><td>1.540994</td><td>0.698066</td><td>0.545091</td><td>-0.084405</td><td>-2.337347</td><td>1</td><td>0</td></tr><tr><th>3</th><td>treatment1</td><td>-2.156683</td><td>1.120198</td><td>0.174293</td><td>-1.741426</td><td>0.488993</td><td>0.638340</td><td>-0.721928</td><td>1.802134</td><td>1.097178</td><td>...</td><td>-2.129098</td><td>-1.183581</td><td>0.000318</td><td>1.105735</td><td>-0.629281</td><td>-0.737041</td><td>-1.525081</td><td>1.416042</td><td>0</td><td>0</td></tr><tr><th>4</th><td>control</td><td>-2.708572</td><td>-0.799698</td><td>-2.199595</td><td>0.574077</td><td>0.083142</td><td>-0.389140</td><td>1.492101</td><td>1.725202</td><td>1.194315</td><td>...</td><td>1.582041</td><td>-1.176077</td><td>1.686322</td><td>0.480035</td><td>1.780710</td><td>0.862094</td><td>0.128872</td><td>-2.851344</td><td>0</td><td>0</td></tr></tbody></table><p>5 rows Ã— 66 columns</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Look at the conversion rate and sample size in each group</span></span><br><span class="line">df.pivot_table(values=<span class="string">&#x27;conversion&#x27;</span>,</span><br><span class="line">               index=<span class="string">&#x27;treatment_group_key&#x27;</span>,</span><br><span class="line">               aggfunc=[np.mean, np.size],</span><br><span class="line">               margins=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead tr th {        text-align: left;    }    .dataframe thead tr:last-of-type th {        text-align: right;    }</style><table border="1" class="dataframe"><thead><tr><th></th><th>mean</th><th>size</th></tr><tr><th></th><th>conversion</th><th>conversion</th></tr><tr><th>treatment_group_key</th><th></th><th></th></tr></thead><tbody><tr><th>control</th><td>0.499050</td><td>100000</td></tr><tr><th>treatment1</th><td>0.599680</td><td>100000</td></tr><tr><th>All</th><td>0.549365</td><td>200000</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_names</span><br></pre></td></tr></table></figure><pre><code>[&#39;x1_informative&#39;, &#39;x2_informative&#39;, &#39;x3_informative&#39;, &#39;x4_informative&#39;, &#39;x5_informative&#39;, &#39;x6_informative&#39;, &#39;x7_informative&#39;, &#39;x8_informative&#39;, &#39;x9_informative&#39;, &#39;x10_informative&#39;, &#39;x11_irrelevant&#39;, &#39;x12_irrelevant&#39;, &#39;x13_irrelevant&#39;, &#39;x14_irrelevant&#39;, &#39;x15_irrelevant&#39;, &#39;x16_irrelevant&#39;, &#39;x17_irrelevant&#39;, &#39;x18_irrelevant&#39;, &#39;x19_irrelevant&#39;, &#39;x20_irrelevant&#39;, &#39;x21_irrelevant&#39;, &#39;x22_irrelevant&#39;, &#39;x23_irrelevant&#39;, &#39;x24_irrelevant&#39;, &#39;x25_irrelevant&#39;, &#39;x26_irrelevant&#39;, &#39;x27_irrelevant&#39;, &#39;x28_irrelevant&#39;, &#39;x29_irrelevant&#39;, &#39;x30_irrelevant&#39;, &#39;x31_irrelevant&#39;, &#39;x32_irrelevant&#39;, &#39;x33_irrelevant&#39;, &#39;x34_irrelevant&#39;, &#39;x35_irrelevant&#39;, &#39;x36_irrelevant&#39;, &#39;x37_irrelevant&#39;, &#39;x38_irrelevant&#39;, &#39;x39_irrelevant&#39;, &#39;x40_irrelevant&#39;, &#39;x41_irrelevant&#39;, &#39;x42_irrelevant&#39;, &#39;x43_irrelevant&#39;, &#39;x44_irrelevant&#39;, &#39;x45_irrelevant&#39;, &#39;x46_irrelevant&#39;, &#39;x47_irrelevant&#39;, &#39;x48_irrelevant&#39;, &#39;x49_irrelevant&#39;, &#39;x50_irrelevant&#39;, &#39;x51_uplift_increase&#39;, &#39;x52_uplift_increase&#39;, &#39;x53_uplift_increase&#39;, &#39;x54_uplift_increase&#39;, &#39;x55_uplift_increase&#39;, &#39;x56_uplift_increase&#39;, &#39;x57_uplift_increase&#39;, &#39;x58_uplift_increase&#39;, &#39;x59_increase_mix&#39;, &#39;x60_uplift_decrease&#39;, &#39;x61_uplift_decrease&#39;, &#39;x62_uplift_decrease&#39;, &#39;x63_uplift_decrease&#39;]</code></pre><h2><span id="feature-selection-with-filter-methods">Feature selection with Filter methods</span></h2><h3><span id="method-f-f-statistics">method = F (F statistics)</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter_f = FilterSelect() </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">method = <span class="string">&#x27;F&#x27;</span></span><br><span class="line">f_imp = filter_f.get_importance(df, X_names, y_name, method, </span><br><span class="line">                      treatment_group = <span class="string">&#x27;treatment1&#x27;</span>)</span><br><span class="line">print(f_imp)</span><br></pre></td></tr></table></figure><pre><code>      method              feature  rank        score        p_value  \0   F filter  x57_uplift_increase   1.0  1973.380496   0.000000e+00   0   F filter  x51_uplift_increase   2.0  1885.342364   0.000000e+00   0   F filter  x54_uplift_increase   3.0  1496.254091   0.000000e+00   0   F filter  x58_uplift_increase   4.0  1269.167710  4.224019e-277   0   F filter       x9_informative   5.0   677.066204  5.151887e-149   0   F filter  x63_uplift_decrease   6.0     9.108409   2.544691e-03   0   F filter  x61_uplift_decrease   7.0     5.978189   1.448472e-02   0   F filter       x19_irrelevant   8.0     5.295584   2.138059e-02   0   F filter       x46_irrelevant   9.0     5.237353   2.210792e-02   0   F filter       x27_irrelevant  10.0     4.573196   3.247713e-02   0   F filter       x11_irrelevant  11.0     4.297030   3.818027e-02   0   F filter       x39_irrelevant  12.0     4.009421   4.524803e-02   0   F filter       x42_irrelevant  13.0     3.788770   5.159896e-02   0   F filter  x60_uplift_decrease  14.0     3.089516   7.879975e-02   0   F filter  x53_uplift_increase  15.0     2.884902   8.941499e-02   0   F filter       x18_irrelevant  16.0     2.863763   9.059688e-02   0   F filter       x22_irrelevant  17.0     2.402012   1.211809e-01   0   F filter  x62_uplift_decrease  18.0     2.310073   1.285396e-01   0   F filter       x40_irrelevant  19.0     2.262581   1.325346e-01   0   F filter       x14_irrelevant  20.0     2.152103   1.423763e-01   0   F filter       x8_informative  21.0     1.947212   1.628892e-01   0   F filter       x33_irrelevant  22.0     1.691045   1.934648e-01   0   F filter       x47_irrelevant  23.0     1.622995   2.026761e-01   0   F filter       x28_irrelevant  24.0     1.525337   2.168150e-01   0   F filter       x7_informative  25.0     1.206987   2.719311e-01   0   F filter     x59_increase_mix  26.0     1.199216   2.734798e-01   0   F filter       x20_irrelevant  27.0     1.176234   2.781252e-01   0   F filter       x41_irrelevant  28.0     1.119234   2.900848e-01   0   F filter       x3_informative  29.0     1.011457   3.145553e-01   0   F filter       x16_irrelevant  30.0     0.999273   3.174877e-01   ..       ...                  ...   ...          ...            ...   0   F filter       x2_informative  34.0     0.775075   3.786527e-01   0   F filter       x45_irrelevant  35.0     0.746410   3.876164e-01   0   F filter       x31_irrelevant  36.0     0.670080   4.130248e-01   0   F filter  x55_uplift_increase  37.0     0.609454   4.349944e-01   0   F filter       x34_irrelevant  38.0     0.606343   4.361689e-01   0   F filter       x44_irrelevant  39.0     0.563659   4.527906e-01   0   F filter       x12_irrelevant  40.0     0.531649   4.659151e-01   0   F filter       x4_informative  41.0     0.412528   5.206899e-01   0   F filter       x26_irrelevant  42.0     0.348929   5.547207e-01   0   F filter       x48_irrelevant  43.0     0.348312   5.550711e-01   0   F filter       x25_irrelevant  44.0     0.333696   5.634916e-01   0   F filter       x24_irrelevant  45.0     0.330729   5.652307e-01   0   F filter       x23_irrelevant  46.0     0.327771   5.669751e-01   0   F filter  x52_uplift_increase  47.0     0.316966   5.734374e-01   0   F filter       x37_irrelevant  48.0     0.246766   6.193618e-01   0   F filter       x15_irrelevant  49.0     0.225643   6.347740e-01   0   F filter       x29_irrelevant  50.0     0.196632   6.574534e-01   0   F filter       x38_irrelevant  51.0     0.109701   7.404853e-01   0   F filter       x35_irrelevant  52.0     0.101365   7.501982e-01   0   F filter      x10_informative  53.0     0.094686   7.583024e-01   0   F filter       x21_irrelevant  54.0     0.056172   8.126528e-01   0   F filter       x43_irrelevant  55.0     0.043168   8.354093e-01   0   F filter       x13_irrelevant  56.0     0.013480   9.075699e-01   0   F filter       x49_irrelevant  57.0     0.008037   9.285639e-01   0   F filter       x17_irrelevant  58.0     0.005137   9.428651e-01   0   F filter       x30_irrelevant  59.0     0.004151   9.486301e-01   0   F filter       x50_irrelevant  60.0     0.001379   9.703808e-01   0   F filter       x36_irrelevant  61.0     0.001062   9.740069e-01   0   F filter       x6_informative  62.0     0.000428   9.834997e-01   0   F filter       x5_informative  63.0     0.000076   9.930457e-01                                  misc  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  ..                              ...  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  0   df_num: 1.0, df_denom: 199996.0  [63 rows x 6 columns]</code></pre><h3><span id="method-lr-likelihood-ratio-test">method = LR (likelihood ratio test)</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">method = <span class="string">&#x27;LR&#x27;</span></span><br><span class="line">lr_imp = filter_f.get_importance(df, X_names, y_name, method, </span><br><span class="line">                      treatment_group = <span class="string">&#x27;treatment1&#x27;</span>)</span><br><span class="line">print(lr_imp)</span><br></pre></td></tr></table></figure><pre><code>Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683135         Iterations 4Optimization terminated successfully.         Current function value: 0.570527         Iterations 6Optimization terminated successfully.         Current function value: 0.568449         Iterations 6Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683127         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683134         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683136         Iterations 4Optimization terminated successfully.         Current function value: 0.683136         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683136         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683129         Iterations 4Optimization terminated successfully.         Current function value: 0.683137         Iterations 4Optimization terminated successfully.         Current function value: 0.683135         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683137         Iterations 4Optimization terminated successfully.         Current function value: 0.683131         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683136         Iterations 4Optimization terminated successfully.         Current function value: 0.683125         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683136         Iterations 4Optimization terminated successfully.         Current function value: 0.683136         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683139         Iterations 4Optimization terminated successfully.         Current function value: 0.683137         Iterations 4Optimization terminated successfully.         Current function value: 0.683135         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683139         Iterations 4Optimization terminated successfully.         Current function value: 0.683137         Iterations 4Optimization terminated successfully.         Current function value: 0.683135         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683136         Iterations 4Optimization terminated successfully.         Current function value: 0.683126         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683135         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683133         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683139         Iterations 4Optimization terminated successfully.         Current function value: 0.683126         Iterations 4Optimization terminated successfully.         Current function value: 0.683113         Iterations 4Optimization terminated successfully.         Current function value: 0.683138         Iterations 4Optimization terminated successfully.         Current function value: 0.683134         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.678404         Iterations 4Optimization terminated successfully.         Current function value: 0.673481         Iterations 5Optimization terminated successfully.         Current function value: 0.683139         Iterations 4Optimization terminated successfully.         Current function value: 0.683139         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683136         Iterations 4Optimization terminated successfully.         Current function value: 0.678797         Iterations 4Optimization terminated successfully.         Current function value: 0.674807         Iterations 5Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683141         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683139         Iterations 4Optimization terminated successfully.         Current function value: 0.678449         Iterations 4Optimization terminated successfully.         Current function value: 0.673272         Iterations 5Optimization terminated successfully.         Current function value: 0.679964         Iterations 4Optimization terminated successfully.         Current function value: 0.676614         Iterations 5Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683140         Iterations 4Optimization terminated successfully.         Current function value: 0.683142         Iterations 4Optimization terminated successfully.         Current function value: 0.683134         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683128         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683137         Iterations 4Optimization terminated successfully.         Current function value: 0.683143         Iterations 4Optimization terminated successfully.         Current function value: 0.683120         Iterations 4       method              feature  rank        score   p_value   misc0   LR filter  x57_uplift_increase   1.0  2070.582853  0.000000  df: 10   LR filter  x51_uplift_increase   2.0  1969.081668  0.000000  df: 10   LR filter  x54_uplift_increase   3.0  1596.059562  0.000000  df: 10   LR filter  x58_uplift_increase   4.0  1339.970602  0.000000  df: 10   LR filter       x9_informative   5.0   830.925812  0.000000  df: 10   LR filter  x63_uplift_decrease   6.0     9.149363  0.002488  df: 10   LR filter  x61_uplift_decrease   7.0     6.013194  0.014199  df: 10   LR filter       x46_irrelevant   8.0     5.484790  0.019183  df: 10   LR filter       x19_irrelevant   9.0     5.345715  0.020773  df: 10   LR filter       x27_irrelevant  10.0     4.433104  0.035248  df: 10   LR filter       x11_irrelevant  11.0     4.422047  0.035477  df: 10   LR filter       x39_irrelevant  12.0     3.874147  0.049035  df: 10   LR filter       x42_irrelevant  13.0     3.735743  0.053260  df: 10   LR filter  x60_uplift_decrease  14.0     3.138529  0.076463  df: 10   LR filter  x53_uplift_increase  15.0     2.865203  0.090514  df: 10   LR filter       x18_irrelevant  16.0     2.840988  0.091888  df: 10   LR filter       x22_irrelevant  17.0     2.504072  0.113552  df: 10   LR filter  x62_uplift_decrease  18.0     2.317535  0.127923  df: 10   LR filter       x14_irrelevant  19.0     2.226707  0.135643  df: 10   LR filter       x40_irrelevant  20.0     2.197060  0.138274  df: 10   LR filter       x8_informative  21.0     1.884233  0.169854  df: 10   LR filter       x47_irrelevant  22.0     1.698939  0.192427  df: 10   LR filter       x33_irrelevant  23.0     1.676522  0.195387  df: 10   LR filter       x28_irrelevant  24.0     1.493135  0.221731  df: 10   LR filter       x7_informative  25.0     1.249156  0.263714  df: 10   LR filter     x59_increase_mix  26.0     1.197540  0.273814  df: 10   LR filter       x41_irrelevant  27.0     1.139287  0.285803  df: 10   LR filter       x20_irrelevant  28.0     1.109899  0.292104  df: 10   LR filter       x1_informative  29.0     1.014855  0.313743  df: 10   LR filter       x3_informative  30.0     0.992359  0.319166  df: 1..        ...                  ...   ...          ...       ...    ...0   LR filter       x32_irrelevant  34.0     0.764228  0.382009  df: 10   LR filter       x2_informative  35.0     0.732987  0.391917  df: 10   LR filter       x31_irrelevant  36.0     0.705755  0.400857  df: 10   LR filter  x55_uplift_increase  37.0     0.598018  0.439335  df: 10   LR filter       x44_irrelevant  38.0     0.590518  0.442219  df: 10   LR filter       x34_irrelevant  39.0     0.556178  0.455804  df: 10   LR filter       x12_irrelevant  40.0     0.549555  0.458499  df: 10   LR filter       x4_informative  41.0     0.392499  0.530989  df: 10   LR filter       x48_irrelevant  42.0     0.357407  0.549950  df: 10   LR filter       x26_irrelevant  43.0     0.346474  0.556116  df: 10   LR filter       x24_irrelevant  44.0     0.321736  0.570566  df: 10   LR filter       x23_irrelevant  45.0     0.316385  0.573789  df: 10   LR filter       x25_irrelevant  46.0     0.310967  0.577087  df: 10   LR filter  x52_uplift_increase  47.0     0.289571  0.590495  df: 10   LR filter       x37_irrelevant  48.0     0.267554  0.604977  df: 10   LR filter       x15_irrelevant  49.0     0.215249  0.642684  df: 10   LR filter       x29_irrelevant  50.0     0.192926  0.660492  df: 10   LR filter       x35_irrelevant  51.0     0.120501  0.728492  df: 10   LR filter       x38_irrelevant  52.0     0.116905  0.732416  df: 10   LR filter      x10_informative  53.0     0.107143  0.743421  df: 10   LR filter       x21_irrelevant  54.0     0.042848  0.836011  df: 10   LR filter       x43_irrelevant  55.0     0.036466  0.848557  df: 10   LR filter       x13_irrelevant  56.0     0.012543  0.910826  df: 10   LR filter       x17_irrelevant  57.0     0.011252  0.915523  df: 10   LR filter       x49_irrelevant  58.0     0.005559  0.940565  df: 10   LR filter       x36_irrelevant  59.0     0.002396  0.960964  df: 10   LR filter       x50_irrelevant  60.0     0.001808  0.966086  df: 10   LR filter       x30_irrelevant  61.0     0.000956  0.975339  df: 10   LR filter       x6_informative  62.0     0.000732  0.978420  df: 10   LR filter       x5_informative  63.0     0.000029  0.995726  df: 1[63 rows x 6 columns]</code></pre><h3><span id="method-kl-kl-divergence">method = KL (KL divergence)</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">method = <span class="string">&#x27;KL&#x27;</span></span><br><span class="line">kl_imp = filter_f.get_importance(df, X_names, y_name, method, </span><br><span class="line">                      treatment_group = <span class="string">&#x27;treatment1&#x27;</span>,</span><br><span class="line">                      n_bins=<span class="number">10</span>)</span><br><span class="line">print(kl_imp)</span><br></pre></td></tr></table></figure><pre><code>       method              feature  rank     score p_value                misc0   KL filter  x51_uplift_increase   1.0  0.026008    None  number_of_bins: 100   KL filter  x57_uplift_increase   2.0  0.023749    None  number_of_bins: 100   KL filter       x9_informative   3.0  0.020550    None  number_of_bins: 100   KL filter  x54_uplift_increase   4.0  0.018411    None  number_of_bins: 100   KL filter  x58_uplift_increase   5.0  0.014443    None  number_of_bins: 100   KL filter  x52_uplift_increase   6.0  0.002416    None  number_of_bins: 100   KL filter  x55_uplift_increase   7.0  0.000283    None  number_of_bins: 100   KL filter       x23_irrelevant   8.0  0.000221    None  number_of_bins: 100   KL filter     x59_increase_mix   9.0  0.000218    None  number_of_bins: 100   KL filter       x21_irrelevant  10.0  0.000206    None  number_of_bins: 100   KL filter       x15_irrelevant  11.0  0.000157    None  number_of_bins: 100   KL filter       x11_irrelevant  12.0  0.000155    None  number_of_bins: 100   KL filter       x46_irrelevant  13.0  0.000150    None  number_of_bins: 100   KL filter       x39_irrelevant  14.0  0.000146    None  number_of_bins: 100   KL filter  x53_uplift_increase  15.0  0.000142    None  number_of_bins: 100   KL filter      x10_informative  16.0  0.000137    None  number_of_bins: 100   KL filter       x2_informative  17.0  0.000135    None  number_of_bins: 100   KL filter       x31_irrelevant  18.0  0.000132    None  number_of_bins: 100   KL filter       x19_irrelevant  19.0  0.000130    None  number_of_bins: 100   KL filter       x40_irrelevant  20.0  0.000125    None  number_of_bins: 100   KL filter       x44_irrelevant  21.0  0.000124    None  number_of_bins: 100   KL filter  x61_uplift_decrease  22.0  0.000118    None  number_of_bins: 100   KL filter  x60_uplift_decrease  23.0  0.000118    None  number_of_bins: 100   KL filter  x63_uplift_decrease  24.0  0.000112    None  number_of_bins: 100   KL filter       x32_irrelevant  25.0  0.000109    None  number_of_bins: 100   KL filter       x35_irrelevant  26.0  0.000104    None  number_of_bins: 100   KL filter       x14_irrelevant  27.0  0.000102    None  number_of_bins: 100   KL filter       x38_irrelevant  28.0  0.000094    None  number_of_bins: 100   KL filter       x27_irrelevant  29.0  0.000091    None  number_of_bins: 100   KL filter       x33_irrelevant  30.0  0.000090    None  number_of_bins: 10..        ...                  ...   ...       ...     ...                 ...0   KL filter       x16_irrelevant  34.0  0.000083    None  number_of_bins: 100   KL filter       x34_irrelevant  35.0  0.000082    None  number_of_bins: 100   KL filter       x18_irrelevant  36.0  0.000076    None  number_of_bins: 100   KL filter       x36_irrelevant  37.0  0.000075    None  number_of_bins: 100   KL filter       x20_irrelevant  38.0  0.000074    None  number_of_bins: 100   KL filter       x4_informative  39.0  0.000073    None  number_of_bins: 100   KL filter       x26_irrelevant  40.0  0.000072    None  number_of_bins: 100   KL filter       x42_irrelevant  41.0  0.000071    None  number_of_bins: 100   KL filter       x8_informative  42.0  0.000071    None  number_of_bins: 100   KL filter       x6_informative  43.0  0.000071    None  number_of_bins: 100   KL filter  x62_uplift_decrease  44.0  0.000065    None  number_of_bins: 100   KL filter       x12_irrelevant  45.0  0.000063    None  number_of_bins: 100   KL filter       x5_informative  46.0  0.000062    None  number_of_bins: 100   KL filter       x1_informative  47.0  0.000060    None  number_of_bins: 100   KL filter       x49_irrelevant  48.0  0.000059    None  number_of_bins: 100   KL filter       x47_irrelevant  49.0  0.000058    None  number_of_bins: 100   KL filter       x48_irrelevant  50.0  0.000057    None  number_of_bins: 100   KL filter       x25_irrelevant  51.0  0.000057    None  number_of_bins: 100   KL filter       x22_irrelevant  52.0  0.000056    None  number_of_bins: 100   KL filter       x41_irrelevant  53.0  0.000049    None  number_of_bins: 100   KL filter       x37_irrelevant  54.0  0.000049    None  number_of_bins: 100   KL filter  x56_uplift_increase  55.0  0.000043    None  number_of_bins: 100   KL filter       x13_irrelevant  56.0  0.000039    None  number_of_bins: 100   KL filter       x50_irrelevant  57.0  0.000038    None  number_of_bins: 100   KL filter       x24_irrelevant  58.0  0.000036    None  number_of_bins: 100   KL filter       x29_irrelevant  59.0  0.000021    None  number_of_bins: 100   KL filter       x30_irrelevant  60.0  0.000020    None  number_of_bins: 100   KL filter       x17_irrelevant  61.0  0.000017    None  number_of_bins: 100   KL filter       x45_irrelevant  62.0  0.000013    None  number_of_bins: 100   KL filter       x7_informative  63.0  0.000011    None  number_of_bins: 10[63 rows x 6 columns]</code></pre><p>We found all these 3 filter methods were able to rank most of the <strong>informative</strong> and <strong>uplift increase</strong> features on the top.</p><h2><span id="performance-evaluation">Performance evaluation</span></h2><p>Evaluate the AUUC (Area Under the Uplift Curve) score with several uplift models when using top features dataset</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train test split</span></span><br><span class="line">df_train, df_test = train_test_split(df, test_size=<span class="number">0.2</span>, random_state=<span class="number">111</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convert treatment column to 1 (treatment1) and 0 (control)</span></span><br><span class="line">treatments = np.where((df_test[<span class="string">&#x27;treatment_group_key&#x27;</span>]==<span class="string">&#x27;treatment1&#x27;</span>), <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">print(treatments[:<span class="number">10</span>])</span><br><span class="line">print(df_test[<span class="string">&#x27;treatment_group_key&#x27;</span>][:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>[1 0 0 0 0 1 0 1 1 0]79114     treatment176043        control47617        control53169        control175702       control111635    treatment1129212       control19247     treatment149272     treatment1199314       controlName: treatment_group_key, dtype: object</code></pre><h3><span id="uplift-randomforest-classfier">Uplift RandomForest Classfier</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uplift_model = UpliftRandomForestClassifier(control_name=<span class="string">&#x27;control&#x27;</span>, max_depth=<span class="number">8</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using all features</span></span><br><span class="line">features = X_names </span><br><span class="line">uplift_model.fit(X = df_train[features].values, </span><br><span class="line">                 treatment = df_train[<span class="string">&#x27;treatment_group_key&#x27;</span>].values,</span><br><span class="line">                 y = df_train[y_name].values)</span><br><span class="line">y_preds = uplift_model.predict(df_test[features].values)</span><br></pre></td></tr></table></figure><h3><span id="select-top-n-features-based-on-kl-filter">Select top N features based on KL filter</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">top_n = <span class="number">10</span></span><br><span class="line">top_10_features = kl_imp[<span class="string">&#x27;feature&#x27;</span>][:top_n]</span><br><span class="line">print(top_10_features)</span><br></pre></td></tr></table></figure><pre><code>0    x51_uplift_increase0    x57_uplift_increase0         x9_informative0    x54_uplift_increase0    x58_uplift_increase0    x52_uplift_increase0    x55_uplift_increase0         x23_irrelevant0       x59_increase_mix0         x21_irrelevantName: feature, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">top_n = <span class="number">15</span></span><br><span class="line">top_15_features = kl_imp[<span class="string">&#x27;feature&#x27;</span>][:top_n]</span><br><span class="line">print(top_15_features)</span><br></pre></td></tr></table></figure><pre><code>0    x51_uplift_increase0    x57_uplift_increase0         x9_informative0    x54_uplift_increase0    x58_uplift_increase0    x52_uplift_increase0    x55_uplift_increase0         x23_irrelevant0       x59_increase_mix0         x21_irrelevant0         x15_irrelevant0         x11_irrelevant0         x46_irrelevant0         x39_irrelevant0    x53_uplift_increaseName: feature, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">top_n = <span class="number">20</span></span><br><span class="line">top_20_features = kl_imp[<span class="string">&#x27;feature&#x27;</span>][:top_n]</span><br><span class="line">print(top_20_features)</span><br></pre></td></tr></table></figure><pre><code>0    x51_uplift_increase0    x57_uplift_increase0         x9_informative0    x54_uplift_increase0    x58_uplift_increase0    x52_uplift_increase0    x55_uplift_increase0         x23_irrelevant0       x59_increase_mix0         x21_irrelevant0         x15_irrelevant0         x11_irrelevant0         x46_irrelevant0         x39_irrelevant0    x53_uplift_increase0        x10_informative0         x2_informative0         x31_irrelevant0         x19_irrelevant0         x40_irrelevantName: feature, dtype: object</code></pre><h4><span id="train-the-uplift-model-again-with-top-n-features">Train the Uplift model again with top N features</span></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using top 10 features</span></span><br><span class="line">features = top_10_features </span><br><span class="line"></span><br><span class="line">uplift_model.fit(X = df_train[features].values, </span><br><span class="line">                 treatment = df_train[<span class="string">&#x27;treatment_group_key&#x27;</span>].values,</span><br><span class="line">                 y = df_train[y_name].values)</span><br><span class="line">y_preds_t10 = uplift_model.predict(df_test[features].values)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using top 15 features</span></span><br><span class="line">features = top_15_features </span><br><span class="line"></span><br><span class="line">uplift_model.fit(X = df_train[features].values, </span><br><span class="line">                 treatment = df_train[<span class="string">&#x27;treatment_group_key&#x27;</span>].values,</span><br><span class="line">                 y = df_train[y_name].values)</span><br><span class="line">y_preds_t15 = uplift_model.predict(df_test[features].values)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using top 20 features</span></span><br><span class="line">features = top_20_features</span><br><span class="line"></span><br><span class="line">uplift_model.fit(X = df_train[features].values, </span><br><span class="line">                 treatment = df_train[<span class="string">&#x27;treatment_group_key&#x27;</span>].values,</span><br><span class="line">                 y = df_train[y_name].values)</span><br><span class="line">y_preds_t20 = uplift_model.predict(df_test[features].values)</span><br></pre></td></tr></table></figure><h3><span id="print-results-for-uplift-model">Print results for Uplift model</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">df_preds = pd.DataFrame([y_preds.ravel(), </span><br><span class="line">                         y_preds_t10.ravel(),</span><br><span class="line">                         y_preds_t15.ravel(),</span><br><span class="line">                         y_preds_t20.ravel(),</span><br><span class="line">                         treatments,</span><br><span class="line">                         df_test[y_name].ravel()],</span><br><span class="line">                        index=[<span class="string">&#x27;All&#x27;</span>, <span class="string">&#x27;Top 10&#x27;</span>, <span class="string">&#x27;Top 15&#x27;</span>, <span class="string">&#x27;Top 20&#x27;</span>, <span class="string">&#x27;is_treated&#x27;</span>, y_name]).T</span><br><span class="line"></span><br><span class="line">plot_gain(df_preds, outcome_col=y_name, treatment_col=<span class="string">&#x27;is_treated&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure><img src="/.io//jupyter_38_0.png" alt="png"><figcaption aria-hidden="true">png</figcaption></figure>]]></content:encoded>
      
      
      
      
      <comments>https://jaysung00.github.io/2020/12/17/jupyter/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ã€Uplift Modelingã€‘Uplift treeì— ê´€í•œ ë…¼ë¬¸ ë‚´ìš© ë²ˆì—­</title>
      <link>https://jaysung00.github.io/2020/12/17/Uplift-tree/</link>
      <guid>https://jaysung00.github.io/2020/12/17/Uplift-tree/</guid>
      <pubDate>Thu, 17 Dec 2020 01:53:17 GMT</pubDate>
      
      <description>&lt;p&gt;í•´ë‹¹ í¬ìŠ¤íŒ…ì€ Tree-based Uplift Modelingì— ê´€ë ¨ëœ ë…¼ë¬¸ ë° ë¬¸í—Œë“¤ì˜ ë²ˆì—­ìž„ì„ ë°íž™ë‹ˆë‹¤.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>í•´ë‹¹ í¬ìŠ¤íŒ…ì€ Tree-based Uplift Modelingì— ê´€ë ¨ëœ ë…¼ë¬¸ ë° ë¬¸í—Œë“¤ì˜ ë²ˆì—­ìž„ì„ ë°íž™ë‹ˆë‹¤.</p><a id="more"></a><hr><p><strong><a href="https://stochasticsolutions.com/pdf/sig-based-up-trees.pdf">Radcliffe &amp; Surry (2011)</a></strong></p><h2><span id="real-world-uplift-modelling-with-significance-based-uplift-trees">Real-World Uplift Modelling with Significance-Based Uplift Trees</span></h2><p><br></p><h4><span id="section61-tree-based-uplift-modeling">Section6.1 Tree-based Uplift Modeling</span></h4><p>CARTì™€ ê°™ì´ ë‘ ê°€ì§€ë¡œ ë¶„í• í•˜ëŠ” ì¼ë°˜ì  tree-based modelì˜ <code>split(ë¶„í• )</code>ì„ ê²°ì •í•˜ëŠ” ê¸°ì¤€ì˜ ì•„ëž˜ì˜ ë‘ê°€ì§€ì˜ ë°”ëžŒì§í•œ ì†ì„±ë“¤ì€ ì„œë¡œ trade-off ê´€ê³„ì— ìžˆë‹¤.</p><ul><li><p>ë‘ê°€ì§€ í•˜ìœ„ê·¸ë£¹ì˜ <code>outcome</code>ì„ ìµœëŒ€í™” í™”ëŠ” ê²ƒ</p></li><li><p>ê·¸ëŸ¬í•œ ë‘ê°€ì§€ ê·¸ë£¹ì˜ <code>size</code>ì˜ ì°¨ë¥¼ ìµœì†Œí™” í•˜ëŠ” ê²ƒ</p></li></ul><p>ì¼ë°˜ì ìœ¼ë¡œ ê·¹ë‹¨ì ì¸ <code>outcome</code>ì„ ë³´ì´ëŠ” ìž‘ì€ ê·¸ë£¹ì„ ì°¾ëŠ” ê²ƒì€ ì–´ë µì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ê²ƒë“¤ì€ ë³¸ì§ˆì ìœ¼ë¡œ ì¶©ëŒí•˜ëŠ” ê²½í–¥ì´ ìžˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¨ í•œëª…ì˜ êµ¬ë§¤ìž (100%ì˜ êµ¬ë§¤ìœ¨)ë§Œì„ <code>split</code>í•´ì„œ ë–¼ì–´ë‚´ëŠ” ê²½ìš°ë¥¼ ìƒê°í•´ë³¼ ìˆ˜ ìžˆë‹¤.</p><p>ìš°ë¦¬ëŠ” ê°™ì€ ê´€ì ìœ¼ë¡œ, ë‹¤ë§Œ ë¶„í• ëœ ê·¸ë£¹ë“¤ê°„ì˜ <code>outcome</code>ì˜ ì°¨ì´ê°€ ì•„ë‹Œ ë¶„í• ëœ ê·¸ë£¹ë“¤ê°„ì˜ <code>Uplift</code>ì˜ ì°¨ì´ë¥¼ Uplif treesì˜ split ì¡°ê±´ìœ¼ë¡œì¨ ì ‘ê·¼í•  ê²ƒì´ë‹¤.</p><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1094996802701618">Hansotia &amp; Rukstales (2001)</a>ì˜ ë°©ë²•ì€ Trade-offë¥¼ ë¬´ì‹œí•˜ê³  ì§ì ‘ì ìœ¼ë¡œ <code>Uplift</code>ì˜ ì°¨ì´(<span class="math inline">\(\Delta\Delta p\)</span>)ë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ì´ë‹¤. ìš°ë¦¬ëŠ” ì´ ì ‘ê·¼ë²•ìœ¼ë¡œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆë‹¤.</p><p>ìš°ë¦¬ëŠ” splitì˜ ê¸°ì¤€ìœ¼ë¡œì¨ Qinië¥¼ ì§ì ‘ì ìœ¼ë¡œ ì´ìš©í•˜ëŠ” ë°©ë²• ë˜í•œ ì‹œë„í–ˆë‹¤. QiniëŠ” ë¶„í• ëœ í•˜ìœ„ê·¸ë£¹ë“¤ì˜ Sizeì™€ Uplift ë³€í™”ë¥¼ ëª¨ë‘ ê³ ë ¤í•œë‹¤. ìš°ë¦¬ëŠ” ì „ì²´ì ì¸ Uplift modelì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ”ë° ìœ ìš©í•œ Qinië¥¼ ì°¾ì•˜ì§€ë§Œ ì´ê²ƒì„ splitì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”ë°ëŠ” ì œí•œì ì¸ ì„±ê³µë°–ì— ì–»ì§€ ëª»í•˜ì˜€ë‹¤. Qiniê°€ rankì˜ ìˆœì„œë¥¼ ë§¤ê¸°ëŠ” ê²ƒë§Œì„ ì¸¡ì •í•œë‹¤ëŠ” ì‚¬ì‹¤ì´ ì´ëŸ¬í•œ ê²°ê³¼ì˜ ì›ì¸ì´ ë˜ì—ˆì„ ê²ƒì´ë‹¤.</p><p>ë˜í•œ ê·¸ë£¹ê°„ì˜ sizeì˜ ì°¨ì´ë¥¼ ì¼ì¢…ì˜ penaltyë¡œ ê°„ì£¼í•˜ì—¬ ì›ëž˜ì˜ uplift ì°¨ì´ë¥¼ ì¡°ì •í•˜ëŠ” íŠ¹ë³„í•œ ì ‘ê·¼ë²•ì„ ì·¨í•  ìˆ˜ë„ ìžˆë‹¤. ë§Œì•½ ì›ëž˜ì˜ uplift ì°¨ì´ê°€ <span class="math inline">\(\Delta\)</span>ì´ê³  ë‘ê°œì˜ í•˜ìœ„ê·¸ë£¹ì˜ sizeê°€ <span class="math inline">\(N_L\)</span> <span class="math inline">\(N_R\)</span>ì´ë¼ê³  í•œë‹¤ë©´, ì–´ë– í•œ kê°’ì— ëŒ€í•˜ì—¬ penaltyë¥¼ ë¶€ì—¬ë°›ì€ split ì¡°ê±´ì˜ í›„ë³´ëŠ” ì´í•˜ì™€ ê°™ì„ ìˆ˜ ìžˆë‹¤.</p><p><span class="math inline">\(\Delta / (\cfrac{N_L+N_R}{2min(N_L,N_R)})^k\)</span></p><p>ì´ ë¶„ëª¨ì˜ penaltyëŠ” í•˜ìœ„ê·¸ë£¹ì˜ sizeê°€ ì„œë¡œ ê°™ì„ ë•Œ 1ì„ ê°–ê³ , ì„œë¡œ ë‹¬ë¼ì§ˆ ìˆ˜ ë¡ í° ê°’ì„ ê°–ëŠ”ë‹¤. (ê²°ê³¼ì ìœ¼ë¡œ penaltyë¥¼ ë°›ì€ upliftëŠ” ì›ëž˜ë³´ë‹¤ ìž‘ì€ ê°’ìœ¼ë¡œ í‰ê°€ëœë‹¤.)</p><p>ì–´ë– í•œ kì— ëŒ€í•œ ë˜ë‹¤ë¥¸ penaltyì˜ ëŒ€ì•ˆì€ ì´í•˜ì™€ ê°™ë‹¤.</p><p><span class="math inline">\(\Delta (1- {\large \lvert \frac{N_L-N_R}{N_L+N_R} \rvert}^k)\)</span></p><p>ì—¬ê¸°ì„œ penaltyëŠ” í•˜ìœ„ê·¸ë£¹ì˜ sizeê°€ ê°™ì„ ë•Œ 0ì´ ë˜ê³  ì‚¬ì´ì¦ˆê°€ ë‹¬ë¼ì§€ë©´ì„œ 1ì— ê°€ê¹Œì›Œ ì§„ë‹¤. (ë‘ ê²½ìš° ëª¨ë‘ këŠ” ê²½í—˜ì ìœ¼ë¡œ ì„¤ì •ë˜ì–´ì•¼ í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì´ë‹¤.)</p><p>ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ëŠ” ì‹¤ì œ ì„¸ìƒì˜ ì–´ë– í•œ ë¬¸ì œì— ëŒ€í•´ì„œë„ ìž˜ ìž‘ë™í•˜ëŠ” penaltyë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤.</p><hr><p><strong><a href="https://tech.wayfair.com/data-science/2019/10/modeling-uplift-directly-uplift-decision-tree-with-kl-divergence-and-euclidean-distance-as-splitting-criteria/">George Fei's blog (2019)</a></strong></p><h2><span id="modeling-uplift-directly-uplift-decision-tree-with-kl-divergence-and-euclidean-distance-as-splitting-criteria">Modeling Uplift Directly: Uplift Decision Tree with KL Divergence and Euclidean Distance as Splitting Criteria</span></h2><p><br></p><h4><span id="-uplift-decision-treeì˜-ë’¤ì—-ìžˆëŠ”-ì´ë¡ ì—-ëŒ€í•´">- Uplift Decision Treeì˜ ë’¤ì— ìžˆëŠ” ì´ë¡ ì— ëŒ€í•´</span></h4><p>ê°ê°ì˜ ë¶„í• ê¸°ì¤€(split criterion)ì´ ë‹¤ë¥¸ ì—¬ëŸ¬ Uplift Decision Tree ì•Œê³ ë¦¬ì¦˜ì´ ì¡´ìž¬í•œë‹¤.</p><p><strong>ì—¬ê¸°ì„œ, ìš°ë¦¬ëŠ” <a href="https://www.semanticscholar.org/paper/Uplift-Modeling-in-Direct-Marketing-Rzepakowski-Jaroszewicz/e979ba084f34345b2ac8783df2b4a3295ae9273f">Piotr Rzepakowski &amp; Szymon Jaroszewicz (2012)</a>ì— ë“±ìž¥í•˜ëŠ” ì •ë³´ ì´ë¡ ì  ë¶„í• ê¸°ì¤€(information theoretical splitting criteria)ì— ëŒ€í•´ ë…¼ì˜í•  ê²ƒì´ë‹¤.</strong></p><p>Single treatment uplift decision treeì˜ ê²½ìš°ì—, ê° nodeëŠ” ë‘ê°œì˜ ë¶„ë¦¬ëœ outcome ì„ í¬í•¨í•œë‹¤. í•˜ë‚˜ëŠ” ì¹˜ë£Œêµ°ì˜ ê²°ê³¼ì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ëŒ€ì¡°êµ°ì˜ ê²°ê³¼ì´ë‹¤.</p><p>Upliftë¥¼ ìµœëŒ€í™” í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” Treeë¥¼ íƒ€ê³  ë‚´ë ¤ê°€ë©´ì„œ ë¶„í• ë˜ëŠ” ë‘ê°€ì§€ì˜ ë¶„í¬ë¥¼ ìµœëŒ€í•œ ë‹¤ë¥´ê²Œ ë§Œë“¤ê³ ìž í•œë‹¤.</p><p>ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°(<a href="https://en.wikipedia.org/wiki/Relative_entropy">Kullback-Leibler divergence</a>)ê³¼ ìœ í´ë¦¬ë“œ ì œê³±ê±°ë¦¬(squared Euclidean Distance)ëŠ” ì •ë³´ì´ë¡ ì—ì„œ ë¶„í¬ê°„ì˜ divergenceë¥¼ ì¸¡ì •í•˜ëŠ” ë‘ê°€ì§€ ë°©ë²•ì´ë‹¤.</p><blockquote><p><strong>Equation 1.</strong><br><span class="math inline">\(KL(P:Q) = \sum_ip_ilog\cfrac{p_i}{q_i}\space\space\space\space\space\space\space\space\space\space\)</span> [Kullback-Leibler Divergence between Two Distributions]<br><span class="math inline">\(E(P:Q) = \sum_i(p_i-q_i)^2\space\space\space\space\space\space\space\space\space\space\)</span>[Squared Euclidean Distance between Two Distributions]</p></blockquote><ul><li>ì•„ëž˜ì²¨ìž <span class="math inline">\(i\)</span>ëŠ” ê°ê°ì˜ outcome class</li><li><span class="math inline">\(p_i\)</span>ì™€ <span class="math inline">\(q_i\)</span>ëŠ” ê°ê° ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ°ì— ìžˆì–´ì„œ outcome class <span class="math inline">\(i\)</span>ê°€ ë  í™•ë¥ </li></ul><p><br></p><p>Treeì—ì„œ ë¶„í• ë˜ì–´ì•¼ í•  nodeì—ì„œ, ìœ„ì˜ ë‘ê°€ì§€ ë°©ë²• ì¤‘ í•˜ë‚˜ë¥¼ ì´ìš©í•´ outcomeë¶„í¬ì˜ divergenceë¥¼ ê³„ì‚°í•  ìˆ˜ ìžˆë‹¤. tree nodeë¥¼ children nodesë¡œ ë¶„í• ì‹œí‚¤ëŠ” 'A' test ì´í›„ì— ìš°ë¦¬ëŠ” 'A' testì˜ ì¡°ê±´í•˜ì—ì„œ outcome class distributionì˜ conditional divergenceë¥¼ ë¹„ìŠ·í•˜ê²Œ ì¸¡ì •í•  ìˆ˜ ìžˆë‹¤.</p><blockquote><p><strong>Equation 2.</strong> [Conditional Divergence for a Given 'A' Test]<br><span class="math inline">\(D{\large(}P^T(Y):P^C(Y)|A{\large)} = \sum_a \cfrac{N(a)}{N}D{\large(}P^T(Y|a):P^C(Y|a)\large)\)</span></p></blockquote><ul><li><span class="math inline">\(a\)</span> : ê° ìžì‹ ë…¸ë“œ</li><li><span class="math inline">\(N\)</span> : ë¶€ëª¨ë…¸ë“œì— ìžˆëŠ” ê°œì²´(instance)ë“¤ì˜ ì´ ìˆ˜</li><li><span class="math inline">\(N(a)\)</span> : ìžì‹ë…¸ë“œ <span class="math inline">\(a\)</span>ì— ìžˆëŠ” ê°œì²´ë“¤ì˜ ìˆ˜</li><li><span class="math inline">\(D\)</span> : divergence measure</li><li><span class="math inline">\(P^T(Y) , P^C(Y)\)</span> : outcome class distribution</li><li><span class="math inline">\(|a\)</span> : ìžì‹ë…¸ë“œ <span class="math inline">\(a\)</span>ì˜ outcome class distribution</li></ul><p><br></p><p>ìµœì ì˜ ë¶„í• ë¥¼ ìœ„í•´, ìš°ë¦¬ëŠ” ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ° ì‚¬ì´ì˜ outcome class distibution ê°„ì˜ divergenceì˜ ì¦ê°€ë¶„(gain)ë¥¼ ìµœëŒ€í™” í•˜ë ¤ê³  í•œë‹¤. ë°”ê¿”ë§í•˜ìžë©´, ì•„ëž˜ì™€ ê°™ì€ ì‹ì„ ìµœëŒ€í™” í•  ê²ƒì´ë‹¤.</p><blockquote><p><strong>Equation 3.</strong> [Gain in Class Distribution Divergence for a Given 'A' Test]<br><br> <span class="math inline">\(D_{gain}(A) = D{\large(}P^T(Y):P^C(Y)|A{\large)}-D{\large(}P^T(Y):P^C(Y){\large)}\)</span></p></blockquote><ul><li>ìš°ë³€ì˜ ì²«ë²ˆì§¸ í•­ì€ ìœ„ì—ì„œ ì–¸ê¸‰ëœ Conditional Divergence for a Given 'A' Test</li><li>ìš°ë³€ì˜ ë‘ë²ˆì§¸ í•­ì€ ë¶€ëª¨ë…¸ë“œì˜ outcome class distribution divergence</li></ul><p><br></p><h4><span id="-ì´-ì´ë¡ ì„-ì„¤ëª…í•˜ê¸°-ìœ„í•œ-example">- ì´ ì´ë¡ ì„ ì„¤ëª…í•˜ê¸° ìœ„í•œ Example</span></h4><p>Uplift decision treeê°€ ì¹˜ë£Œì™€ ëŒ€ì¡° class distributionì˜ ì¦ê°€ë¶„ì„ ìµœëŒ€í™”í•˜ì—¬ nodeë¥¼ ì–´ë–»ê²Œ ë¶„í• ì‹œí‚¤ëŠ”ì§€ ë” ìž˜ ì„¤ëª…í•˜ê¸° ìœ„í•´ ë‹¤ìŒì˜ ì˜ˆë¥¼ ì œì‹œí•œë‹¤.</p><p>ì£¼ì–´ì§„ Tree nodeì— 8ëª…ì˜ ê³ ê°ì— í•´ë‹¹í•˜ëŠ” ì´ 8ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ ìžˆìœ¼ë©°, ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ°ì— ê°ê° 4ëª…ì˜ ê³ ê°ì´ ìžˆë‹¤ê³  ìƒìƒí•´ë³´ìž. ì¹˜ë£Œ ê·¸ë£¹ì˜ ê³ ê° 4ëª…ì¤‘ 3ëª…ì´ ìº íŽ˜ì¸ì— ë°˜ì‘(ì „í™˜; convert)í–ˆê³ , ëŒ€ì¡° ê·¸ë£¹ì˜ ê³ ê° 4ëª… ì¤‘ 2ëª…ì´ ë°˜ì‘ì„ í•˜ì˜€ë‹¤. ìš°ë¦¬ëŠ” ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ° ì‚¬ì´ì˜ outcome class distibution ê°„ì˜ divergenceì˜ ì¦ê°€ë¶„(gain)ë¥¼ ìµœëŒ€í™” í•˜ë„ë¡ ì´ ë…¸ë“œë¥¼ ë¶„í• í•˜ëŠ” ë°©ë²•ì„ ì°¾ê³  ì‹¶ë‹¤.</p><p><img src="https://i.imgur.com/CUUFwx3.png" width="500px"><br>[ìžì‹ë…¸ë“œì˜ ê²°ê³¼ì— ì¡´ìž¬í•˜ëŠ” ë¶„í¬ ì°¨ì´ë¥¼ ìµœëŒ€í™”í•˜ì—¬ decision tree nodeê°€ ë¶„í•  ë˜ëŠ” ì˜ˆ]</p><p>ì´ë¡ ì ìœ¼ë¡œ Uplift decision treeì˜ ë¶„í• ê¸°ì¤€ì€ ì—¬ëŸ¬ê°€ì§€ì˜ ë¶„í• (Multiway splits)ì™€ í˜¸í™˜ëœë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ì˜ˆì—ì„œì™€ ê°™ì´ ë¶„í• ì´ ë‘ê°œì˜ í•˜ìœ„ ë…¸ë“œë§Œ ìƒì„±í•˜ëŠ” ì´ì§„ ë¶„í• ì´ ë” ì¼ë°˜ì ì´ë‹¤. ìš°ë¦¬ëŠ” ìœ„ì˜ ê·¸ë¦¼ì— ë‚˜íƒ€ë‚œ ë¶„í• ê°€ ìš°ë¦¬ê°€ ì°¾ê³ ìžˆëŠ” ìµœì ì˜ ë¶„í• ì´ë¼ê³  ì£¼ìž¥í•˜ê³ ìž í•œë‹¤. ì´ëŸ¬í•œ ì£¼ìž¥ì„ ì¦ëª…í•˜ê¸° ìœ„í•´, ë¨¼ì € ìœ í´ë¦¬ë“œ ì œê³±ê±°ë¦¬(squared Euclidean Distance)ë¥¼ í†µí•´ divergenceë¥¼ ì¸¡ì •í•˜ë³´ìž.</p><p><br><br>1. ë¨¼ì € ë¶€ëª¨ë…¸ë“œì˜ class distributionì˜ divergenceë¥¼ êµ¬í•œë‹¤.</p><p><span class="math inline">\(D{\large(}P^T(Y):P^C(Y){\large)} = \sum_{\large i \in \{converted, not\space converted \}}(p_i - q_i)^2 = (0.75 - 0.5)^2 + (0.25 - 0.5)^2 = 0.125\)</span></p><p>ì´ê²ƒì€ ë‹¨ìˆœížˆ ë°˜ì‘ë¥ (ì „í™˜ë¥ ; conversion rate)ì˜ ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ° ê°„ ìœ í´ë¦¬ë“œ ì œê³±ê±°ë¦¬ì™€ ë¹„ë°˜ì‘ë¥ (ë¹„ì „í™˜ë¥ ; non-conversion rate)ì˜ ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ° ê°„ì˜ ìœ í´ë¦¬ë“œ ì œê³±ê±°ë¦¬ ë”í•œ ê°’ì´ë‹¤.</p><p><br><br>2. ë¶€ëª¨ë…¸ë“œì—ì„œì™€ ê°™ì´ ë¶„í• ëœ ë‘ê°œì˜ ìžì‹ë…¸ë“œì—ì„œë„ class distribution divergenceë“¤ì„ ê°ê° êµ¬í•œë‹¤.</p><p><em>In left child node</em> , <span class="math inline">\(\sum_{\large i \in \{converted, not\space converted \}}(p_i - q_i)^2 = (1 - 0)^2 + (0 - 1)^2 = 2\)</span></p><p><em>In right child node</em> , <span class="math inline">\(\sum_{\large i \in \{converted, not\space converted \}}(p_i - q_i)^2 = (0 - 1)^2 + (1 - 0)^2 = 2\)</span></p><p><br><br>3. ë¶„í• ë¥¼ í†µí•œ divergenceì˜ ê°œì„ ì— ë¯¸ì¹œ ë‘ ìžì‹ë…¸ë“œì˜ ìƒëŒ€ì ì¸ ì˜í–¥ì„ ì •ê·œí™”í•˜ê¸° ìœ„í•´, ë¶„í• ì˜ ì¡°ê±´í•˜ì—ì„œ outcome class distributionì˜ conditional divergenceë¥¼ êµ¬í•œë‹¤. (Eq.2)</p><p><span class="math inline">\(D{\large(}P^T(Y):P^C(Y)|A{\large)} = \sum_{\large a \in \{left\space child,\space right\space child \} }\cfrac{N(a)}{N} D{\large (} P^T(Y|a) : P^C(Y|a){\large)} = \cfrac{5}{8}\cdot2 + \cfrac{3}{8}\cdot2 = 2\)</span></p><p><br><br>4. ë¶„í• ê°€ ì´ë£¨ì–´ì¡Œì„ ë•Œ, ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ° ì‚¬ì´ì˜ outcome class distibution ê°„ì˜ divergenceì˜ ì¦ê°€ë¶„(gain)ì„ êµ¬í•œë‹¤. (Eq.3)</p><p><span class="math inline">\(D_{gain}(A) = D{\large(}P^T(Y):P^C(Y)|A{\large)}-D{\large(}P^T(Y):P^C(Y){\large)} = 2-0.125 = 1.875\)</span></p><p>ë‘ ê°œì˜ ìžì†ì˜ class distributionì´ ê°€ìž¥ ë‹¤ë¥¼ ë•Œ (ì¦‰ divergenceê°€ ê°€ìž¥ í´ ë•Œ) ë‘ ìžì†ì˜ ìœ í´ë¦¬ë“œ ì œê³±ê±°ë¦¬ê°€ ê°€ìž¥ ìµœëŒ€í™” ë˜ë¯€ë¡œ, ìœ„ì˜ ê³„ì‚° ê²°ê³¼ëŠ” ìµœëŒ€ê°’ì´ë¼ê³  í•  ìˆ˜ìžˆë‹¤.</p><p><br></p><p>ë‹¤ì‹œ ë§í•´, <strong>ì™¼ìª½ ìžì‹ë…¸ë“œì˜ í´ëž˜ìŠ¤ ë¶„í¬ëŠ” ì¹˜ë£Œêµ°ê³¼ í†µì œêµ°ì„ ë¹„êµí–ˆì„ ë•Œ ê°€ìž¥ <code>Persuadables</code>ì— ê°€ê¹Œìš´ ë¶„í¬ê°€ ë˜ê²Œë” í•˜ê³ , ì˜¤ë¥¸ìª½ ìžì‹ë…¸ë“œì˜ í´ëž˜ìŠ¤ ë¶„í¬ëŠ” ê°€ìž¥ <code>Sleeping Dogs</code>ì— ê°€ê¹Œìš´ ë¶„í¬ê°€ ë˜ê²Œë” í•˜ëŠ” splitì„ ì„ íƒí•˜ëŠ” ê²ƒì´ë‹¤.</strong></p><p>ëª¨ë¸ì´ í›ˆë ¨ë  ë•Œ, ì´ìš©ê°€ëŠ¥í•œ íŠ¹ì§•ëŸ‰ì˜ ì—¬ëŸ¬ê°€ì§€ ë‹¤ì–‘í•œ ê°’ì— ëŒ€í•´ ì—¬ëŸ¬ splitì„ ë°˜ë³µí•˜ë©´ì„œ ì´ëŸ¬í•œ ìµœì í™” splitì´ ì°¾ì•„ì§ˆ ê²ƒì´ë‹¤.</p><p><br></p><h4><span id="-ìž ìž¬ì ì¸-ì¤€æº–ìµœì -ë¶„í• -potential-suboptimal-splits">- ìž ìž¬ì ì¸ ì¤€(æº–)ìµœì  ë¶„í•  (Potential Suboptimal Splits)</span></h4><p>ìœ„ì—ì„œ ì œì‹œí•œ ë¶„í•  ì „ëžµì— ì¡´ìž¬í•  ìˆ˜ ìžˆëŠ” ì´í•˜ì˜ <em>ë‘ê°€ì§€ ìž ìž¬ì ì¸ ë¬¸ì œ</em> ì— ëŒ€ì²˜í•´ì•¼í•  í•„ìš”ê°€ ìžˆë‹¤.</p><p><br> 1. ê³ ë¥´ì§€ ëª»í•œ ì¹˜ë£Œ/í†µì œêµ°ì˜ ë¶„í• </p><p>ë¨¼ì €, ê³ ë¥´ì§€ ëª»í•œ ì¹˜ë£Œ/í†µì œêµ°ì˜ ë¶„í• ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ëŒ€ë¶€ë¶„ì˜ ì¹˜ë£Œê°œì²´ë“¤ì„ í•˜ë‚˜ì˜ í•˜ìœ„ë‚˜ë¬´ì— ì§‘ì–´ë„£ê³  ê·¸ ë‚˜ë¬´ì— ê±°ì˜ í†µì œí´ëž˜ìŠ¤ì˜ ê°œì²´ë“¤ì´ ì¡´ìž¬í•˜ì§€ ì•Šì„ë•Œ ì¼ì–´ë‚œë‹¤. ì´ê²ƒì€ ë¶„í• ì— ì‚¬ìš©ëœ íŠ¹ì§•ëŸ‰ì´ ì¹˜ë£Œí• ë‹¹ ë¼ë²¨ê³¼ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì¦‰ Uplift modelingì˜ í•„ìˆ˜ì ì¸ ê°€ì •ì¸ <strong>Unconfounded assumption</strong> ì´ ìœ„ë°˜ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”ìš±ì´ ëª¨ë“  leaf nodeëŠ” ì¶©ë¶„í•œ ì¹˜ë£Œì™€ í†µì œ ê°œì²´ë“¤ì„ í¬í•¨í•´ì•¼ í•˜ë¯€ë¡œ ì´ëŸ¬í•œ ë¶„í• ëŠ” ì•žìœ¼ë¡œì˜ ë¶„í• ë¥¼ ë”ìš± ì–´ë µê²Œ ë§Œë“¤ ê²ƒì´ë‹¤.</p><p><br> 2. ì—¬ëŸ¬ê°œì˜ ìžì†ë…¸ë“œë¡œ ë‚˜ëˆ„ëŠ” ë¶„í• ë¥¼ ì„ íƒí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ ê²½í–¥ì„±</p><p>ì—¬ëŸ¬ê°œì˜ ìžì†ë…¸ë“œë¡œ ë‚˜ëˆ„ëŠ” ë¶„í• ëŠ”, í›ˆë ¨ë°ì´í„°ì— ì ìš©ë˜ì—ˆì„ ë•Œ ë” ë†’ì€ ì¦ê°€ë¶„(gain)ì„ ê°–ëŠ” ê²½í–¥ì„±ì´ ìžˆê¸° ë•Œë¬¸ì— ë°œìƒí•œë‹¤. ê·¸ëŸ¬ë‚˜ í…ŒìŠ¤íŠ¸ë°ì´í„°ì—ëŠ” ì œëŒ€ë¡œ ì¶”ë¡ í•´ë‚´ì§€ëª»í•˜ë©° ê²°ê³¼ì ìœ¼ë¡œ overfittingì„ ë°œìƒì‹œí‚¨ë‹¤.</p><p>ì•„ëž˜ì— ì œì‹œëœ ì •ê·œí™”ì¸ìˆ˜(Normalization factor)ë“¤ì€ ì•žì— ì–¸ê¸‰í•œ ë°”ì´ì–´ìŠ¤ë¥¼ ì‹œì •í•˜ê¸° ìœ„í•´ ê³ ì•ˆë˜ì—ˆë‹¤. ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°ê³¼ ìœ í´ë¦¬ë“œ ì œê³±ê±°ë¦¬ë¥¼ í†µí•œ ë‘ê°€ì§€ ë¶„í• ê¸°ì¤€ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì´ ë˜í•œ ë‘ê°€ì§€ ë‹¤ë¥¸ ìœ í˜•ì˜ ì •ê·œí™”ê°’ì„ ê°€ì§„ë‹¤.</p><blockquote><p><strong>Equation 4</strong> [Normalization Value for Splitting Based on KL Divergence]<br><span class="math inline">\(I(A)=H(\cfrac{N^T}{N},\cfrac{N^C}{N})KL(P^T(A):P^C(A))+\cfrac{N^T}{N}H(P^T(A))+\cfrac{N^C}{N}H(P^C(A))+\cfrac{1}{2}\)</span></p></blockquote><blockquote><p><strong>Equation 5</strong> [Normalization Value for Splitting Based on Euclidean Distance]<br><span class="math inline">\(J(A) = Gini(\cfrac{N^T}{N},\cfrac{N^C}{N})E(P^T(A):P^C(A))+\cfrac{N^T}{N}Gini(P^T(A))+\cfrac{N^C}{N}Gini(P^C(A))+\cfrac{1}{2}\)</span></p></blockquote><p>ë‘ penaltyí•­ì´ ìƒë‹¹ížˆ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì—, ì—¬ê¸°ì„œëŠ” ìœ í´ë¦¬ë“œ ì œê³±ê±°ë¦¬ì— ëŒ€í•´ì„œë§Œ ë…¼í•  ê²ƒì´ë‹¤. (Eq.5)</p><p><span class="math inline">\(J(A)=\)</span></p><p>â…°. <span class="math inline">\(Gini(\cfrac{N^T}{N},\cfrac{N^C}{N})\cdot E(P^T(A):P^C(A))\)</span></p><ul><li><p>ì²«ë²ˆì§¸ í•­ì€ ê³ ë¥´ì§€ ëª»í•œ ì¹˜ë£Œ/í†µì œêµ°ì˜ ë¶„í• ì„ ì˜ˆë°©í•œë‹¤.</p></li><li><p>ì´ í•­ì˜ ì•žë¶€ë¶„ì€ ë¶€ëª¨ë…¸ë“œì˜ ì§€ë‹ˆ ë¶ˆìˆœë„(Gini impurity)<span class="math inline">\(^{[*1]}\)</span>ì´ê³ , ë¶€ëª¨ë…¸ë“œì˜ ì¹˜ë£Œ/í†µì œì˜ ë¶ˆê· í˜•ì´ í´ ìˆ˜ë¡ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ê°’ì´ë‹¤. ì´ë ‡ê²Œ ì„¤ì •ëœ ì´ìœ ëŠ” ë¶€ëª¨ë…¸ë“œì— ì´ë¯¸ í° ì¹˜ë£Œ/í†µì œ ë¶ˆê· í˜•ì´ ì¡´ìž¬í•˜ëŠ” ê²½ìš°, ê²°ê³¼ì ìœ¼ë¡œ ë”°ë¼ì˜¤ëŠ” ìžì‹ë…¸ë“œì˜ ë¶ˆê· í˜•ì— ëŒ€í•´ ê³„ì†ì ìœ¼ë¡œ ì²˜ë²Œí•˜ëŠ” ê²ƒì€ ê³µì •í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.</p></li><li><p>ì´ í•­ì˜ ë’·ë¶€ë¶„ì€ ëª¨ë“  ìžì‹ë…¸ë“œì—ì„œì˜ ì¹˜ë£Œë¹„ìœ¨ê³¼ í†µì œë¹„ìœ¨ê°„ì˜ ìœ í´ë¦¬ë“œ ì œê³±ê±°ë¦¬ì´ë‹¤. ì´ ê°’ì€ ëª¨ë“  ìžì‹ë…¸ë“œì—ì„œ ë‘ ë¹„ìœ¨ì´ ë™ì¼í•œ ê²½ìš°ì—ë§Œ ìµœì†Œí™”ëœë‹¤.</p></li></ul><p><br></p><p>â…±. <span class="math inline">\(+\cfrac{N^T}{N}Gini(P^T(A))+\cfrac{N^C}{N}Gini(P^C(A))\)</span></p><ul><li><p>ì´ì–´ì§€ëŠ” ë‘ í•­ì€ ì—¬ëŸ¬ê°œì˜ ìžì†ë…¸ë“œë¡œ ë¶„í• ë˜ëŠ” ê²ƒì— ëŒ€í•´ penaltyë¥¼ ë¶€ì—¬í•œë‹¤.</p></li><li><p>ì´ëŠ” ê°™ì€ ë¬¸ì œë¥¼ ì „í†µì ì¸ decision tree algorithmì´ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ê³¼ ë¹„ìŠ·í•˜ë‹¤.</p></li><li><p>ì§€ë‹ˆ ë¶ˆìˆœë„ëŠ” ë¶„í• ì— ì˜í•´ ìžì†ë…¸ë“œì˜ ê°œìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¦ê°€í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‘ê°œì˜ ë™ë“±í•œ ìžì†ë…¸ë“œë¡œ ë¶„í• ê°€ ì´ë£¨ì–´ ì§ˆ ë•Œ <span class="math inline">\(1 - 0.5^2 -0.5^2 = 0.5\)</span> ê°€ ë˜ê³ , 4ê°œì˜ ë™ë“±í•œ ìžì†ë…¸ë“œë¡œ ë¶„í• ë  ë•Œ <span class="math inline">\(1-4\cdot 0.25^4 = 0.75\)</span>ê°€ ëœë‹¤.</p></li></ul><p><br></p><p>â…². <span class="math inline">\(+\cfrac{1}{2}\)</span></p><ul><li>ë§ˆì§€ë§‰ <span class="math inline">\(\cfrac{1}{2}\)</span> í•­ì€. ìž‘ì€ ì¦ê°€ë¶„(gain)ì„ ê°–ì§€ë§Œ ìž‘ì€ ì •ê·œí™”ìš”ì¸ìœ¼ë¡œ ë‚˜ëˆ”ìœ¼ë¡œì¨ ê³¼ëŒ€í‰ê³¼ë˜ëŠ” ë¶„í• ë¥¼ ì„ í˜¸í•˜ì§€ ì•Šê¸° ìœ„í•´ ì¡´ìž¬í•œë‹¤.</li></ul><p><br></p><p><span style="font-size: 85%"> <span class="math inline">\(^{[*1]}:\)</span> ì§€ë‹ˆ ë¶ˆìˆœë„ëŠ” ë¶€ë¶„ ì§‘í•©ì—ì„œ ë¬´ìž‘ìœ„ë¡œ ì„ íƒí•œ ì›ì†Œê°€ ë¶€ë¶„ ì§‘í•©ì˜ ë¼ë²¨ ë¶„í¬ì— ë”°ë¼ ë¬´ìž‘ìœ„ë¡œ ë¼ë²¨ì„ ë¶™ì¸ ê²½ìš° ì–¼ë§ˆë‚˜ ìžì£¼ ìž˜ëª» ë¼ë²¨ì„ ë¶™ì´ëŠ”ì§€ë¥¼ ì¸¡ì •í•œ ê²ƒì´ë‹¤. ë¼ë²¨ <span class="math inline">\(i\)</span>ê°€ ì„ íƒë˜ëŠ” í™•ë¥  <span class="math inline">\(p_i\)</span>ê³¼ ì‹¤ìˆ˜í•  í™•ë¥ <span class="math inline">\(1-p_i\)</span>ì„ ê³±í•´ì„œ ëª¨ë“  ê°œì²´ì— ëŒ€í•´ í•©ì‚°í•´ì„œ ê³„ì‚°í•œë‹¤. <span class="math inline">\(\sum^J_{i=1}p_i(1-p_i)=\sum^J_{i=1}(p_i-p_i^2)=\sum^J_{i=1}p_i-\sum^J_{i=1}p_i^2=1-\sum^J_{i=1}p_i^2\)</span> </span></p><hr><p><strong><a href="https://www.diva-portal.org/smash/get/diva2:1328437/FULLTEXT01.pdf">Henrik Karlsson (2019)</a></strong></p><h2><span id="uplift-modeling-identifying-optimal-treatment-group-allocation-and-whom-to-contact-to-maximize-return-on-investment">Uplift Modeling: Identifying Optimal Treatment Group Allocation and Whom to Contact to Maximize Return on Investment</span></h2><p><br></p><h3><span id="413-model-uplift-directly">4.1.3 Model Uplift Directly</span></h3><p>treeê¸°ë°˜ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ë¥¼ í•˜ìœ„ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³  í‰ê°€í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆë‹¤. ì´ê²ƒì€ ì°¨ì´ë¥¼ ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ°ê°„ì˜ ì°¨ì´ì™€ ê°™ì´ ì°¨ì´(differences)ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°ì— ìœ ìš©í•˜ë‹¤. treeê¸°ë°˜ì˜ ë°©ë²•ì€ uplift ë¶„ì•¼ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì—¬ëŸ¬ ì—°êµ¬ìžë“¤ì—ê²Œ í™œìš©ë˜ì–´ì™”ë‹¤.</p><p><br></p><h4><span id="-general-tree-based-methods">- General Tree-based Methods</span></h4><p>Treeê¸°ë°˜ ë°©ë²•ì€ í¬ê²Œ ë‚˜ëˆ ì„œ ë¶„í• (splitting)ì™€ ê°€ì§€ì¹˜ê¸°(pruning)ë¼ëŠ” ë‘ê°€ì§€ ìŠ¤í…ì„ ê°€ì§€ê³  ìžˆë‹¤.</p><p>ë¶„í•  ìŠ¤í…ì€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ì„œ <em>pure</em> í•œ ë…¸ë“œë¥¼ ê°€ëŠ¥í•œ í•œ ìƒì„±í•˜ëŠ” ìµœì ì˜ ë¶„í• ë¥¼ ì°¾ê¸° ìœ„í•´ ë…¸ë ¥í•œë‹¤. ê°€ì§€ì¹˜ê¸° ìŠ¤í…ì€ ë‚˜ë¬´ì˜ ì¼ë°˜í™”ë¥¼ ê°œì„ í•˜ì§€ ëª»í•˜ëŠ” ë…¸ë“œë‚˜ ê°€ì§€ë“¤ì„ ì œê±°í•œë‹¤.</p><p>ë…¸ë“œê°€ <em>pure</em> í•˜ë‹¤ëŠ” ê²ƒì€ ë…¸ë“œì— ì†í•œ ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ê°€ ê°€ëŠ¥í•œ í•œ ê°€ìž¥ ë¹„ìŠ·í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. treeì•Œê³ ë¦¬ì¦˜ì˜ íƒ€ìž…ì— ë”°ë¼, ê° ë…¸ë“œëŠ” (CART treeì™€ ê°™ì´) ë‘ê°œì˜ ë…¸ë“œë¡œ ë‚˜ëˆ„ì–´ì§ˆ ìˆ˜ ë„ìžˆê³ , (CHAID treeì™€ ê°™ì´) ì—¬ëŸ¬ê°œì˜ ë…¸ë“œë¡œ ë‚˜ëˆ„ì–´ì§ˆ ìˆ˜ë„ ìžˆë‹¤.</p><p>ì•Œê³ ë¦¬ì¦˜ì€ ëª¨ë“  ë…¸ë“œë“¤ì´ ì™„ì „ížˆ <em>pure</em> í•´ì§€ê±°ë‚˜, í˜¹ì€ ë©ˆì¶”ëŠ” ê¸°ì¤€ì„ ë§Œì¡±í•  ë•Œê¹Œì§€ ë…¸ë“œë¥¼ ê³„ì†í•´ì„œ ë¶„í• ì‹œí‚¤ë©° treeë¥¼ ìžë¼ê²Œí•œë‹¤. ë§Œì•½ treeê°€ ëª¨ë“  ë…¸ë“œê°€ <em>pure</em> í•´ì§ˆ ë•Œê¹Œì§€ ìžëž€ë‹¤ë©´, ëª¨ë¸ì€ í›ˆë ¨ë°ì´í„°ë¥¼ ì™„ë²½ížˆ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ ê²°ê³¼ëŠ” overfittingë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì— ìž˜ ì¼ë°˜í™”ë˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. treeì˜ ë¶„í• ì§€ì ì„ ì •í•˜ê¸° ìœ„í•´ ì •ë³´ ì´ë“(information gain)ì„ ê³„ì‚°í•˜ê³ , ì •ë³´ ì´ë“ì´ ê°€ìž¥ ë†’ì€ ìž ìž¬ì  ë¶„í• ê°€ ìˆ˜í–‰ë  ê²ƒì´ë‹¤.</p><p>ì •ë³´ì´ë“ì€ ìžì‹ë…¸ë“œì— í• ë‹¹ëœ ë°ì´í„°ì˜ ë¹„ìœ¨ì— ë…¸ë“œì˜ ìˆœë„(purity)ë¥¼ ê³±í•˜ì—¬ ì¶”ì •í•œë‹¤.</p><p>CARTë‚˜ Quinlan's C4.5 treeì™€ ê°™ì€ ë§Žì€ treeê¸°ë°˜ì˜ ë°©ë²•ë“¤ì´ ì´ëŸ¬í•œ í•˜í–¥ì‹ìœ¼ë¡œ ë…¸ë“œë¥¼ ë¶„í• í•œ ë’¤ ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ë¶„í• ì„ ê°€ì§€ì¹˜ê¸°í•˜ëŠ” 2step ì ‘ê·¼ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ë°©ë²•ì„ í™œìš©í•˜ëŠ” ì´ìœ ëŠ” tree methodê°€ ë§¤ìš° ì„ í˜•ì ì´ì§€ ì•Šê³  ê·¸ì— ë”°ë¼ ì„ íƒëœ íŠ¹ì§•ëŸ‰ë“¤ì˜ ìƒí˜¸ìž‘ìš©ì— ê°•í•˜ê²Œ ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì´ë‹¤.</p><p>ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì–´ì§„ ë¶„í• ê°€ í˜„ìž¬ ë…¸ë“œì—ì„œë§Œ í‰ê°€ë  ë•Œ ì˜ë¯¸ê°€ ì—†ì–´ ë³´ì¼ ìˆ˜ ìžˆì§€ë§Œ, ê·¸ë³´ë‹¤ ë” ì•„ëž˜ë¡œ ë¶„í• ë  ë•Œ í˜„ìž¬ ë¶„í• ì™€ ê´€ë ¨ì§€ì–´ì ¸ ë§¤ìš° ì¤‘ìš”í•´ì§ˆ ìˆ˜ ìžˆë‹¤.<br>ì¦‰ ê° ë¶„í• ì€ ë¯¸ëž˜ì˜ ë¶„í•  ê°€ëŠ¥ì„±ì„ ë¬´ì‹œí•œ í˜„ìž¬ ë…¸ë“œì—ì„œ í‰ê°€ë˜ë¯€ë¡œ, ë¶„í• ê°€ ë§Žì€ ê¹Šì€ treeë¥¼ ë§Œë“¤ê³  ê·¸ í›„ì— ì¶©ë¶„ížˆ ê¸°ì—¬í•˜ì§€ ëª»í•œ ë¶„í• ë¥¼ ê°€ì§€ì¹˜ê¸°í•˜ëŠ” ê²ƒì´ë‹¤.</p><p>treeì˜ ê¹Šì´ë¥¼ ì œí•œí•˜ëŠ” ì •ì§€ ê¸°ì¤€ì„ ì‚½ìž…í•˜ì—¬ ê°€ì§€ì¹˜ê¸° ìž‘ì—…ì„ treeì— ì ìš©í•˜ê±°ë‚˜ ë‘ ê°€ì§€ë¥¼ í•¨ê»˜ ì ìš©í•¨ìœ¼ë¡œì¨ ì˜¤ë²„í”¼íŒ…ì„ í”¼í•  ìˆ˜ ìžˆë‹¤. ê°€ì§€ì¹˜ê¸° ìž‘ì—…ì€ ì‚¬ìš© ì¤‘ì¸ treeì˜ ì¢…ë¥˜ì— ë”°ë¼ ìœ ì˜ì„± ì‹œí—˜ì— ê·¼ê±°í•˜ì—¬ ë¶„í•  ì „ ë˜ëŠ” treeì˜ ì„±ìž¥ì´ ëë‚œ í›„ì— ìˆ˜í–‰í•  ìˆ˜ ìžˆë‹¤. í›„ìžì˜ ë°©ë²•ì´ ë” ì¼ë°˜ì ì´ë‹¤.</p><p>ì¼ë°˜ì ìœ¼ë¡œ ê¹Šê²Œ ìžë¼ëŠ” ê²ƒì´ í—ˆìš©ëœ decision treeëŠ” ë™ì¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì„ í–ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì„œë¡œ ë‹¤ë¥¸ treeë“¤ ê°„ì˜ ë°”ì´ì–´ìŠ¤ê°€ ë‚®ê³  ë¶„ì‚°ì´ ë†’ë‹¤. (low bias and high variance <span class="math inline">\(\rightarrow\)</span> <strong>Overfitting</strong> )</p><p>ê¹Šì€ tree ëª¨ë¸ì€ ë°ì´í„°ë¥¼ ìž˜ ë¶„ë¥˜í•  ìˆ˜ ìžˆê¸° ë•Œë¬¸ì— ë°”ëžŒì§í•˜ë‹¤ê³  í•  ìˆ˜ìžˆì§€ë§Œ, ì„œë¡œ ë‹¤ë¥¸ decision tree ê°„ì˜ ë†’ì€ ë¶„ì‚°ì´ robust<span class="math inline">\(^{[*2]}\)</span>í•˜ì§€ ëª»í•œ ê²°ê³¼ë¥¼ ë‚³ëŠ”ë‹¤. ì´ëŸ¬í•œ decision treeê°„ì˜ ë¶„ì‚°ì„ ì¤„ì´ê¸° ìœ„í•´ Random forest algorithmì´ ê°œë°œë˜ì—ˆë‹¤. ì´ê²ƒì€ ë§Žì€ decision treeë¥¼ ë§Œë“¤ê³  ê° treeì˜ ê²°ê³¼ë¥¼ í‰ê· í•˜ì—¬ ë” robustí•œ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤. ì´ê²ƒì€ ëª¨ë¸ì—ì„œ ì•½ê°„ ë” ë§Žì€ ë°”ì´ì–´ìŠ¤ë¼ëŠ” costë¥¼ ìˆ˜ë°˜í•˜ì§€ë§Œ, ê·¸ ê²°ê³¼ë¡œ ë¶„ì‚°ì´ ê°ì†Œí•œë‹¤. Random forestëŠ” Baggingì„ ì‚¬ìš©í•˜ê³  ê°ê°ì˜ treeë“¤ì„ ë‹¤ë¥¸ íŠ¹ì§•ì˜ ë¶€ë¶„ì§‘í•©ìœ¼ë¡œ í›ˆë ¨ì‹œì¼œ deep treeì˜ íš¨ê³¼ë¥¼ í›¨ì”¬ ë” í‰ê· í™”í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤. ì´ ë³´ê³ ì„œì—ì„œëŠ” uplift random forestëŠ” íŠ¹ì§•ëŸ‰ê°¯ìˆ˜ì˜ ì œê³±ê·¼ì„ ì‚¬ìš©í•˜ì—¬ ê° treeë¥¼ í›ˆë ¨ì‹œí‚¨ë‹¤.</p><p><br><br><span style="font-size: 85%"> <span class="math inline">\(^{[*2]}:\)</span> ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì—ì„œ robustëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ì˜ ê°•ê±´ì„±ì„ ê°€ë¦¬í‚¨ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ robustí•˜ë‹¤ê³  ê°„ì£¼ë˜ë ¤ë©´ testing errorê°€ training errorì™€ ì¼ì¹˜í•´ì•¼ í•˜ê±°ë‚˜ ë°ì´í„° ì§‘í•©ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ í›„ì—ë„ ì„±ëŠ¥ì´ ì•ˆì •ì ì´ì–´ì•¼ í•œë‹¤. </span></p><h4><span id="-tree-based-methods-for-estimating-uplift">- Tree-based Methods for Estimating Uplift</span></h4><p>Treeê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ Uplift Modelingì„ í•  ë•ŒëŠ” Upliftê°€ í¬ì°©ë  ìˆ˜ ìžˆë„ë¡ ë¶„í• ê¸°ì¤€ì„ ì¡°ì •í•œë‹¤. ë¬¸í—Œì— ë”°ë¥´ë©´ Upliftë¥¼ ê°€ìž¥ ìž˜ ì¶”ì •í•˜ê¸° ìœ„í•´ ë¶„í• ê¸°ì¤€ì„ ì¡°ì •í•˜ëŠ” ëª‡ ê°€ì§€ ë°©ë²•ì´ ì œì‹œë˜ì–´ ìžˆìœ¼ë©°, ì–´ë–¤ ë°©ë²•ì´ ê°€ìž¥ ì¢‹ì€ì§€ì— ëŒ€í•œ í•©ì˜ëŠ” ì•„ì§ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. Hansotia and Rukstales (2002)ëŠ” 'ê° ìžì‹ë…¸ë“œ(binary)ì—ì„œì˜ ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ°ì˜ í™•ë¥  ì°¨ì´' ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ìµœëŒ€í™” í•˜ê¸° ìœ„í•œ ë¶„í• ê¸°ì¤€ì„ ì œì‹œí–ˆë‹¤.<br>Rzepakowski &amp; Jaroszewicz (2012)ëŠ” ì •ë³´ì´ë¡ ìœ¼ë¡œ ë¶€í„° 'ì°¨ì´'ì˜ ê°œë…ì„ ë¶„í•  ê¸°ì¤€ìœ¼ë¡œ ë„ìž…í–ˆëŠ”ë°, Treeê¸°ë°˜ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ° ì‚¬ì´ì˜ ë¶„í¬ì  ì°¨ì´ë¥¼ ìµœëŒ€í™”í•˜ë ¤ê³  ë…¸ë ¥í•¨ìœ¼ë¡œì¨ ìƒìŠ¹ì„ í¬ì°©í•œë‹¤. ì´ ë³´ê³ ì„œëŠ” Rzepakowski &amp; Jaroszewicz (2012)ì˜ ë¶„í•  ê¸°ì¤€ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.</p><p>Distributional divergence(ë¶„í¬ì˜ ì°¨ì´)ëž€ q(x)ë¥¼ p(x)ì˜ ê·¼ì‚¬ì¹˜ë¡œ ì‚¬ìš©í•  ë•Œ ì†ì‹¤ë˜ëŠ” ì •ë³´ì˜ ì–‘ì„ ë‚˜íƒ€ë‚´ëŠ” ì²™ë„ë¡œ, ì—¬ê¸°ì„œ q(x)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í‘œë³¸ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚´ê³  p(x)ëŠ” ì´ë¡ ì ì¸ ë¶„í¬ë¡œë¶€í„° ë„ì¶œëœ ë°ì´í„°ë¡œ ë‚˜íƒ€ë‚´ì–´ì§„ë‹¤. ì´ divergenceëŠ” ë‘ í™•ë¥ ë¶„í¬ ì‚¬ì´ì˜ "ê±°ë¦¬"ì´ì§€ë§Œ, ëŒ€ì¹­ì ì´ê±°ë‚˜ ì‚¼ê°ë¶€ë“±ì‹ì„ ë§Œì¡±í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ ë¯¸í„°ë²• ê±°ë¦¬ë³´ë‹¤ ì•½í•œ ì²™ë„ì´ë‹¤. divergence ì²™ë„ë¥¼ ë¶„í• ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•  ê²½ìš° ëª¨ë“  ë…¸ë“œì—ì„œ ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ° ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ìµœëŒ€í™”í•˜ë ¤ê³  í•œë‹¤. treeë“¤ì˜ ì•™ìƒë¸”ì„ ì‚¬ìš©í•˜ë ¤ê³  í•  ë•Œ, ì˜ˆì¸¡ë˜ëŠ” Upliftê°’ì€ ê°œë³„ treeì—ì„œì˜ ì˜ˆì¸¡ Upliftê°’ì„ í‰ê· í•˜ì—¬ ì–»ëŠ”ë‹¤. ë¶„í• ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë„¤ ê°€ì§€ ë‹¤ë¥¸ divergence ì²™ë„ëŠ” ì•„ëž˜ì—ì„œ í™•ì¸í•  ìˆ˜ ìžˆë‹¤.</p><p><br></p><p>Rzepakowski &amp; Jaroszewicz(2012)ëŠ” Upliftì„ í¬ì°©í•˜ê¸° ìœ„í•´ ë¶„í•  ê¸°ì¤€ì´ ì¶©ì¡±í•´ì•¼ í•˜ëŠ” ì„¸ ê°€ì§€ í•­ëª©ì„ ì œì‹œí•œë‹¤.</p><ol type="1"><li><p>ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ°ì˜ class ë¶„í¬ê°€ ëª¨ë“  ë¶„í• ì—ì„œ ë™ì¼í•  ê²½ìš° ë¶„í• ê¸°ì¤€ì¹˜ë¥¼ ìµœì†Œê°’ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•œë‹¤.</p><ul><li>UpliftëŠ” ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ° ì‚¬ì´ì˜ ê°€ëŠ¥í•œ ê°€ìž¥ í° ë¶„í¬ì°¨ì´ë¥¼ ë§Œë“¤ì–´ëƒ„ìœ¼ë¡œì¨ í¬ì°©ë˜ê¸° ë•Œë¬¸ì—, ë‘ ì§‘ë‹¨ì´ ë™ì¼í•  ë•ŒëŠ” ë¶„í• ê¸°ì¤€ì¹˜ê°€ ìµœì†Œê°’ìœ¼ë¡œ í‰ê°€ë˜ëŠ” ê²ƒì´ íƒ€ë‹¹í•˜ë‹¤.</li></ul></li><li><p>ë¶„í• ê¸°ì¤€ì¹˜ëŠ” testê°€ ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ°ì—ì„œì˜ ê° outcomeê³¼ í†µê³„ì ìœ¼ë¡œ ë…ë¦½ë˜ì–´ ìžˆëŠ” ê²½ìš° 0ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•œë‹¤.</p><ul><li><p>ì¼ë°˜ì ì¸ decision treeì—ì„œëŠ” outcomeê³¼ í†µê³„ì ìœ¼ë¡œ ë…ë¦½ëœ ë¶„í• ê°€ treeë¥¼ ê°œì„ ì‹œí‚¤ì§€ ëª»í•˜ë¯€ë¡œ ë¶„í• ì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•´ì„œëŠ” ì•ˆëœë‹¤ê³  ëª…ì‹œí•˜ê³  ìžˆë‹¤.</p></li><li><p>ê·¸ëŸ¬ë‚˜ uplift modelingì—ì„œëŠ” ë¶„í¬ë¥¼ ì´ì „ë³´ë‹¤ ë” ìœ ì‚¬í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìžˆìœ¼ë©°, ì´ëŠ” ìŒì˜ ë¶„í• ê¸°ì¤€ì¹˜ë¥¼ ê°–ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì¦‰ ë…ë¦½ì ì¸ ë¶„í• ê°€ ë°œìƒí•  ìˆ˜ ìžˆëŠ” ìµœì•…ì˜ ë¶„í• ê°€ ì•„ë‹ ìˆ˜ ìžˆìŒì„ ì˜ë¯¸í•œë‹¤.</p></li></ul></li><li><p>ëŒ€ì¡°êµ°ì˜ í¬ê¸°ê°€ 0ì¼ ê²½ìš°ì˜ ë¶„í• ê¸°ì¤€ì¹˜ëŠ” decision treeê°€ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ ë¶„í• ê¸°ì¤€ì¹˜ê¹Œì§€ ê°ì†Œì‹œì¼œì•¼í•œë‹¤.</p></li></ol><p><br></p><p>ì´ ë³´ê³ ì„œëŠ” Uplift random forestì™€ í•¨ê»˜ ë„¤ ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ ë¶„í• ê¸°ì¤€ì„ ë‹¤ë£¬ë‹¤. ê°ê°ì€ <span class="math inline">\(P=(p_1,\cdots,p_n)\)</span>ê³¼ <span class="math inline">\(Q=(q_1,\cdots,q_n)\)</span>ì˜ ë¶„í¬ ì‚¬ì´ì˜ divergece ê°’ì„ ê³„ì‚°í•œë‹¤.</p><ul><li>Kullback-Leibler divergence :<br><span class="math inline">\(KL(P:Q)=\sum_ip_ilog\cfrac{p_i}{q_i}\)</span></li></ul><p><br></p><ul><li><p>squared Euclidean distance :</p><p><span class="math inline">\(ED(P:Q)=\sum_i(p_i-q_i)^2\)</span></p></li></ul><p><br></p><ul><li><span class="math inline">\(\chi^2\)</span>-divergence :<br><span class="math inline">\(\chi^2(P:Q)=\sum_i\cfrac{(p_i-q_i)^2}{q_i}\)</span></li></ul><p><br></p><ul><li><p>L1-norm divergence :</p><p><span class="math inline">\(L1(P:Q)=\sum_i|p_i-q_i|\)</span></p></li></ul><p>(Kullback-Leibler divergence, squared Euclidean distance, <span class="math inline">\(\chi^2\)</span>-divergenceëŠ” Rzepakowski and Jaroszewicz (2012b)ì— ì†Œê°œë˜ì—ˆê³ , L1-norm divergenceëŠ” Guelman, GuillÂ´en, and PÂ´erez-MarÂ´Ä±n (2015)ì—ì„œ ì†Œê°œë˜ì—ˆë‹¤.)</p><p>Random forest ëª¨ë¸ì€ ì£¼ì–´ì§„ êµ°ì— ì†í•˜ëŠ” êµ¬ë§¤ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë°˜í™˜í•œë‹¤. Uplift scoreì€ ì´í•˜ì™€ ê°™ì€ ì‹ìœ¼ë¡œ ê³„ì‚°ëœë‹¤.</p><p><span class="math inline">\(Uplift\space score = P(purchase|treatment\space group) - P(purchase|control\space group)\)</span></p><p>Scoreê°€ ë†’ë‹¤ëŠ” ê²ƒì€ ì¹˜ë£Œë¡œ ì¸í•œ êµ¬ë§¤ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ê°€ì •í•˜ê³  ìžˆìœ¼ë¯€ë¡œ scoreë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë¶„ë¥˜í•´ì•¼ í•œë‹¤. scoreê°€ ìŒìˆ˜ì¸ê²ƒì€ ì¹˜ë£Œë¥¼ í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ê·¸ê°œì¸ì˜ êµ¬ë§¤í™•ë¥ ì´ ë” ë†’ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. upliftë¥¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§ í•˜ëŠ” ê²ƒì˜ ìž¥ì ì€ ì´ì§„ì ì¸(binary) ëª©ì ë³€ìˆ˜ì™€ ì—°ì†ì ì¸(continuous) ëª©ì ë³€ìˆ˜ ëª¨ë‘ì— ì ìš©í•  ìˆ˜ ìžˆë‹¤ëŠ” ì ì´ë‹¤.</p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/UPLIFT-MODELING/">UPLIFT MODELING</category>
      
      <category domain="https://jaysung00.github.io/categories/UPLIFT-MODELING/c-Methods/">c.Methods</category>
      
      <category domain="https://jaysung00.github.io/categories/UPLIFT-MODELING/c-Methods/2-Tree-based-Algorithm/">2.Tree-based Algorithm</category>
      
      
      <category domain="https://jaysung00.github.io/tags/Uplift-modeling/">Uplift modeling</category>
      
      <category domain="https://jaysung00.github.io/tags/Uplift-Tree/">Uplift Tree</category>
      
      
      <comments>https://jaysung00.github.io/2020/12/17/Uplift-tree/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ã€Uplift Modelingã€‘ë³€ìˆ˜ì˜ ì„ íƒë°©ë²•ì— ëŒ€í•˜ì—¬</title>
      <link>https://jaysung00.github.io/2020/12/17/Selection/</link>
      <guid>https://jaysung00.github.io/2020/12/17/Selection/</guid>
      <pubDate>Thu, 17 Dec 2020 01:46:51 GMT</pubDate>
      
      <description>&lt;p&gt;í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” &lt;a href=&quot;https://arxiv.org/pdf/2005.03447.pdf&quot;&gt;Zhenyu Zhao at el. &quot;Feature Selection Methods for Uplift Modeling&quot; (2020)&lt;/a&gt;ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ, Uberì‚¬ì—ì„œ ê°œë°œí•œ íŒŒì´ì¬ ì˜¤í”ˆì†ŒìŠ¤ íŒ¨í‚¤ì§€ &lt;a href=&quot;https://github.com/uber/causalml&quot;&gt;CausalML&lt;/a&gt;ì— êµ¬í˜„ëœ ì†ŒìŠ¤ì½”ë“œë“¤ì„ ë‹¤ë£¨ê³  ìžˆìŠµë‹¤.&lt;/p&gt;
&lt;hr /&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” <a href="https://arxiv.org/pdf/2005.03447.pdf">Zhenyu Zhao at el. "Feature Selection Methods for Uplift Modeling" (2020)</a>ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ, Uberì‚¬ì—ì„œ ê°œë°œí•œ íŒŒì´ì¬ ì˜¤í”ˆì†ŒìŠ¤ íŒ¨í‚¤ì§€ <a href="https://github.com/uber/causalml">CausalML</a>ì— êµ¬í˜„ëœ ì†ŒìŠ¤ì½”ë“œë“¤ì„ ë‹¤ë£¨ê³  ìžˆìŠµë‹¤.</p><hr><a id="more"></a><h1><span id="introduction">* Introduction</span></h1><hr><p>Feature Selection ë°©ë²•ì€ ê° featureì— ëŒ€í•´ ì¤‘ìš”ë„ ì ìˆ˜(importance score)ë¥¼ ê³„ì‚°í•œ ë’¤ ì ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëž­í¬ë¥¼ ë§¤ê¸´ë‹¤. Uplift modelì€ ì´ë ‡ë“¯ ê°€ìž¥ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨ëœ ë³€ìˆ˜ë“¤ë§Œì„ ê°€ì§€ê³  ë§Œë“¤ì–´ ì§ˆ ìˆ˜ ìžˆë‹¤. Uplift modelingì—ì„œ ì¤‘ìš”í•œ ë³€ìˆ˜ë“¤ì—ê²Œë§Œ ì§‘ì¤‘í•˜ëŠ” ê²ƒì€ ëª‡ê°€ì§€ì˜ ì´ë“ì„ ê°€ì ¸ë‹¤ ì¤€ë‹¤.</p><ol type="1"><li>í›ˆë ¨ì„ ìœ„í•œ ë¹ ë¥¸ ê³„ì‚°ì²˜ë¦¬<br></li><li>overfitting ë¬¸ì œë¥¼ í”¼í•¨ìœ¼ë¡œì¨ ë”ìš± ì •í™•í•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥<br></li><li>ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ë‚®ì€ ìœ ì§€ë¹„ìš©<br></li><li>ë”ìš± ì‰¬ìš´ ëª¨ë¸ í•´ì„ê³¼ ì§„ë‹¨</li></ol><p>ì´ë ‡ë“¯ feature selectionì€ Uplift modelingì— ìžˆì–´ì„œ ì¤‘ìš”í•œ ë¬¸ì œìž„ì—ë„ ë¶ˆêµ¬í•˜ê³  ì§€ê¸ˆê¹Œì§€ ê´€ë ¨ ë¬¸í—Œì—ì„œ ê±°ì˜ ë…¼ì˜ë˜ì–´ì˜¤ì§€ ëª»í–ˆë‹¤. ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ì— ìžˆì–´ì„œì˜ ë³€ìˆ˜ì„ íƒë°©ë²•ì˜ ì—°êµ¬ëŠ” V. <a href="https://www.researchgate.net/profile/Veronica_Bolon-Canedo/publication/257482053_A_review_of_feature_selection_methods_on_synthetic_data/links/549d23980cf2d6581ab4acc7.pdf">Bolon-Canedo et al.(2013)</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0045790613003066?via%3Dihub">G. Chandrashekar &amp; F. Sahin(2014)</a>, J. <a href="http://www.math.chalmers.se/Stat/Grundutb/GU/MSA220/S18/featselect.pdf">Tang(2014)</a>ê³¼ ê°™ì€ ë…¼ë¬¸ë“¤ì—ì„œ ìž˜ ë…¼ì˜ë˜ì–´ìžˆì§€ë§Œ, ì´ê²ƒë“¤ì€ Uplift modelingì—ì„œ ìµœì ì˜ ë³€ìˆ˜ì„ íƒ ë°©ë²•ì€ ì•„ë‹ˆë‹¤.</p><p>ì´ ë…¼ë¬¸ì—ì„œëŠ” ë°©ë²•ë¡ ì ì´ê³  ê²½í—˜ì ì¸ í‰ê°€ ê´€ì ì—ì„œ ë³€ìˆ˜ì„ íƒì„ ë‹¤ë£¬ë‹¤.</p><hr><h1><span id="ì¼ë°˜ì ì¸-ë³€ìˆ˜ì„ íƒ-ë°©ë²•ê³¼ì˜-ê´€ê³„">* ì¼ë°˜ì ì¸ ë³€ìˆ˜ì„ íƒ ë°©ë²•ê³¼ì˜ ê´€ê³„</span></h1><hr><h5><span id="ì¼ë°˜ì ì¸-ë³€ìˆ˜ì„ íƒë²•ì˜-ì¢…ë¥˜">ì¼ë°˜ì ì¸ ë³€ìˆ˜ì„ íƒë²•ì˜ ì¢…ë¥˜</span></h5><ol type="1"><li>filter methods<br></li><li>wrapped methods<br></li><li>embedded methods</li></ol><h5><span id="ì¼ë°˜ì ì¸-ë³€ìˆ˜ì„ íƒë²•ì´-uplift-modelingì—ì„œ-ìµœì„ ì´-ì•„ë‹Œ-ì´ìœ ">ì¼ë°˜ì ì¸ ë³€ìˆ˜ì„ íƒë²•ì´ Uplift modelingì—ì„œ ìµœì„ ì´ ì•„ë‹Œ ì´ìœ </span></h5><ul><li><p>ë¶„ë¥˜ë¬¸ì œë¥¼ ìƒê°í–ˆì„ë•Œ ì¼ë°˜ì  ë³€ìˆ˜ì„ íƒë²•ì˜ ëª©ì ì€ featureì— ê¸°ë°˜í•˜ì—¬ outcomeì´ ê° í´ëž˜ìŠ¤ì— í•´ë‹¹ë  í™•ë¥ ì˜ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ featureì˜ ì¤‘ìš”ë„ëŠ” í´ëž˜ìŠ¤ í™•ë¥ ê³¼ ê´€ê³„ê°€ ê¹Šë‹¤.</p></li><li><p>ë°˜ë©´ì— Uplift modelì˜ ëª©ì ì€ CATEë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì—¬ê¸°ì„œ ì¢‹ì€ featureëŠ” í´ëž˜ìŠ¤ í™•ë¥ ì´ ì•„ë‹ˆë¼ ì¹˜ë£Œíš¨ê³¼ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìžˆê²Œ í•´ì£¼ëŠ” ê²ƒì´ì–´ì•¼ í•œë‹¤. ì´ ë‘ê°€ì§€ ì˜ˆì¸¡ëŒ€ìƒì´ í•­ìƒ ì¼ì¹˜í•  í•„ìš”ëŠ” ì—†ìœ¼ë¯€ë¡œ Uplift modelingì—ì„œ ì¼ë°˜ì  ë³€ìˆ˜ì„ íƒë²•ì€ ìµœì„ ì´ ì•„ë‹ ìˆ˜ ìžˆë‹¤.</p></li></ul><p><br></p><hr><h1><span id="uplift-modelingì„-ìœ„í•œ-ë³€ìˆ˜ì„ íƒ-ë°©ë²•">* uplift modelingì„ ìœ„í•œ ë³€ìˆ˜ì„ íƒ ë°©ë²•</span></h1><hr><h4><span id="ì˜ˆì‹œë¡œì¨-x1-x63-ì˜-ì´-63ê°œ-feature-ê°€-ì¡´ìž¬í•˜ëŠ”-ê°€ìƒì˜-dataë¥¼-ìƒê°í•˜ìž">ì˜ˆì‹œë¡œì¨ x1 , ... , x63 ì˜ ì´ 63ê°œ feature ê°€ ì¡´ìž¬í•˜ëŠ” ê°€ìƒì˜ dataë¥¼ ìƒê°í•˜ìž.</span></h4><p><img src="https://i.imgur.com/kNDqVcM.png" width="600px"></p><p><br></p><h4><span id="a-filter-methods">A. <em>Filter Methods</em></span></h4><p>ì´ ë°©ë²•ì€ ê° featureì— ëŒ€í•˜ì—¬ ì¹˜ë£Œíš¨ê³¼ì™€ featureê°„ì˜ í•œê³„ê´€ê³„(marginal relationship)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ë„ ì ìˆ˜(Importance score)ë¥¼ ê³„ì‚°í•œë‹¤. ì´ê²ƒì€ í•œë²ˆì— í•˜ë‚˜ì˜ featureì— ëŒ€í•œ ê°„ë‹¨í•œ ê³„ì‚°ë§Œ ì´ë£¨ì–´ì§€ë¯€ë¡œ ë¹ ë¥¸ ì „ì²˜ë¦¬ ë‹¨ê³„ì´ë‹¤.</p><h5><span id="1-f-filter">1. F filter</span></h5><h5><span id="-causalml-packageì—ì„œ-implementation">- <code>Causalml</code> packageì—ì„œ implementation</span></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> causalml.feature_selection.filters <span class="keyword">import</span> FilterSelect</span><br><span class="line">filter_f = FilterSelect() </span><br><span class="line">method = <span class="string">&#x27;F&#x27;</span></span><br><span class="line">f_imp = filter_f.get_importance(df, X_names, y_name, method, </span><br><span class="line">                      treatment_group = <span class="string">&#x27;treatment1&#x27;</span>)</span><br><span class="line"><span class="comment"># X_namesëŠ” featuresì˜ ì»¬ëŸ¼ë¦¬ìŠ¤íŠ¸</span></span><br><span class="line">f_imp</span><br></pre></td></tr></table></figure><p>[output]<br><img src="https://i.imgur.com/rZ69DlY.png" width="600px"></p><p><br></p><h5><span id="-ì½”ë“œì˜-ë™ìž‘-ì›ë¦¬ë¥¼-ìžì„¸í•˜ê²Œ-ì‚´íŽ´ë³´ìž">- ì½”ë“œì˜ ë™ìž‘ ì›ë¦¬ë¥¼ ìžì„¸í•˜ê²Œ ì‚´íŽ´ë³´ìž.</span></h5><p>ì¹˜ë£Œì—¬ë¶€ë³€ìˆ˜(<span class="math inline">\(Z\)</span>)ì™€ í™•ì¸í•˜ê³ ìží•˜ëŠ” feature(<span class="math inline">\(x_i\)</span>) ê·¸ë¦¬ê³  ê·¸ë“¤ì˜ êµí˜¸ìž‘ìš©í•­(interaction term)ì„ ì‚¬ìš©í•˜ì—¬ outcomeë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì„ í˜•íšŒê·€ëª¨ë¸ì´ë‹¤.</p><p><span class="math inline">\(y_{outcome} = \beta_0 + \beta_1Z + \beta_2x_i + \beta_3(Z\cdot x_i)\)</span></p><p>ì¤‘ìš”ë„ ì ìˆ˜(importance score)ëŠ” êµí˜¸ìž‘ìš©í•­ <span class="math inline">\(\beta_3(Z\cdot x_i)\)</span>ì˜ ê³„ìˆ˜<span class="math inline">\(\beta_3\)</span>ì— ëŒ€í•œ F-í†µê³„ê°’ ìœ¼ë¡œ ì •ì˜ëœë‹¤. ì´ í†µê³„ê°’ì´ í¬ë©´ í•´ë‹¹ featureëŠ” ê°•í•œ heterogeneous treatment effectì™€ ìƒê´€ì´ ìžˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">method = <span class="string">&#x27;F&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># treatment_indicator column ìƒì„± (treatment : 1 , control : 0)</span></span><br><span class="line">df = df[df[<span class="string">&#x27;treatment_group_key&#x27;</span>].isin([<span class="string">&#x27;control&#x27;</span>, <span class="string">&#x27;treatment1&#x27;</span>])]</span><br><span class="line">df[<span class="string">&#x27;treatment_indicator&#x27;</span>] = <span class="number">0</span></span><br><span class="line">df.loc[df[<span class="string">&#x27;treatment_group_key&#x27;</span>]==<span class="string">&#x27;treatment1&#x27;</span>,<span class="string">&#x27;treatment_indicator&#x27;</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">all_result = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> x_name_i <span class="keyword">in</span> X_names: </span><br><span class="line">    Y = df[<span class="string">&#x27;conversion&#x27;</span>]</span><br><span class="line">    X = df[[<span class="string">&#x27;treatment_indicator&#x27;</span>, x_name_i]]</span><br><span class="line">    X = sm.add_constant(X)</span><br><span class="line">    X[<span class="string">&#x27;&#123;&#125;-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;treatment_indicator&#x27;</span>, x_name_i)] = X[[<span class="string">&#x27;treatment_indicator&#x27;</span>, x_name_i]].product(axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    model = sm.OLS(Y, X)</span><br><span class="line">    result = model.fit()</span><br><span class="line">    <span class="comment"># êµí˜¸ìž‘ìš©ì˜ ì¤‘ìš”ë„ë¥¼ ì•Œê³ ì‹¶ë‹¤</span></span><br><span class="line">    <span class="comment"># êµí˜¸ìž‘ìš©í•­ì˜ ê³„ìˆ˜ Î²_3ì— ëŒ€í•´ì„œë§Œ fê²€ì • </span></span><br><span class="line">    F_test = result.f_test(np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line">    F_test_result = pd.DataFrame(&#123;</span><br><span class="line">        <span class="string">&#x27;feature&#x27;</span>: x_name_i, </span><br><span class="line">        <span class="string">&#x27;method&#x27;</span>: <span class="string">&#x27;F-statistic&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;score&#x27;</span>: F_test.fvalue[<span class="number">0</span>][<span class="number">0</span>], </span><br><span class="line">        <span class="string">&#x27;p_value&#x27;</span>: F_test.pvalue, </span><br><span class="line">        <span class="string">&#x27;misc&#x27;</span>: <span class="string">&#x27;df_num: &#123;&#125;, df_denom: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(F_test.df_num, F_test.df_denom), </span><br><span class="line">    &#125;, index=[<span class="number">0</span>]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    one_result = F_test_result</span><br><span class="line">    </span><br><span class="line">    all_result = pd.concat([all_result, one_result])</span><br><span class="line"></span><br><span class="line">all_result = all_result.sort_values(by=<span class="string">&#x27;score&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">all_result[<span class="string">&#x27;rank&#x27;</span>] = all_result[<span class="string">&#x27;score&#x27;</span>].rank(ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">all_result[<span class="string">&#x27;method&#x27;</span>] = method + <span class="string">&#x27; filter&#x27;</span></span><br><span class="line"></span><br><span class="line">all_result[[<span class="string">&#x27;method&#x27;</span>, <span class="string">&#x27;feature&#x27;</span>, <span class="string">&#x27;rank&#x27;</span>, <span class="string">&#x27;score&#x27;</span>, <span class="string">&#x27;p_value&#x27;</span>, <span class="string">&#x27;misc&#x27;</span>]]</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">[output]  </span><br><span class="line">&lt;img src=&quot;https://i.imgur.com/rZ69DlY.png&quot; width=&quot;600px&quot;&gt;&lt;/img&gt;  </span><br><span class="line"></span><br><span class="line">&lt;br&gt;  </span><br><span class="line"></span><br><span class="line">ê° feature ë³„ë¡œ F í†µê³„ëŸ‰ì„ ê¸°ì¤€ìœ¼ë¡œ scoringí•´ì„œ ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë°˜í™˜í•œë‹¤. </span><br><span class="line"></span><br><span class="line">&lt;br&gt;  </span><br><span class="line"></span><br><span class="line"><span class="comment">##### 2. LR filter (Likelihood ratio)</span></span><br><span class="line"><span class="comment">##### - `Causalml` packageì—ì„œ implementation</span></span><br><span class="line">```python</span><br><span class="line">method = <span class="string">&#x27;LR&#x27;</span></span><br><span class="line">lr_imp = filter_f.get_importance(df, X_names, y_name, method, </span><br><span class="line">                      treatment_group = <span class="string">&#x27;treatment1&#x27;</span>)</span><br><span class="line">lr_imp</span><br></pre></td></tr></table></figure></p><p>[output]<br><img src="https://i.imgur.com/1Rar2L8.png" width="450px"></p><p><br></p><p>ì—¬ê¸°ì„œëŠ” scoreë¥¼ ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨í˜•ì˜ êµí˜¸ìž‘ìš©í•­ ê³„ìˆ˜ì— ëŒ€í•œ likelihood ratio ê²€ì • í†µê³„ëŸ‰ìœ¼ë¡œ ì •ì˜í•œë‹¤. ê° feature ë³„ LR í†µê³„ëŸ‰ì„ ê¸°ì¤€ìœ¼ë¡œ scoringí•´ì„œ ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë°˜í™˜í•œë‹¤.</p><p><br></p><h5><span id="3-filter-method-with-k-bins">3. Filter method with K bins</span></h5><p>ì—¬ê¸°ì—ëŠ” <a href="https://www.semanticscholar.org/paper/Uplift-Modeling-in-Direct-Marketing-Rzepakowski-Jaroszewicz/e979ba084f34345b2ac8783df2b4a3295ae9273f">Piotr Rzepakowski &amp; Szymon Jaroszewicz (2012)</a>ì—ì„œ ì œì•ˆëœ uplift treeì˜ ë¶„í• ê¸°ì¤€ìœ¼ë¡œ ë¶€í„° ì„¸ê°€ì§€ ë°©ë²•ì´ ì¡´ìž¬í•œë‹¤.</p><p><br></p><h5><span id="3-1-kullback-leibler-divergence">3-1. Kullback-Leibler divergence</span></h5><h5><span id="-causalml-packageì—ì„œ-implementation">- <code>Causalml</code> packageì—ì„œ implementation</span></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">method = <span class="string">&#x27;KL&#x27;</span></span><br><span class="line">kl_imp = filter_f.get_importance(df, X_names, y_name, method, </span><br><span class="line">                      treatment_group = <span class="string">&#x27;treatment1&#x27;</span>,</span><br><span class="line">                      n_bins=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>[output]<br><img src="https://i.imgur.com/fgdy1vw.png" width="500px"></p><p><br></p><h5><span id="3-2-the-squared-euclidean-distance">3-2. the squared Euclidean distance</span></h5><h5><span id="-causalml-packageì—ì„œ-implementation">- <code>Causalml</code> packageì—ì„œ implementation</span></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">method = <span class="string">&#x27;ED&#x27;</span></span><br><span class="line">kl_imp = filter_f.get_importance(df, X_names, y_name, method, </span><br><span class="line">                      treatment_group = <span class="string">&#x27;treatment1&#x27;</span>,</span><br><span class="line">                      n_bins=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>[output]<br><img src="https://i.imgur.com/AFzR2iL.png" width="500px"></p><p><br></p><h5><span id="3-3-the-chi-squared-divergence">3-3. the Chi-squared divergence</span></h5><h5><span id="-causalml-packageì—ì„œ-implementation">- <code>Causalml</code> packageì—ì„œ implementation</span></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">method = <span class="string">&#x27;Chi&#x27;</span></span><br><span class="line">kl_imp = filter_f.get_importance(df, X_names, y_name, method, </span><br><span class="line">                      treatment_group = <span class="string">&#x27;treatment1&#x27;</span>,</span><br><span class="line">                      n_bins=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>[output]<br><img src="https://i.imgur.com/YTOLqlc.png" width="500px"></p><p><br></p><h5><span id="-ì½”ë“œì˜-ë™ìž‘-ì›ë¦¬ë¥¼-ìžì„¸í•˜ê²Œ-ì‚´íŽ´ë³´ìž">- ì½”ë“œì˜ ë™ìž‘ ì›ë¦¬ë¥¼ ìžì„¸í•˜ê²Œ ì‚´íŽ´ë³´ìž.</span></h5><p>ì£¼ì–´ì§„ featureì— ëŒ€í•´ ì´ ë°©ë²•ì€ ë¨¼ì € ìƒ˜í”Œì„ featureì˜ ë°±ë¶„ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Kê°œì˜ binìœ¼ë¡œ ë‚˜ëˆˆë‹¤. (ì—¬ê¸°ì„œ KëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°) ì¤‘ìš”ë„ ì ìˆ˜ëŠ” ì´ëŸ¬í•œ Kê°œì˜ binsì— ëŒ€í•œ ì²˜ë¦¬íš¨ê³¼ì˜ divergence measureë¡œ ì •ì˜ëœë‹¤.</p><p>êµ¬ì²´ì ìœ¼ë¡œ, outcome ë³€ìˆ˜ì— Cê°œì˜ í´ëž˜ìŠ¤ê°€ ìžˆë‹¤ê³  ê°€ì •í•´ë³´ìž.</p><p><span class="math inline">\(P_k = (p_{k1},...,p_{kC})\)</span> ì™€ <span class="math inline">\(Q_k = (q_{k1},...,q_{kC})\)</span>ê°€ ê°ê° ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ°ì˜ <span class="math inline">\(k\)</span>ë²ˆì§¸(<span class="math inline">\(k=1,...,K\)</span>) binì—ì„œì˜ í´ëž˜ìŠ¤ë³„ sampleì˜ ë¹„ìœ¨ì´ë¼ê³  í–ˆì„ ë•Œ, ì¤‘ìš”ë„ ì ìˆ˜ëŠ” ì´í•˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.</p><p><span class="math inline">\(\Delta = \sum^K_{k=1}\cfrac{N_k}{N}D(P_k:Q_k)\)</span></p><p><span class="math inline">\(N_k\)</span> : <span class="math inline">\(k\)</span>ë²ˆì§¸ binì˜ ìƒ˜í”Œì‚¬ì´ì¦ˆ<br><span class="math inline">\(N\)</span> : ì „ì²´ ìƒ˜í”Œ ì‚¬ì´ì¦ˆ<br><span class="math inline">\(D\)</span> : distribution divergence<br>- Kullback-Leibler divergence (denoted as <strong>KL</strong> )<br>- the squared Euclidean distance(denoted as <strong>ED</strong> )<br>- the chi-squared divergence (denoted as <strong>Chi</strong> )</p><ul><li><span class="math inline">\(KL(P_k:Q_k)=\sum^n_{i=1}p_{ki}log\cfrac{p_{ki}}{q_{ki}}\)</span></li></ul><p><br></p><ul><li><span class="math inline">\(ED(P_k:Q_k)=\sum^n_{i=1}(p_{ki}-q_{ki})^2\)</span></li></ul><p><br></p><ul><li><span class="math inline">\(\chi^2(P_k:Q_k)=\sum^n_{i=1}\cfrac{(p_{ki}-q_{ki})^2}{q_{ki}}\)</span></li></ul><p><br><br>[ë¨¼ì € ì„¸ê°œì˜ í•¨ìˆ˜ë¥¼ ì •ì˜] <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_GetNodeSummary</span>(<span class="params">data,</span></span></span><br><span class="line"><span class="function"><span class="params">                        experiment_group_column=<span class="string">&#x27;treatment_group_key&#x27;</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                        y_name=<span class="string">&#x27;conversion&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        To count the conversions and get the probabilities by treatment groups. This function comes from the uplift tree algorithm, that is used for tree node split evaluation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        data : DataFrame</span></span><br><span class="line"><span class="string">            The DataFrame that contains all the data (in the current &quot;node&quot;).  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        results : dict</span></span><br><span class="line"><span class="string">            Counts of conversions by treatment groups, of the form: </span></span><br><span class="line"><span class="string">            &#123;&#x27;control&#x27;: &#123;0: 10, 1: 8&#125;, &#x27;treatment1&#x27;: &#123;0: 5, 1: 15&#125;&#125;</span></span><br><span class="line"><span class="string">        nodeSummary: dict</span></span><br><span class="line"><span class="string">            Probability of conversion and group size by treatment groups, of </span></span><br><span class="line"><span class="string">            the form:</span></span><br><span class="line"><span class="string">            &#123;&#x27;control&#x27;: [0.490, 500], &#x27;treatment1&#x27;: [0.584, 500]&#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: results and nodeSummary are both dict with treatment_group_key</span></span><br><span class="line">        <span class="comment"># as the key.  So we can compute the treatment effect and/or </span></span><br><span class="line">        <span class="comment"># divergence easily.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Counts of conversions by treatment group</span></span><br><span class="line">        results_series = data.groupby([experiment_group_column, y_name]).size()</span><br><span class="line">        </span><br><span class="line">        treatment_group_keys = results_series.index.levels[<span class="number">0</span>].tolist()</span><br><span class="line">        y_name_keys = results_series.index.levels[<span class="number">1</span>].tolist()</span><br><span class="line"></span><br><span class="line">        results = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> ti <span class="keyword">in</span> treatment_group_keys: </span><br><span class="line">            results.update(&#123;ti: &#123;&#125;&#125;) </span><br><span class="line">            <span class="keyword">for</span> ci <span class="keyword">in</span> y_name_keys:</span><br><span class="line">                results[ti].update(&#123;ci: results_series[ti, ci]&#125;) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Probability of conversion and group size by treatment group</span></span><br><span class="line">        nodeSummary = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> treatment_group_key <span class="keyword">in</span> results: </span><br><span class="line">            n_1 = results[treatment_group_key][<span class="number">1</span>]</span><br><span class="line">            n_total = (results[treatment_group_key][<span class="number">1</span>] </span><br><span class="line">                       + results[treatment_group_key][<span class="number">0</span>])</span><br><span class="line">            y_mean = <span class="number">1.0</span> * n_1 / n_total</span><br><span class="line">            nodeSummary[treatment_group_key] = [y_mean, n_total]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> results, nodeSummary </span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Divergence-related functions, from upliftpy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_kl_divergence</span>(<span class="params">pk, qk</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate KL Divergence for binary classification.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    pk (float): Probability of class 1 in treatment group</span></span><br><span class="line"><span class="string">    qk (float): Probability of class 1 in control group</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> qk &lt; <span class="number">0.1</span>**<span class="number">6</span>:</span><br><span class="line">        qk = <span class="number">0.1</span>**<span class="number">6</span></span><br><span class="line">    <span class="keyword">elif</span> qk &gt; <span class="number">1</span> - <span class="number">0.1</span>**<span class="number">6</span>:</span><br><span class="line">        qk = <span class="number">1</span> - <span class="number">0.1</span>**<span class="number">6</span></span><br><span class="line">    S = pk * np.log(pk / qk) + (<span class="number">1</span>-pk) * np.log((<span class="number">1</span>-pk) / (<span class="number">1</span>-qk))</span><br><span class="line">    <span class="keyword">return</span> S</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_evaluate_KL</span>(<span class="params">nodeSummary, control_group=<span class="string">&#x27;control&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the multi-treatment unconditional D (one node)</span></span><br><span class="line"><span class="string">    with KL Divergence as split Evaluation function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    nodeSummary (dict): a dictionary containing the statistics for a tree node sample</span></span><br><span class="line"><span class="string">    control_group (string, optional, default=&#x27;control&#x27;): the name for control group </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Notes</span></span><br><span class="line"><span class="string">    -----</span></span><br><span class="line"><span class="string">    The function works for more than one non-control treatment groups.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> control_group <span class="keyword">not</span> <span class="keyword">in</span> nodeSummary:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    pc = nodeSummary[control_group][<span class="number">0</span>]</span><br><span class="line">    d_res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> treatment_group <span class="keyword">in</span> nodeSummary:</span><br><span class="line">        <span class="keyword">if</span> treatment_group != control_group:</span><br><span class="line">            d_res += _kl_divergence(nodeSummary[treatment_group][<span class="number">0</span>], pc)</span><br><span class="line">    <span class="keyword">return</span> d_res</span><br></pre></td></tr></table></figure><p><br><br><strong>[Kullback-Leibler divergenceë¥¼ ì´ìš©í•œ Scoringì˜ ê³¼ì •]</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">method = <span class="string">&#x27;KL&#x27;</span></span><br><span class="line">n_bins = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">all_result = pd.DataFrame()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x_name_i <span class="keyword">in</span> X_names: </span><br><span class="line"></span><br><span class="line">    totalSize = <span class="built_in">len</span>(df.index)</span><br><span class="line">    x_bin = pd.qcut(df[x_name_i].values, n_bins, labels=<span class="literal">False</span>, </span><br><span class="line">                    duplicates=<span class="string">&#x27;raise&#x27;</span>)</span><br><span class="line">    d_children = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i_bin <span class="keyword">in</span> <span class="built_in">range</span>(x_bin.<span class="built_in">max</span>() + <span class="number">1</span>): <span class="comment"># range(n_bins):</span></span><br><span class="line">        nodeSummary = _GetNodeSummary(</span><br><span class="line">            data=df.loc[x_bin == i_bin], </span><br><span class="line">            experiment_group_column=<span class="string">&#x27;treatment_group_key&#x27;</span>, y_name=<span class="string">&#x27;conversion&#x27;</span></span><br><span class="line">        )[<span class="number">1</span>]</span><br><span class="line">        nodeScore = _evaluate_KL(nodeSummary,</span><br><span class="line">                                     control_group=<span class="string">&#x27;control&#x27;</span>)</span><br><span class="line">        nodeSize = <span class="built_in">sum</span>([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">list</span>(nodeSummary.values())])</span><br><span class="line">        d_children += nodeScore * nodeSize / totalSize</span><br><span class="line"></span><br><span class="line">    parentNodeSummary = _GetNodeSummary(</span><br><span class="line">        data=df, experiment_group_column=<span class="string">&#x27;treatment_group_key&#x27;</span>, y_name=<span class="string">&#x27;conversion&#x27;</span></span><br><span class="line">    )[<span class="number">1</span>]</span><br><span class="line">    d_parent = _evaluate_KL(parentNodeSummary, </span><br><span class="line">                                    control_group=<span class="string">&#x27;control&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">    d_res = d_children - d_parent</span><br><span class="line">        </span><br><span class="line">    one_result = pd.DataFrame(&#123;</span><br><span class="line">        <span class="string">&#x27;feature&#x27;</span>: x_name_i, </span><br><span class="line">        <span class="string">&#x27;method&#x27;</span>: method,</span><br><span class="line">        <span class="string">&#x27;score&#x27;</span>: d_res, </span><br><span class="line">        <span class="string">&#x27;p_value&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">        <span class="string">&#x27;misc&#x27;</span>: <span class="string">&#x27;number_of_bins: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">min</span>(n_bins, x_bin.<span class="built_in">max</span>()+<span class="number">1</span>)),<span class="comment"># format(n_bins),</span></span><br><span class="line">    &#125;, index=[<span class="number">0</span>]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    all_result = pd.concat([all_result, one_result])</span><br><span class="line"></span><br><span class="line">all_result = all_result.sort_values(by=<span class="string">&#x27;score&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">all_result[<span class="string">&#x27;rank&#x27;</span>] = all_result[<span class="string">&#x27;score&#x27;</span>].rank(ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">all_result[<span class="string">&#x27;method&#x27;</span>] = method + <span class="string">&#x27; filter&#x27;</span></span><br><span class="line"></span><br><span class="line">all_result[[<span class="string">&#x27;method&#x27;</span>, <span class="string">&#x27;feature&#x27;</span>, <span class="string">&#x27;rank&#x27;</span>, <span class="string">&#x27;score&#x27;</span>, <span class="string">&#x27;p_value&#x27;</span>, <span class="string">&#x27;misc&#x27;</span>]]</span><br></pre></td></tr></table></figure><p>[output]<br><img src="https://i.imgur.com/fgdy1vw.png" width="500px"></p><p><br></p><p><br></p><h4><span id="b-embedded-methods">B. <em>Embedded Methods</em></span></h4><p>ì´ ë°©ë²•ì€ uplift modelë¥¼ í›ˆë ¨ì‹œí‚¬ ë•Œ ë‚˜ì˜¤ëŠ” ë¶€ì‚°ë¬¼ë¡œ ë³€ìˆ˜ì˜ ì¤‘ìš”ì„±ì„ ì–»ëŠ”ë‹¤. ì´ê²ƒì€ meta-learnerê³¼ uplift tree ë‘˜ë‹¤ì—ì„œ ì–»ì–´ì§ˆ ìˆ˜ ìžˆë‹¤.</p><h5><span id="-meta-learner">- Meta-learner</span></h5><p>feature ì¤‘ìš”ë„ëŠ” base-learnerë¡œ ë¶€í„° ì–»ì–´ì§„ë‹¤.<br>ì˜ˆë¥¼ ë“¤ì–´ <em>Two Model approach</em> ì—ì„œëŠ” featureì˜ ì¤‘ìš”ë„ì ìˆ˜ëŠ” ë‘ base-learnerê°€ ì‚°ì¶œí•œ embeddingëœ ì¤‘ìš”ë„ ì ìˆ˜ì˜ í•©ìœ¼ë¡œ ì •ì˜ë  ìˆ˜ ìžˆë‹¤.</p><p><br></p><h5><span id="-uplift-tree">- Uplift tree</span></h5><p>featureì— ëŒ€í•œ ì¤‘ìš”ë„ ì ìˆ˜ëŠ” Treeì—ì„œ Tree nodeê°€ ë¶„í• ë˜ëŠ” ë™ì•ˆì˜ ì†ì‹¤í•¨ìˆ˜ì— ëŒ€í•œ ëˆ„ì ê¸°ì—¬ë¡œ ì •ì˜í•  ìˆ˜ ìžˆë‹¤. ì´ëŠ” ëŒ€ìƒì´ íŠ¹ë³„í•œ ë¶„í•  ê¸°ì¤€ì´ ìžˆëŠ” Uplift treeë¼ëŠ” ì ì„ ì œì™¸í•˜ë©´ ì¼ë°˜ì ìœ¼ë¡œ ìž˜ ì•Œë ¤ì§„ classification treeì˜ embedded feature ì¤‘ìš”ë„ì™€ ìœ ì‚¬í•˜ë‹¤. ê° ë¶„í• ì—ì„œ ìš°ë¦¬ëŠ” <strong>distribution divergence ì˜ ì¦ê°€ë¶„(gain)</strong> ì„ ê³„ì‚°í•œë‹¤.</p><p><span class="math inline">\(\Delta = \sum_{k=left,\space right} D(P_k:Q_k)- D(P:Q)\)</span><br>(<span class="math inline">\(P,Q\)</span>ëŠ” ê°ê° ì¹˜ë£Œêµ°ê³¼ ëŒ€ì¡°êµ°ì˜ Outcome distribution)</p><p>featureì˜ ì¤‘ìš”ë„ ì ìˆ˜ëŠ” í•´ë‹¹ featureê°€ ì‚¬ìš©ëœ ë…¸ë“œ ë¶„í• ë¡œ ë¶€í„° ë°œìƒí•˜ëŠ” ëª¨ë“  <span class="math inline">\(\Delta\)</span>ë¥¼ ë”í•˜ëŠ” ê²ƒìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìžˆë‹¤.</p><p><br></p><h4><span id="ì´í›„-ìƒëžµëœ-ë‚´ìš©">ì´í›„ ìƒëžµëœ ë‚´ìš© ;</span></h4><p>ìœ„ì—ì„œ ì†Œê°œí•œ ë³€ìˆ˜ì„ íƒ ë°©ë²•ë“¤ì˜ í‰ê°€ (synthetic data &amp; real-world data), ë…¼ë¬¸ì´ ì‹¤ì œ ì ìš©ì—ì„œ ì¶”ì²œí•˜ëŠ” ë°©ë²• ë“±</p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/UPLIFT-MODELING/">UPLIFT MODELING</category>
      
      <category domain="https://jaysung00.github.io/categories/UPLIFT-MODELING/b-Feature-Selection/">b.Feature Selection</category>
      
      
      <category domain="https://jaysung00.github.io/tags/Uplift-modeling/">Uplift modeling</category>
      
      <category domain="https://jaysung00.github.io/tags/Feature-Selection/">Feature Selection</category>
      
      
      <comments>https://jaysung00.github.io/2020/12/17/Selection/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ã€Uplift Modelingã€‘ë„ìž…ì˜ ë°°ê²½ì— ëŒ€í•´ ìƒê°í•´ë³´ìž</title>
      <link>https://jaysung00.github.io/2020/12/17/UM-overview/</link>
      <guid>https://jaysung00.github.io/2020/12/17/UM-overview/</guid>
      <pubDate>Thu, 17 Dec 2020 01:40:48 GMT</pubDate>
      
      <description>&lt;p&gt;í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” &lt;a href=&quot;https://towardsdatascience.com/a-quick-uplift-modeling-introduction-6e14de32bfe0&quot;&gt;Towards data science ì— ê²Œìž¬ëœ ë¸”ë¡œê·¸ &#39;Uplift Modeling: A Quick Introduction&#39;&lt;/a&gt;ì˜ ë‚´ìš©ì„ ì •ë¦¬ &amp;amp; ì¶”ê°€í•œ ë‚´ìš©ìž„ì„ ë°íž™ë‹ˆë‹¤.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” <a href="https://towardsdatascience.com/a-quick-uplift-modeling-introduction-6e14de32bfe0">Towards data science ì— ê²Œìž¬ëœ ë¸”ë¡œê·¸ 'Uplift Modeling: A Quick Introduction'</a>ì˜ ë‚´ìš©ì„ ì •ë¦¬ &amp; ì¶”ê°€í•œ ë‚´ìš©ìž„ì„ ë°íž™ë‹ˆë‹¤.</p><a id="more"></a><hr><h1><span id="introduction">* Introduction</span></h1><hr><blockquote><p>"ëˆ„ê°€ ë¯¸ëž˜ì— êµ¬ë§¤ë¥¼ í•  ê²ƒ ê°™ì€ê°€ ?"</p></blockquote><p>ê³ ì „ì ì¸ <em>ê²½í–¥ì„± ëª¨ë¸(propensity model)</em> ì´ í•˜ëŠ” ê²ƒì€ ì›ëž˜ êµ¬ë§¤ë¡œ ì´ì–´ì§€ë ¤ê³  í•œ ê³ ê°ì„ ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•´ ë°œê²¬í•´ë‚¼ ë¿ì´ë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ì›ëž˜ êµ¬ë§¤ë¥¼ í•˜ë ¤ê³  í–ˆì—ˆë˜ ê³ ê°ê³¼ ìº íŽ˜ì¸ì— ì˜í•´ êµ¬ë§¤ì„¤ë“ì´ í•„ìš”í•œ ê³ ê°ì„ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ”ë‹¤.</p><p><br></p><hr><h4><span id="ì´ë²ˆì—”-ìƒˆë¡­ê²Œ-ìº íŽ˜ì¸treatment-ì„-í†µí•œ-í”„ë¡œëª¨ì…˜ì„-í•œë‹¤ê³ -ìƒê°í•´ë³´ìž-ê³¼ì—°-ëˆ„êµ¬ë¥¼-íƒ€ê²Ÿìœ¼ë¡œ-í•´ì•¼í• ê¹Œ">ì´ë²ˆì—” ìƒˆë¡­ê²Œ ìº íŽ˜ì¸(treatment) ì„ í†µí•œ í”„ë¡œëª¨ì…˜ì„ í•œë‹¤ê³  ìƒê°í•´ë³´ìž. ê³¼ì—° ëˆ„êµ¬ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ í•´ì•¼í• ê¹Œ.</span></h4><p><br></p><blockquote><p>"ì–´ë– í•œ ê³ ê°ì´ <strong>ìº íŽ˜ì¸ì„ í†µí•œ ë§¤ìƒì´ ë†’ì„ ê²ƒ</strong> ì¸ê°€?"</p></blockquote><p>ìœ„ì™€ ê°™ì€ ì§ˆë¬¸ì„ ê¸°ì¤€ìœ¼ë¡œ ê¸°ëŒ“ê°’ì´ ë†’ì€ ì‚¬ëžŒì„ íƒ€ê²Ÿìœ¼ë¡œ í•  ìˆ˜ ìžˆì„ ê²ƒì´ë‹¤. ì´ê²ƒì„ ì˜ˆì¸¡í•´ì„œ íƒ€ê²ŸíŒ…ì„ í•˜ëŠ” ëª¨ë¸ì„ <em>Outcome model</em> ì´ë¼ê³  í•  ìˆ˜ìžˆë‹¤.</p><p>ì¦‰, íƒ€ê²Ÿë³€ìˆ˜ë¥¼ <span class="math inline">\(Outcome = P(buy|treatment)\)</span> ë¡œ ì„¤ì •í•œ ê²ƒì´ë‹¤.</p><p><br></p><blockquote><p>"ìº íŽ˜ì¸ì´ ê³ ê°ì—ê²Œ ì‹¤ì œ ìš°ë¦¬íšŒì‚¬ ì œí’ˆì˜ êµ¬ë§¤ë¥¼ <strong>ìœ ë°œí–ˆë‚˜</strong> ?"<br>" <strong>ì´ë¯¸ ì‚¬ë ¤ê³  í–ˆë˜ ì‚¬ëžŒì—ê²Œ ìº íŽ˜ì¸ì„ í•˜ëŠ” ë‚­ë¹„</strong> ë¥¼ í•˜ì§€ëŠ” ì•Šì•˜ë‚˜?"<br>"ìº íŽ˜ì¸ì´ ëˆ„êµ°ê°€ì˜ <strong>êµ¬ë§¤ë¥¼ ë”ìš± ì•…í™”</strong> ì‹œí‚¤ì§€ëŠ” ì•Šì•˜ë‚˜?"</p></blockquote><p>ê·¸ëŸ¬ë‚˜ <em>Uplift modeling</em> ì€ ìœ„ì™€ ê°™ì€ ë”ìš± ì¤‘ìš”í•œ ì§ˆë¬¸ì— ë‹µí•˜ê³ ìž í•œë‹¤.</p><p>ì´ê²ƒì€ íƒ€ê²Ÿë³€ìˆ˜ë¥¼ <span class="math inline">\(Lift = P(buy|treatment) - P(buy|no treatment)\)</span>ë¡œ ë‘” ê²ƒ ê³¼ ê°™ë‹¤.</p><p><br></p><p><img src="https://i.imgur.com/0YNzU0R.png" width="450px"></p><p><br></p><p>ìœ„ì˜ ê·¸ë¦¼ (Yi and Frost <a href="https://tech.wayfair.com/data-science/2018/10/pylift-a-fast-python-package-for-uplift-modeling/">2018a</a>) ê³¼ ê°™ì´ ê³ ê°ì´ ìº íŽ˜ì¸ì˜ ëŒ€ìƒì´ ë˜ëŠ”ì§€ ì—¬ë¶€ì™€ ê·¸ì— ë”°ë¥¸ ê³ ê°ì˜ í–‰ë™ì— ë”°ë¼ 4ê°€ì§€ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ê³ ê°ì˜ íƒ€ìž…ì„ ë¶„ë¥˜í•  ìˆ˜ ìžˆë‹¤.</p><ul><li><p><strong>'persuadables'</strong> :<br>ë§ˆì¼€íŒ… ìº íŽ˜ì¸ì— ë…¸ì¶œì´ ë˜ë©´ êµ¬ë§¤ë¥¼ í•˜ì§€ë§Œ ë…¸ì¶œë˜ì§€ ì•Šìœ¼ë©´ êµ¬ë§¤í•˜ì§€ ì•ŠëŠ” ê·¸ë£¹<br><span class="math inline">\(Lift = P(buy|treatment) - P(buy|no treatment) = 1\)</span></p></li><li><p><strong>'sure things'</strong> :<br>ìº íŽ˜ì¸ê³¼ ê´€ê³„ì—†ì´ ì–´ì§œí”¼ êµ¬ë§¤í•  ì˜ˆì •ì¸ ê·¸ë£¹<br><span class="math inline">\(Lift = P(buy|treatment) - P(buy|no treatment) = 0\)</span></p></li><li><p><strong>'lost causes'</strong> :<br>ìº íŽ˜ì¸ê³¼ ê´€ê³„ì—†ì´ ì–´ì§œí”¼ êµ¬ë§¤í•˜ì§€ ì•Šì„ ê·¸ë£¹<br><span class="math inline">\(Lift = P(buy|treatment) - P(buy|no treatment) = 0\)</span></p></li><li><p><strong>'sleeping dogs'</strong> :<br>ìº íŽ˜ì¸ì— ë…¸ì¶œë˜ì§€ ì•Šìœ¼ë©´ êµ¬ë§¤í•˜ì§€ë§Œ ì˜¤ížˆë ¤ ë…¸ì¶œë  ê²½ìš° êµ¬ë§¤ë¥¼ í•˜ì§€ ì•Šê²Œë˜ëŠ” ê·¸ë£¹<br>('ì´ëŸ° ê´‘ê³ ì— ëˆì„ ì“°ëŠ” íšŒì‚¬ì˜ ì œí’ˆì„ êµ¬ë§¤í•˜ê³  ì‹¶ì§€ ì•Šì•„!' í˜¹ì€ 'ë‚˜ì˜ í”„ë¼ì´ë²„ì‹œê°€ ì´ìš©ë˜ëŠ” ê³³ì— ëˆì„ ì“°ê³  ì‹¶ì§€ ì•Šì•„!' ë“±ê³¼ ê°™ì€ ì´ìœ )<br><span class="math inline">\(Lift = P(buy|treatment) - P(buy|no treatment) = -1\)</span></p></li></ul><p><br></p><p>ëª¨ë“  ê³ ê°ë“¤ì— ëŒ€í•´ ì†Œì†ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¯¸ë¦¬ ì•Œ ìˆ˜ ìžˆëŠ” ì´ìƒì ì¸ ì„¸ê³„ê°€ ì¡´ìž¬í•œë‹¤ë©´, ê·¸ì— ë”°ë¼ 'Persuadables' ì„¸ê·¸ë¨¼íŠ¸ì˜ ê³ ê°ë“¤ë§Œ íƒ€ê²Ÿì— ë„£ê³  'Sleeping dogs' ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì ˆëŒ€ ê³ ê°ì€ ë„£ì§€ ì•Šì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ì‹¤ì—ì„œëŠ” ê° ê³ ê°ì´ ì–´ëŠ ì„¸ê·¸ë¨¼íŠ¸ì˜ ê³ ê°ì¸ì§€ ì•„ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ê·¸ ëŒ€ì‹ , í†µê³„ì˜ íž˜ê³¼ ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ <strong>í•´ë‹¹ê³ ê°ê³¼ "ë¹„ìŠ·í•œ ê³ ê°"ì´ í‰ê· ì ìœ¼ë¡œ ì–´ëŠ ì„¸ê·¸ë¨¼íŠ¸ì— ì†í•´ ìžˆëŠ”ì§€</strong> ëŠ” ì•Œ ìˆ˜ ìžˆì„ ê²ƒì´ë‹¤. ì´ê²ƒì´ Uplift modelingì´ ìš°ë¦¬ì—ê²Œ ì•Œë ¤ì£¼ëŠ” ê²ƒì´ë‹¤.</p><p>ëª¨ë“  ê°œì¸ì€ -1 ë¶€í„° 1 ì‚¬ì´ì˜ liftê°’ì„ ê°–ê²Œ ë˜ê³  ìš°ë¦¬ëŠ” ì´ ê°’ì„ í†µí•´ íƒ€ê²Ÿì„ ê²°ì •í•  ê²ƒì´ë‹¤.<br>ë§Œì•½ ëª¨ë¸ì´ ì •í™•í•˜ë‹¤ë©´, <strong>ë†’ì€ liftê°’ì„ ê°€ì§„ ê³ ê°ì—ê²Œ ë” ë†’ì€ ìº íŽ˜ì¸ íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìžˆì„ ê²ƒì´ê³  ë‚®ì€ liftê°’ì˜ ê³ ê°ì—ê²ŒëŠ” ë‚®ì€ ìº íŽ˜ì¸ íš¨ê³¼ê°€ ë‚˜íƒ€ë‚  ê²ƒì´ë‹¤.</strong></p><p><br></p><hr><h1><span id="uplift-modelingì˜-ì ‘ê·¼ë°©ë²•">* Uplift modelingì˜ ì ‘ê·¼ë°©ë²•</span></h1><hr><p>Uplift modelingì€ íŠ¹ì •ê³ ê°ì—ê²Œ ìº íŽ˜ì¸ì„ ì œê³µí•˜ëŠ”ê²ƒì´ ì´ë“ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ê²°ì •í•˜ëŠ” taskë¥¼ ìœ„í•´ ë§Œë“¤ì–´ì¡Œê³ , ì´ê²ƒì€ <strong>ì–´ë–¤ ê³ ê°ì´ ì–´ë–¤ ì„¸ê·¸ë¨¼íŠ¸ì— ì†í•˜ëŠ”ì§€ ê²°ì •í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒ</strong> ì´ë©°, ê²°ê³¼ì ìœ¼ë¡œ <strong>ë§ˆì¼€íŒ… ìˆ˜ë‹¨ì´ ê³ ê°ì˜ êµ¬ë§¤ë¡œ ì´ì–´ì§€ëŠ” í™•ë¥ ì„ ê²°ì •í•˜ëŠ” ê²ƒì„ ë•ëŠ” ëª¨ë¸ë§</strong> ì´ë‹¤.</p><p>ì´ëŸ¬í•œ Uplift modelingì€ ì—¬ëŸ¬ê°€ì§€ ì ‘ê·¼ ë°©ë²•ìœ¼ë¡œ ì—°êµ¬ë˜ì–´ì˜¤ê³  ìžˆë‹¤.</p><p>ë‹¤ë¥¸ í¬ìŠ¤íŒ…ì—ì„œ ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ì ‘ê·¼ë°©ë²•ì— ëŒ€í•´ ìžì„¸ížˆ ë‹¤ë£¨ê²Œ ë  ê²ƒì´ë‹¤.</p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/UPLIFT-MODELING/">UPLIFT MODELING</category>
      
      <category domain="https://jaysung00.github.io/categories/UPLIFT-MODELING/a-Overall/">a.Overall</category>
      
      
      <category domain="https://jaysung00.github.io/tags/Uplift-modeling/">Uplift modeling</category>
      
      
      <comments>https://jaysung00.github.io/2020/12/17/UM-overview/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ã€Causal Inferenceâ‘¡ã€‘ì¡°ìž‘ë³€ìˆ˜ë²•(IV)ì— ê´€í•˜ì—¬</title>
      <link>https://jaysung00.github.io/2020/12/04/CN2/</link>
      <guid>https://jaysung00.github.io/2020/12/04/CN2/</guid>
      <pubDate>Fri, 04 Dec 2020 08:34:44 GMT</pubDate>
      
      <description>&lt;hr /&gt;</description>
      
      
      
      <content:encoded><![CDATA[<hr><a id="more"></a><h1><span id="rctë¥¼-ì‚¬ìš©í• -ìˆ˜-ì—†ì„-ë•ŒëŠ”-ì–´ë–»ê²Œ-í• ê¹Œ">* RCTë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•ŒëŠ” ì–´ë–»ê²Œ í• ê¹Œ?</span></h1><hr><p>Level 1. ì‹¤í—˜(Experimental) ë ˆë²¨ (ê°œìž…ì—°êµ¬)</p><ul><li>ê´€ì°°ìž(í•´ì„ìž)ê°€ ê°œìž…ì˜ ê³„íšì„ ì„¸ì›Œì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ <span class="math inline">\(\rightarrow\)</span> RCT (ë¬´ìž‘ìœ„í™”ë¹„êµì‹¤í—˜)</li></ul><p><br></p><h3><span id="blacktriangleright-level-2-ì¤€ì‹¤í—˜quasi-experimental-ë ˆë²¨"><span class="math inline">\(\blacktriangleright\)</span> Level 2. ì¤€ì‹¤í—˜(Quasi Experimental) ë ˆë²¨</span></h3><ul><li><p>ë¹„êµì  ì§ˆ ì¢‹ì€ ê´€ì°°ì—°êµ¬ ì •ë„ì˜ ëŠë‚Œ</p></li><li><p>ì‹¤í—˜ë°ì´í„°ê°€ ì•„ë‹ˆë¼ <strong>ê´€ì°°ë°ì´í„°</strong> ë¥¼ ì‚¬ìš©í•´ì„œ, ê°œìž…ì˜ ì—¬ë¶€ ë“±ìœ¼ë¡œ ê²°ê³¼ì—ì˜ ì˜í–¥ì„ ì¶”ì •</p></li><li><p><strong>ì¸¡ì •ë˜ì§€ ì•Šì€ êµëž€ì¸ìž</strong>ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìžˆëŠ” ê²ƒì€ ì‚¬ì‹¤ìƒ RCTë§Œì´ ì‰½ê²Œ ê°€ëŠ¥í•˜ì§€ë§Œ, <strong>ëª¨ë“  êµëž€ì¸ìžë¥¼ ì¶©ë¶„ížˆ ì¸¡ì •í•  ìˆ˜ ìžˆë‹¤ê³  í•˜ë©´</strong> ì¤€ì‹¤í—˜ì„¤ê³„ë¡œë„ ì¶©ë¶„ížˆ ì¸ê³¼ê´€ê³„ë¥¼ ì„¤ëª…í•  ìˆ˜ ìžˆë‹¤.</p></li><li><p><strong>ë³€ìˆ˜ì¡°ìž‘ë²•(IV)</strong> , <strong>ì°¨ì˜ ì°¨ ë¶„ì„(DID)</strong> , <strong>ê²½í–¥ìŠ¤ì½”ì–´ ë§¤ì¹­(PS)</strong> , <strong>íšŒê·€ë¶ˆì—°ì† ë””ìžì¸(RDD)</strong> ë“±ì´ ìžˆë‹¤.</p></li></ul><p><br></p><p>Level 3. ê´€ì°°(Observation) ë ˆë²¨</p><ul><li>ë”ìš± ì¸ê³¼ì¶”ë¡  í•˜ê¸°ì— ì·¨ì•½í•œ ê´€ì°°ì—°êµ¬ ë””ìžì¸</li></ul><p><br></p><hr><h1><span id="i-ì¡°ìž‘ë³€ìˆ˜ë²•-iv-instrumental-variable-methods"><span class="math inline">\(I\)</span>. ì¡°ìž‘ë³€ìˆ˜ë²• (IV; Instrumental variable methods)</span></h1><hr><h3><span id="-ì¡°ìž‘ë³€ìˆ˜ë²•ì´ëž€">- ì¡°ìž‘ë³€ìˆ˜ë²•ì´ëž€?</span></h3><ul><li>ë§¤ìš° ì–´ë µì§€ë§Œ ì´ë¡ ìƒ ì™„ë²½í•˜ê²Œ ì„¤ê³„ëœë‹¤ë©´ <strong>ì¸¡ì •ë˜ì§€ ì•Šì€ êµëž€ì¸ìž</strong> ë„ ì²˜ë¦¬í•  ìˆ˜ ìžˆëŠ” ë°©ë²•.</li></ul><p><br></p><ul><li><p><strong>ì™¸ìƒë³€ìˆ˜</strong> ; ëª¨ë¸ì˜ ë°–ì—ì„œ ê²°ì •ë˜ì–´ ì£¼ì–´ì§„ ë³€ìˆ˜ (ì„¤ëª…ë³€ìˆ˜ë¡œì¨ ë°”ëžŒì§í•˜ë‹¤).</p><ul><li><strong>ëª¨ë¸ì˜ ìž”ì°¨í•­</strong> ê³¼ ë…ë¦½í•˜ê²Œ ëœë‹¤.</li></ul></li></ul><p><br></p><ul><li><p><strong>ë‚´ìƒë³€ìˆ˜</strong> ; ëª¨ë¸ ë‚´ì—ì„œ ê²°ì •ë˜ëŠ” ë³€ìˆ˜ (ì„¤ëª…ë³€ìˆ˜ë¡œì¨ ë°”ëžŒì§í•˜ì§€ ì•Šë‹¤).</p><ul><li>ì˜ˆë¥¼ ë“¤ì–´ <em>ì¸¡ì •ë˜ì§€ ì•Šì€ êµëž€ì¸ìž</em> ê°€ ìžˆëŠ” ê²½ìš° ì˜í–¥ì„ ë°›ëŠ” <strong>ì„¤ëª…ë³€ìˆ˜ì™€ ëª¨ë¸ì˜ ìž”ì°¨í•­ ì‚¬ì´ì— ìƒê´€</strong> ê´€ê³„ê°€ ìƒê²¨ë²„ë¦¬ê³  ì´ ì„¤ëª…ë³€ìˆ˜ëŠ” <strong>ë‚´ìƒë³€ìˆ˜</strong> ê°€ ë˜ì–´ <em>ë‚´ìƒì„± ë°”ì´ì–´ìŠ¤</em> ë¥¼ ë§Œë“ ë‹¤.</li></ul></li></ul><p><br></p><ul><li><p>ì´ë ‡ê²Œ ë°œìƒëœ <strong>ë‚´ìƒë³€ìˆ˜</strong> ë¥¼ <strong>ì™¸ìƒí™”</strong> í•˜ê¸° ìœ„í•´ ë„ìž…ëœ ë³€ìˆ˜ <span class="math inline">\(Z\)</span>ë¥¼ <strong>ì¡°ìž‘ë³€ìˆ˜(IV)</strong> ë¼ê³  í•œë‹¤.</p></li><li><p>ì´ëŸ¬í•œ ì¡°ìž‘ë³€ìˆ˜ë¥¼ ì´ìš©í•´, ì„¤ëª…ë³€ìˆ˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ <strong>ì¡°ìž‘ë³€ìˆ˜ë²•</strong> ì´ë¼ê³  í•œë‹¤.</p></li></ul><p><br></p><blockquote><p><span class="math inline">\([ ì˜ˆì‹œ ]\)</span><br><img src="https://i.imgur.com/Q8xihRF.png" width="550px"><br>- ì°¸ì˜ ê´€ê³„ : <span class="math inline">\(ln(wage) = a + b \cdot educ + c \cdot ability + u\)</span></p></blockquote><p>ìž„ê¸ˆ(Wage)ì™€ êµìœ¡ë…„ìˆ˜(Education)ì™€ ëŠ¥ë ¥(Ability)ì€ ìœ„ì™€ ê°™ì€ ê´€ê³„ê°€ ìžˆë‹¤ê³  ê°€ì •í•œë‹¤.</p><p>ê·¸ëŸ¬ë‚˜ ëŠ¥ë ¥ì€ ì‹¤ì œë¡œ ì¸¡ì •ë¶ˆê°€í•˜ê¸° ë•Œë¬¸ì— êµìœ¡ìˆ˜ì¤€ë§Œìœ¼ë¡œ ìž„ê¸ˆê³¼ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ëª¨ë¸ì„ í–ˆë‹¤ê³  í•˜ìž.</p><ul><li><span class="math inline">\(ln(wage) = a&#39; + b&#39; \cdot educ + v\)</span></li></ul><p>ì´ ë•Œ, ëŠ¥ë ¥(Ability)ì€ <strong>ê´€ì¸¡ë˜ì§€ ì•Šì€ êµëž€ì¸ìž</strong> ê°€ ë˜ê³  ê³„ìˆ˜ <span class="math inline">\(b&#39;\)</span>ì—ëŠ” ë‚´ìƒì„± ë°”ì´ì–´ìŠ¤ê°€ ì¡´ìž¬í•˜ê²Œ ëœë‹¤.</p><p><br></p><h3><span id="-ì¡°ìž‘ë³€ìˆ˜ë²•ì˜-ê³¼ì •">- ì¡°ìž‘ë³€ìˆ˜ë²•ì˜ ê³¼ì •</span></h3><p><strong>ë‚´ìƒë³€ìˆ˜</strong> ë¥¼ í”¼ì„¤ëª…ë³€ìˆ˜ë¡œ ë³„ë„ì˜ ì¡°ìž‘ë³€ìˆ˜ <span class="math inline">\(IV\)</span>ë¥¼ í†µí•´ ì„¤ëª…í•˜ëŠ” ëª¨ë¸ì„ ìž‘ì„±í•œë‹¤.</p><ul><li><span class="math inline">\(\begin{align} ln(wage) &amp;= a&#39; + b&#39; \cdot educ + \underline v \\ &amp;= a&#39; + b&#39; \cdot educ + \underline {c&#39; \cdot X+ v&#39;} \end{align}\)</span></li></ul><p><span class="math inline">\(\space\space\space\space\space\space\space\space\space\space\space\space\space\)</span> (Cë¥¼ ìž”ì°¨í•­ì— í¬í•¨ëœ <em>êµëž€ì¸ìž</em> ë¼ê³  ê°€ì •)</p><ul><li><span class="math inline">\(\bf educ = \alpha + \beta \cdot \underline {IV} + \gamma \cdot X + \epsilon\)</span><br><br><br><img src="https://i.imgur.com/SJ5bifx.png" width="600px"></li></ul><p><br></p><ul><li><p><strong>ì¡°ìž‘ë³€ìˆ˜ <span class="math inline">\(\bf IV\)</span>ì˜ ì¡°ê±´</strong></p><ol type="1"><li><p><code>Exclusion restriction</code> : IVëŠ” ì›ëž˜ì˜ ê²°ê³¼ë¶€ë¶„ì— í•´ë‹¹í•˜ëŠ” ë³€ìˆ˜(Wage)ì—ê²Œ ê°œìž…ë³€ìˆ˜(ì„¤ëª…ë³€ìˆ˜, Education)ë¥¼ í†µí•´ì„œë§Œ ì˜í–¥ì„ ì¤„ ìˆ˜ìžˆë‹¤.</p></li><li><p><code>No instrument-outcome confounder</code>ï¼šIVì™€ ì›ëž˜ì˜ ê²°ê³¼ë¶€ë¶„ì— í•´ë‹¹í•˜ëŠ” ë³€ìˆ˜(Wage)ì—ê²Œ ë™ì‹œì— ì˜í–¥ì„ ì£¼ëŠ” <strong>ê³µí†µì˜ ì›ì¸(L ë˜ëŠ” X)</strong> ì´ ì¡´ìž¬í•˜ì§€ ì•Šì•„ì•¼ í•œë‹¤</p></li><li><p><code>Instrument relevance</code> : ê°œìž…ë³€ìˆ˜(ì„¤ëª…ë³€ìˆ˜, Education)ì—ê²ŒëŠ” í™•ì‹¤ížˆ ì˜í–¥ì„ ì£¼ëŠ” ë³€ìˆ˜ì—¬ì•¼ í•œë‹¤.</p></li><li><p><code>Monotonicity</code> : ì¡°ìž‘ë³€ìˆ˜ê°€ ì—­íš¨ê³¼ë¥¼ ë‚´ëŠ” ì‚¬ëžŒ(Defiers)ì´ ì¡´ìž¬í•˜ì§€ ì•Šì•„ì•¼ í•œë‹¤. (ì™„ì „ížˆ ë°˜ëŒ€ë¡œ ì›€ì§ì´ëŠ” ì¼€ì´ìŠ¤)</p></li></ol></li></ul><p>ì´ìƒì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¡°ìž‘ë³€ìˆ˜ë¥¼ ë°œê²¬í•´, ì´ë¥¼ í†µí•´ êµëž€ì¸ìžì— ì˜í•œ íš¨ê³¼ë¥¼ ì œê±°í•œë‹¤.<br><br></p><h4><span id="ê·¸ëŸ¬ë‚˜-ì´ëŸ¬í•œ-ì¡°ê±´ì„-ë§Œì¡±í•˜ëŠ”-ì¡°ìž‘ë³€ìˆ˜ë¥¼-ì°¾ì•„ë‚´ëŠ”-ê²ƒì€-ë§¤ìš°-ì–´ë µê³ -íŠ¹ížˆ-ë¹„ì¦ˆë‹ˆìŠ¤-í˜„ìž¥ì—ì„œëŠ”-ê±°ì˜-ë¶ˆê°€ëŠ¥ì—-ê°€ê¹ë‹¤">ã€ê·¸ëŸ¬ë‚˜, ì´ëŸ¬í•œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¡°ìž‘ë³€ìˆ˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì€ ë§¤ìš° ì–´ë µê³  íŠ¹ížˆ ë¹„ì¦ˆë‹ˆìŠ¤ í˜„ìž¥ì—ì„œëŠ” ê±°ì˜ ë¶ˆê°€ëŠ¥ì— ê°€ê¹ë‹¤.ã€‘</span></h4><p><br></p><hr><h2><span id="i-i-ì²˜ì¹˜ì˜ë„ì—-ì˜í•œ-ë¶„ì„-intention-to-treat-analysis"><span class="math inline">\(I-I.\)</span> ì²˜ì¹˜ì˜ë„ì— ì˜í•œ ë¶„ì„ (Intention to treat analysis)</span></h2><h3><span id="spacespacespacespacespace-ê·¸ëŸ¼ì—ë„-ë¶ˆêµ¬í•˜ê³ -ìƒê°í•´ë³´ëŠ”-ì¡°ìž‘ë³€ìˆ˜ë²•ivì˜-í™œìš©-ê°€ëŠ¥ì„±"><span class="math inline">\(\space\space\space\space\space\)</span> - ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ìƒê°í•´ë³´ëŠ” ì¡°ìž‘ë³€ìˆ˜ë²•(IV)ì˜ í™œìš© ê°€ëŠ¥ì„±</span></h3><hr><p><br><br>- ì‹¤ì œ ê°œìž…ì—ì„œëŠ” ê°œìž…ì˜ ëŒ€ìƒì´ì§€ë§Œ ë”°ë¥´ì§€ ì•ŠëŠ” ê²½ìš°ë‚˜ ëŒ€ìƒì´ ì•„ë‹ˆì§€ë§Œ ë”°ë¥´ëŠ” ê²½ìš°ì™€ ê°™ì´ ì°¸ê°€ìžê°€ ê°œìž…ì˜ë„ì™€ ë°˜ëŒ€ë¡œ ì›€ì§ì´ëŠ” ê²½ìš°ê°€ ì¡´ìž¬í•œë‹¤.</p><ul><li><p><em>'ì •ì±…ì´ë‚˜ ì¹˜ë£Œì™€ ê°™ì€ ê°œìž…ì˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œ ê°œìž…ì˜ë„ëŒ€ë¡œ ì›€ì§ì´ëŠ” ë¶€ë¶„ì§‘ë‹¨ë§Œì„ ë¹„êµí•˜ëŠ” ê²ƒì´ ì¢‹ì§€ ì•Šì„ê¹Œ'</em> ë¼ëŠ” ì´ë¡ </p></li><li><p>ê°œìž…ì„ í–‰í•˜ëŠ” ì°¸ê°€ìžì˜ ì˜ë„ë¥¼ <strong>1ê³¼ 0ì„ ê°–ëŠ” dummy ì¡°ìž‘ë³€ìˆ˜ <span class="math inline">\(z\)</span></strong> ë¡œ ìƒê°í•œë‹¤.</p></li></ul><p><img src="https://i.imgur.com/qCI2qOc.png" width="600px"></p><p><br></p><ul><li><span class="math inline">\(\begin{equation}z= \left \{\begin{array}{l}1ã€€(ê°œìž…ì˜ë„ ìžˆìŒ) \\0ã€€(ê°œìž…ì˜ë„ì—†ìŒ)\end{array}\right.\end{equation}\)</span></li></ul><p><br></p><ul><li><span class="math inline">\(d = zd_1 + (1-z)d_0\)</span><br>(<span class="math inline">\(d\)</span>ëŠ” <span class="math inline">\(z\)</span>ê°€ 1ì¼ ë•Œ <span class="math inline">\(d_1\)</span>ì´ ë˜ê³ , <span class="math inline">\(z\)</span>ê°€ 0ì¼ ë•Œ <span class="math inline">\(d_0\)</span>ì´ ëœë‹¤.)</li></ul><p><br></p><ul><li><span class="math inline">\(\begin{equation}d= \left \{\begin{array}{l}1ã€€(ì‹¤í–‰) \\0ã€€(ì‹¤í–‰í•˜ì§€ì•ŠìŒ)\end{array}\right.\end{equation}\)</span></li></ul><p><br></p><ul><li><span class="math inline">\(y=dy_1 + (1-d)y_0\)</span><br>(ë§ˆì°¬ê°€ì§€ë¡œ <span class="math inline">\(y\)</span>ëŠ” <span class="math inline">\(d\)</span>ê°€ 1ì¼ ë•Œ <span class="math inline">\(y_1\)</span>ì´ ë˜ê³ , <span class="math inline">\(d\)</span>ê°€ 0ì¼ ë•Œ <span class="math inline">\(y_0\)</span>ì´ ëœë‹¤.)</li></ul><p><br></p><ul><li><p>ì¡°ìž‘ë³€ìˆ˜<span class="math inline">\(z\)</span>ì˜ ê°€ì •</p><ul><li><p><span class="math inline">\(\bf (y_0,y_1) \perp z|d,\)</span> : ë³€ìˆ˜ <span class="math inline">\(d\)</span>ì— ì˜í•´ <span class="math inline">\(z\)</span>ì™€ <span class="math inline">\(y\)</span>ê°€ <a href="https://jaysung00.github.io/2020/11/14/BN1/"><span class="math inline">\(d\)</span>-seperate</a> ë˜ë¯€ë¡œ <code>Exclusion restriction</code> ì™€ <code>No instrument-outcome confounder</code> ë§Œì¡±</p></li><li><p><span class="math inline">\(\bf d_{i1} \geq d_{i0}\)</span> : <code>Monotonicity</code> ë§Œì¡± (ì œë¹„ì— ë½‘ížˆë©´ ê³µë¦½ì— ê°€ê³ , ì œë¹„ì— ë–¨ì–´ì§€ë©´ ì‚¬ë¦½ì— ê°€ëŠ” DefiersëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ”ë‹¤ê³ )</p></li><li><p>(<span class="math inline">\(d\)</span>ì™€ <span class="math inline">\(z\)</span>ì˜ ì •ì˜ì— ì˜í•´ <code>Instrument relevance</code>ëŠ” ìžë™ìœ¼ë¡œ ë§Œì¡±)</p></li></ul></li></ul><p><br></p><table><thead><tr class="header"><th style="text-align: center;">ì„¤ê³„ìžì˜ ì˜ë„ <span class="math inline">\(z\)</span></th><th style="text-align: center;">ì°¸ê°€ìžì˜ ì˜ì‚¬ <span class="math inline">\(d\)</span></th><th style="text-align: center;">Notation</th><th style="text-align: left;">ìƒí™©ì˜ í•´ì„</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;"><span class="math inline">\(d_1\)</span> = 1</td><td style="text-align: left;">ê°œìž…ì˜ ëŒ€ìƒì´ ë˜ì–´ì„œ ì°¸ê°€ìžê°€ ê°œìž…ì˜ë„ì— ë§žê²Œ ë”°ë¥´ëŠ” ê²½ìš°</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;"><span class="math inline">\(d_1\)</span> = 0</td><td style="text-align: left;">ê°œìž…ì˜ ëŒ€ìƒì´ ë˜ì—ˆì§€ë§Œ ì°¸ê°€ìžê°€ ê°œìž…ì˜ë„ì— ë”°ë¥´ì§€ ì•ŠëŠ” ê²½ìš° (noncompliance)</td></tr><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;"><span class="math inline">\(d_0\)</span> = 1</td><td style="text-align: left;">ê°œìž…ì˜ ëŒ€ìƒì´ ì•„ë‹˜ì—ë„ ë¶ˆêµ¬í•˜ê³  ê°œìž…ì˜ë„ì˜ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ëŠ” ê²½ìš° (noncompliance)</td></tr><tr class="even"><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;"><span class="math inline">\(d_0\)</span> = 0</td><td style="text-align: left;">ê°œìž…ì˜ ëŒ€ìƒì´ ì•„ë‹ˆì˜€ê¸° ë•Œë¬¸ì— ê°œìž…ì˜ë„ì— ë”°ë¥´ì§€ ì•ŠëŠ” ê²½ìš°</td></tr></tbody></table><p><br></p><h3><span id="-êµ­ì†Œì -í‰ê· íš¨ê³¼late-local-average-treatment-effect">- <strong>êµ­ì†Œì  í‰ê· íš¨ê³¼(LATE; local average treatment effect)</strong></span></h3><p><br></p><ul><li><p>ì •ì˜ ; <span class="math inline">\(\Large LATE = E(y_1-y_0|d_1=1,d_0=0)\)</span></p></li><li><p>ì˜ë¯¸ ; ê°œìž…ì˜ë„ì˜ ë°©í–¥ëŒ€ë¡œ ì›€ì§ì—¬ì£¼ëŠ” ë¶€ë¶„ì§‘í•©ì—ì„œë§Œ ì¸¡ì •í•œ íš¨ê³¼</p></li></ul><p><br></p><h3><span id="-lateì˜-ì¶”ì •">- LATEì˜ ì¶”ì •</span></h3><p><br></p><p>ì¡°ìž‘ë³€ìˆ˜ <span class="math inline">\(z\)</span>ì˜ ê°€ì •ì— ì˜í•´,</p><p><span class="math inline">\(\begin{align} E(y|z=1) - E(y|z=0) &amp;= E(dy_1+(1-d)y_0|z=1) - E(dy_1+(1-d)y_0| z=0)\\ &amp;= E(d_1y_1+(1-d_1)y_0|z=1)-E(d_0y_1+(1-d_0)y_0|z=0) \\ &amp;= E(d_1y_1 + (1-d)y_0)-E(d_0y_1 + (1-d_0)y_0)\\ &amp;= E((d_1-d_0)(y_1-y_0)) \\ &amp;\space \\ &amp;\space\space\space\space\space\space\space\space\space\space\space\cdots d_1-d_0ëŠ”\space\{-1,0,1\},\space\space Monotonicityê°€ì •ì—\space\spaceì˜í•´\space\space p(d_1-d_0=-1)=0 \space\space ì´ë¯€ë¡œ\\ &amp;\space \\ &amp;=\sum_{a=-1,0,1}aE(y_1-y_0|d_1-d_0 = a)p(d_1-d_0 = a) \\ &amp;= \underline {E(y_1-y_0|d_1-d_0=1)} \space p(d_1-d_0=1) \end{align}\)</span></p><p><br></p><p>ìœ„ ì‹ì˜ ìš°ë³€ì— <span class="math inline">\(E(y_1-y_0|d_1-d_0=1)\)</span>ê°€ <span class="math inline">\(\bf LATE\)</span> ì˜ ì •ì˜ê°€ ë˜ë¯€ë¡œ,</p><p><br></p><p><span class="math inline">\(\begin{align} {\bf LATE} &amp;= E(y_1-y_0|d_1-d_0=1) \\ &amp;\space \\ &amp;= \cfrac{E(y|z=1) - E(y|z=0)}{p(d_1-d_0=1)} \\&amp;\space \\ &amp;= \cfrac{E(y|z=1) - E(y|z=0)}{ p(d_1=1,d_0=0)} \\ &amp;\space \\ &amp;= \cfrac{E(y|z=1) - E(y|z=0)}{\{p(d_1=1,d_0=1) + p(d_1=1,d_0=0)\} - p(d_0=1,d_1=1)} \\ &amp;\space \\ &amp;= \cfrac{E(y|z=1) - E(y|z=0)}{p(d_1=1)-p(d_0=1)} \\ &amp;\space \\ &amp;= {\bf \cfrac{E(y|z=1) - E(y|z=0)}{E(d|z=1)-E(d|z=0)}} \end{align}\)</span></p><p><br></p><p>ë¡œ ë°”ê¿”ì“°ëŠ” ê²ƒì´ ê°€ëŠ¥í•´, ì´ê²ƒì€ <span class="math inline">\(z\)</span>ê°€ 2ì§„ë³€ìˆ˜(binary variable)ì¸ ê²½ìš°ì˜ <strong>ì¡°ìž‘ë³€ìˆ˜ì¶”ì •ëŸ‰</strong> ê³¼ ê°™ê³ , <span class="math inline">\(\bf LATE\)</span>ëŠ” ì´ê²ƒìœ¼ë¡œ ì¶”ì •ê°€ëŠ¥í•˜ê²Œ ëœë‹¤.</p><p><br></p><hr><p><br></p><blockquote><p>[ì˜ˆì‹œ]<br><strong>ì½œë¡¬ë¹„ì•„ì—ì„œ ì´ë£¨ì–´ì§„ 'ë°”ìš°ì³ì œë„'ëŠ” í•™ì—…ì„±ì  í–¥ìƒì˜ íš¨ê³¼ê°€ ìžˆì—ˆì„ê¹Œ?</strong></p></blockquote><blockquote><p>[ìƒí™©ì„¤ëª…]<br>- ë°”ìš°ì³ì œë„ëž€, ì œë¹„ë½‘ê¸°ë¡œ ìž¥í•™ìƒì„ ì„ ì •í•´ì„œ ì‚¬ë¦½ ì¤‘í•™êµì˜ ìˆ˜ì—…ë£Œ ì ˆë°˜ì„ ë¶€ë‹´í•´ì£¼ëŠ” ì œë„ì´ë‹¤.<br>- ê·¸ëŸ¬ë‚˜ ë°”ìš°ì³ì œë„ ë§Œìœ¼ë¡œ ìˆ˜ì—…ë£Œë¥¼ ê°ë‹¹í•˜ê¸° íž˜ë“¤ì–´ ì œë¹„ë½‘ê¸°ì—ì„œ ì„ ë°œë˜ì–´ë„ ì‚¬ë¦½ì¤‘í•™êµì˜ ìž…í•™ì„ í¬ê¸°í•˜ëŠ” í•™ìƒë“¤ì´ ì¡´ìž¬í–ˆë‹¤.<br>- ë¶€ëª¨ë‹˜ë“¤ì˜ ê²½ì œì ëŠ¥ë ¥ì´ ì¢‹ê±°ë‚˜ ì‚¬ë¦½í•™êµë¥¼ ì„ í˜¸í•˜ëŠ” í•™ìƒë“¤ì€ ì œë¹„ë½‘ê¸°ì—ì„œ ë–¨ì–´ì ¸ë„ ì‚¬ë¦½í•™êµì— ìž…í•™í–ˆë‹¤.</p></blockquote><blockquote><p>[ê°€ì • (ì‹¤ì œ ë°ì´í„°ê°€ ì•„ë‹˜) ]<br>- 1,000ëª…ì˜ í•™ìƒì´ ì œë¹„ë¥¼ ë½‘ì•„ ë‹¹ì²¨ëœ í•™ìƒì€ 300ëª…ì´ì˜€ë‹¤.<br>- ì œë¹„ì— ë‹¹ì²¨ëœ 300ëª…ì˜ í•™ì—…ì„±ì ì˜ í‰ê· ì€ 80ì <br>- ì œë¹„ì— ë‹¹ì²¨ë˜ì§€ ì•Šì€ 700ëª…ì˜ í•™ì—…ì„±ì ì€ 60ì <br>- ì œë¹„ì— ë‹¹ì²¨ëì„ ë•Œ ì‚¬ë¦½í•™êµì— ì§„í•™í•™ í™•ë¥ ì€ 90%<br>- ì œë¹„ì— ë‹¹ì²¨ë˜ì§€ ì•Šì•˜ìŒì—ë„ ì‚¬ë¦½í•™êµì— ì§„í•™í•  í™•ë¥ ì€ 15%</p></blockquote><p><br></p><ul><li><span class="math inline">\(\begin{equation}z= \left \{\begin{array}{l}1ã€€(ì œë¹„ë½‘ê¸°ë‹¹ì²¨) \\0ã€€(ì œë¹„ë½‘ê¸°íƒˆë½)\end{array}\right.\end{equation}\)</span></li></ul><p><br></p><ul><li><span class="math inline">\(d = zd_1 + (1-z)d_0\)</span></li></ul><p><br></p><ul><li><span class="math inline">\(\begin{equation}d= \left \{\begin{array}{l}1ã€€(ì‚¬ë¦½í•™êµì§„í•™) \\0ã€€(ê³µë¦½í•™êµì§„í•™)\end{array}\right.\end{equation}\)</span></li></ul><p><br></p><ul><li><span class="math inline">\(y=dy_1 + (1-d)y_0\)</span> (ì„±ì )</li></ul><p><br></p><ul><li><p>ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ê¶ê¸ˆí•œ ê²ƒì€ <strong>[ë°”ìš°ì³ì œë„] <span class="math inline">\(\rightarrow\)</span> [ì‚¬ë¦½í•™êµì§„í•™] <span class="math inline">\(\rightarrow\)</span> [ì„±ì í–¥ìƒ]</strong> ì˜ ì¸ê³¼ìŠ¤í† ë¦¬(causal story)ë¥¼ ê°€ì§„ íš¨ê³¼ì´ë‹¤.</p></li><li><p>ê·¸ëŸ¬ë¯€ë¡œ ìš°ë¦¬ê°€ ê´€ì‹¬ìžˆëŠ” ì¼€ì´ìŠ¤ëŠ” <strong>ì œë¹„ë½‘ê¸°ì— ë¶™ìœ¼ë©´ ì‚¬ë¦½í•™êµì— ê°€ê³  ë–¨ì–´ì§€ë©´ ê³µë¦½í•™êµì— ì§„í•™í•  í•™ìƒ</strong> ì´ë‹¤.</p></li><li><p>ì¦‰, ì œë¹„ë½‘ê¸°ì— ë¶™ë˜ ì•ˆë¶™ë˜ ì‚¬ë¦½í•™êµì— ê°ˆ í•™ìƒ (<span class="math inline">\(d_1=1,d_0=1\)</span>)ì´ë‚˜ ë¶™ë˜ ì•ˆë¶™ë˜ ê³µë¦½í•™êµì— ê°ˆ í•™ìƒ (<span class="math inline">\(d_1=0,d_0=0\)</span>)ì€ ê³ ë ¤ì˜ ëŒ€ìƒì´ ì•„ë‹ˆë‹¤.</p></li></ul><p><br></p><ul><li><p>ë”°ë¼ì„œ, <span class="math inline">\(ATE = E(y_1-y_0)\)</span> ë¥¼ êµ¬í•˜ë©´ ë‹¨ìˆœížˆ ì‚¬ë¦½í•™êµì™€ ê³µë¦½í•™êµì˜ ì„±ì  ì°¨ì´ë¥¼ êµ¬í•˜ê²Œ ëœë‹¤.</p></li><li><p>ì´ ê²½ìš° <span class="math inline">\({\bf LATE} = E(y_1-y_0|d_1=1,d_0=0)\)</span> ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ë°”ëžŒì§í•  ê²ƒì´ë‹¤.</p></li></ul><p><br></p><ul><li><p><strong><span class="math inline">\({\bf LATE}\)</span>ì˜ ì¶”ì •</strong></p><ul><li><p><span class="math inline">\(E(y|z=1) = 80\)</span></p></li><li><p><span class="math inline">\(E(y|z=0) = 60\)</span></p></li><li><p><span class="math inline">\(E(d|z=1) = 0.9\)</span></p></li><li><p><span class="math inline">\(E(d|z=0) = 0.15\)</span></p></li><li><p><span class="math inline">\(\begin{align} LATE &amp;= \cfrac{E(y|z=1) - E(y|z=0)}{E(d|z=1)-E(d|z=0)}\\ &amp;\space \\ &amp;= \cfrac{80-60}{0.9-0.15} \fallingdotseq 26.6666 \end{align}\)</span></p></li><li><p>êµ­ì†Œì  í‰ê· íš¨ê³¼(LATE)ì˜ ê´€ì ì—ì„œ 26.66ì ì˜ ì„±ì í–¥ìƒíš¨ê³¼ê°€ ìžˆì—ˆë‹¤ê³  ì¶”ì •í•  ìˆ˜ ìžˆë‹¤.</p></li></ul></li></ul><p><br></p><h4><span id="ê²°ë¡ ì´ì™€-ê°™ì´-ì¤€ì‹¤í—˜ìœ¼ë¡œì¨ì˜-ì¡°ìž‘ë³€ìˆ˜ë²•ì€-ìƒë‹¹ì´-ì–´ë µì§€ë§Œ-bf-late-ì˜-ì•„ì´ë””ì–´ë¡œì¨ì˜-ì¡°ìž‘ë³€ìˆ˜ë²•ì€-ìƒê°í•´ë³¼ë§Œ-í•˜ë‹¤">ã€ê²°ë¡ ã€‘ì´ì™€ ê°™ì´ ì¤€ì‹¤í—˜ìœ¼ë¡œì¨ì˜ ì¡°ìž‘ë³€ìˆ˜ë²•ì€ ìƒë‹¹ì´ ì–´ë µì§€ë§Œ <span class="math inline">\(\bf LATE\)</span> ì˜ ì•„ì´ë””ì–´ë¡œì¨ì˜ ì¡°ìž‘ë³€ìˆ˜ë²•ì€ ìƒê°í•´ë³¼ë§Œ í•˜ë‹¤.</span></h4><p><br></p><hr><h2><span id="reference">* Reference</span></h2><p>í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” <a href="https://www.youtube.com/watch?v=u8hsTkLg2xc&amp;t=159s">ìœ íŠœë¸Œ ì±„ë„ã€Œãƒ‡ãƒ¼ã‚¿ã®ç§‘å­¦ã®ãƒ¡ã‚½ãƒ‰ãƒ­ã‚¸ãƒ¼ã€ì˜ å±±ç”°å…¸ä¸€ë‹˜ì˜ ê°•ì˜</a>ë¥¼ í‹€ë¡œ ë‚´ìš©ì„ ì •ë¦¬ &amp; ì¶”ê°€ í–ˆìŒì„ ë°íž™ë‹ˆë‹¤.</p><p>ê·¸ ì™¸ ì°¸ì¡°</p><p><a href="https://www.amazon.co.jp/dp/4000069721/ref=cm_sw_r_tw_dp_U_x_5LQxEbXZJDK4H">èª¿æŸ»è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆç§‘å­¦â€•å› æžœæŽ¨è«–ãƒ»é¸æŠžãƒã‚¤ã‚¢ã‚¹ãƒ»ãƒ‡ãƒ¼ã‚¿èžï¼ˆæ˜Ÿé‡Žå´‡å®ï¼‰</a></p><p><a href="http://www.ier.hit-u.ac.jp/~kitamura/lecture/Hit/08Statsys5.pdf">æœæŸ»å¤‰æ•°æ³•ï¼ˆä¸€æ©‹å¤§å­¦çµŒæ¸ˆç ”ç©¶æ‰€, åŒ—æ‘ è¡Œä¼¸ï¼‰</a></p><p><a href="https://healthpolicyhealthecon.com/2015/02/23/experiment-and-quasi-experiment-1/">æ“ä½œå¤‰æ•°æ³•Instrumental variable methodsã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚° (æ´¥å·å‹ä»‹)</a></p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/">Prerequisite</category>
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/Causal-Inference/">Causal Inference</category>
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/Causal-Inference/a-Overall/">a.Overall</category>
      
      
      <category domain="https://jaysung00.github.io/tags/Causal-Inference/">Causal Inference</category>
      
      <category domain="https://jaysung00.github.io/tags/Bayesian-Network/">Bayesian Network</category>
      
      
      <comments>https://jaysung00.github.io/2020/12/04/CN2/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ã€Causal Inferenceâ‘ ã€‘ì¸ê³¼ì¶”ë¡ ì˜ ëª©ì ê³¼ RCTì— ê´€í•˜ì—¬</title>
      <link>https://jaysung00.github.io/2020/11/30/CI1/</link>
      <guid>https://jaysung00.github.io/2020/11/30/CI1/</guid>
      <pubDate>Mon, 30 Nov 2020 02:39:29 GMT</pubDate>
      
      <description>&lt;hr /&gt;</description>
      
      
      
      <content:encoded><![CDATA[<hr><a id="more"></a><h1><span id="ì¸ê³¼ì¶”ë¡ causal-inferenceìœ¼ë¡œ-ë­˜-í• -ìˆ˜-ìžˆëŠ”ë°">* ì¸ê³¼ì¶”ë¡ (Causal Inference)ìœ¼ë¡œ ë­˜ í•  ìˆ˜ ìžˆëŠ”ë°?</span></h1><hr><blockquote><p>"A ì•„ì´ìŠ¤í¬ë¦¼ì„ ê³µì¤‘íŒŒ CFì— ë‚´ë³´ëƒˆì„ë•Œ, í•´ë‹¹ ì•„ì´ìŠ¤í¬ë¦¼ì˜ ë§¤ìƒì€ ì–¼ë§ˆë‚˜ ì˜¬ëžì„ê¹Œ?"</p></blockquote><blockquote><p>"ì „ ì‚¬ì› ëŒ€ìƒ Python ì—°ìˆ˜ í”„ë¡œê·¸ëž¨ì„ ì„¤ì¹˜ í–ˆì„ë•Œ, ì‚¬ì› ë“¤ì˜ ì¼ì˜ ëŠ¥ë¥ ì€ ì–¼ë§ˆë‚˜ ì˜¬ëžì„ê¹Œ?"</p></blockquote><p>ì´ì™€ ê°™ì€ ì§ˆë¬¸ë“¤ì€ ë¹„ì¦ˆë‹ˆìŠ¤ì—ì„œ ì¼ìƒì ìœ¼ë¡œ í”ížˆ ë‚˜ì˜¬ ìˆ˜ ìžˆëŠ” ì§ˆë¬¸ë“¤ì´ë‹¤.</p><p>í•˜ì§€ë§Œ ì´ì— ëŒ€í•´ <strong>ê¹Šì€ ê³ ì°° ì—†ì´ ë‹¨ìˆœí•˜ê²Œ íš¨ê³¼ë¥¼ ì •ì˜í•˜ê³  í‰ê°€í•¨</strong> ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” ìˆ˜ë§Žì€ <strong>ë°”ì´ì–´ìŠ¤</strong> ë¥¼ ë§Œë“¤ì–´ ë‚´ê³  ìžˆë‹¤.</p><p>ê³µì¤‘íŒŒ CFì˜ íš¨ê³¼ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë‹¨ìˆœížˆ CF ì „í›„ì˜ ë§¤ìƒì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•´ì„œ, CFì™€ ê´€ê³„ì—†ì´ ì‹œê¸°ì ìœ¼ë¡œ ë‚ ì”¨ê°€ ë”ì›Œì ¸ì„œ ì˜¤ë¥¸ ë§¥ì£¼ì˜ ë§¤ìƒê¹Œì§€ë„ <strong>CFì˜ íš¨ê³¼</strong>ë¡œì¨ í‰ê°€í•´ë²„ë¦°ë‹¤.</p><p>ë˜í•œ Python ì—°ìˆ˜ë¥¼ ì‹ ì²­í•œ ì‚¬ëžŒë“¤ì€ ê·¸ë ‡ì§€ ì•Šì€ ì‚¬ì›ë“¤ë³´ë‹¤ ì›ëž˜ë¶€í„° ìš°ìˆ˜í•œ ì‚¬ëžŒì´ ë§Žì„ìˆ˜ ìžˆë‹¤. ì›ëž˜ë¶€í„° ì¼ì˜ ëŠ¥ë¥ ì´ ë†’ì€ ì—°ìˆ˜ìžê·¸ë£¹ê³¼ ë¹„ì—°ìˆ˜ìž ê·¸ë£¹ì˜ ëŠ¥ë¥ ì„ ë‹¨ìˆœ ë¹„êµí•´ì„œ ì›ëž˜ì˜ ì°¨ì´ê¹Œì§€ë„ <strong>pythonì—°ìˆ˜ì˜ íš¨ê³¼</strong>ë¡œ í‰ê°€í•´ë²„ë¦°ë‹¤.</p><p>ì´ì²˜ëŸ¼ ì¸ê³¼ì¶”ë¡ ì˜ ëª©ì ë¥¼ ì² ì €í•˜ê²Œ ë¹„ì¦ˆë‹ˆìŠ¤ì  ê´€ì ì—ì„œ ë³´ìžë©´ <strong>ì–´ë– í•œ ì‹œì±…ì˜ ì •í™•í•œ íš¨ê³¼ì¸¡ì •</strong> ì„ ìœ„í•œ ì´ë¡  &amp; ê¸°ìˆ  ë¶„ì•¼ë¼ê³  í•  ìˆ˜ ìžˆë‹¤.</p><p><br></p><hr><h1><span id="inference-ì˜-ì‹ ë¢°ì„±ì˜-3ë‹¨ê³„">* Inference ì˜ ì‹ ë¢°ì„±ì˜ 3ë‹¨ê³„</span></h1><hr><h3><span id="level-1-ì‹¤í—˜experimental-ë ˆë²¨">Level 1. ì‹¤í—˜(Experimental) ë ˆë²¨</span></h3><ul><li><p><strong>RCT (Randomized Controlled Trial; ë¬´ìž‘ìœ„í™” ë¹„êµ ì‹¤í—˜)</strong></p></li><li><p>3ê°€ì§€ ê¸°ë³¸ìš”ê±´</p><p>(1). <strong>ë¹„êµ</strong> : <code>Control Group</code>ê³¼ <code>Treatment Group</code> ì˜ ë¹„êµë¥¼ í†µí•´ ë…ë¦½ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì³¤ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê³¼ì •</p><p>(2). <strong>ì¡°ìž‘</strong> : ì‹œê°„ì ìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ê°€ ë¨¼ì € ë°œìƒí•˜ê³  ê·¸ í›„ì— ë’¤ë”°ë¼ ì¢…ì†ë³€ìˆ˜ê°€ ë°œìƒí•¨ì„ ìž…ì¦í•˜ê¸° ìœ„í•´, ìž„ì˜ë¡œ ë…ë¦½ë³€ìˆ˜ë¥¼ ì˜ë„ì ì¸ ì‹œê¸°ì— ë°œìƒí•˜ë„ë¡í•˜ê³  ì´ì— ë’¤ë”°ë¥¸ ì¢…ì†ë³€ìˆ˜ì˜ ë³€í™”ë¥¼ ì¸¡ì •í•˜ë„ë¡ ì‹œê°„ì  ìˆœì„œë¥¼ ì¡°ìž‘í•˜ëŠ” ê²ƒ <em>(ì¸ê³¼ì„±ì˜ ì„ í›„ê´€ê³„)</em></p><p>(3). <strong>í†µì œ</strong> : í—ˆìœ„ì  ê´€ê³„ê°€ ì•„ë‹Œ ê²ƒì„ ìž…ì¦í•˜ê¸° ìœ„í•´, ë…ë¦½ë³€ìˆ˜ë¥¼ ì œì™¸í•œ ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìžˆëŠ” ì—¬ëŸ¬ ë³€ìˆ˜ë“¤ì´ ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•˜ë„ë¡ ìƒí™©ì„ ì˜ë„ì ìœ¼ë¡œ í†µì œí•˜ëŠ” ê²ƒ</p></li></ul><h3><span id="level-2-ì¤€ì‹¤í—˜quasi-experimental-ë ˆë²¨">Level 2. ì¤€ì‹¤í—˜(Quasi Experimental) ë ˆë²¨</span></h3><ul><li><p><code>Level 1</code>ì˜ ì‹¤í—˜ì„¤ê³„ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ëª…í™•ížˆ êµ¬ëª…í•  ìˆ˜ ìžˆì§€ë§Œ, ì¸ìœ„ì  í†µì œê°€ ì–´ë µê±°ë‚˜ ìœ¤ë¦¬ì  ë¬¸ì œë“±ìœ¼ë¡œ ì¸í•´ (<em>íŠ¹ížˆ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ê²½ìš° ì œí•œëœ ì˜ˆì‚° ë“±ì— ì˜í•´</em>) ì‹¤ì œ í™œìš©ì´ ë§¤ìš° ì–´ë µë‹¤. ì´ì— ë”°ë¼ ë¹„ë¡ ì‹¤í—˜ ì„¤ê³„ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•˜ì§€ë§Œ, ê·¸ ëŒ€ì•ˆì ì¸ ë°©ë²•ìœ¼ë¡œ í™œìš©ë˜ëŠ” ë°©ë²•ì´ë‹¤.</p></li><li><p>ëŒ€í‘œì ì¸ ë°©ë²•</p><p>(1). <strong>ì‹œê³„ì—´ ì„¤ê³„(time-series design)</strong> : ë¹„êµì§‘ë‹¨ì„ ë³„ë„ë¡œ ì„¤ì •í•˜ê¸° ê³¤ëž€í•œ ê²½ìš°ì— <em>í•˜ë‚˜ì˜ ì§‘ë‹¨</em> ì„ ì„ íƒí•´ì„œ, ë…ë¦½ë³€ìˆ˜ ë„ìž…ì˜ ì „í›„ìƒíƒœë¥¼ ë¹„êµí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì™¸ì ìš”ì¸ì— ëŒ€í•œ í†µì œê°€ ì–´ë µê¸° ë•Œë¬¸ì— (ê° ê¸°ê°„ë§ˆë‹¤ ì™¸ë¶€ì˜ ì˜í–¥ì´ ë‹¤ë¥´ë‹¤), ìœ„í—˜ì´ ìžˆì„ ìˆ˜ ìžˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ê°™ì€ ì¡°ì‚¬ë¥¼ ì—¬ëŸ¬ ì§‘ë‹¨ì—ì„œ ë˜í’€ì´í•˜ì—¬ ì‹¤ì‹œí•˜ì—¬ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìžˆëŠ”ì§€ í™•ì¸í•  í•„ìš”ê°€ ìžˆë‹¤.</p><p>(2). <strong>ë¹„ë™ì¼ í†µì œì§‘ë‹¨ ì„¤ê³„(nonequivalent control group design)</strong> : ë¹„ë™ì¼ í†µì œì§‘ë‹¨ ì„¤ê³„ëŠ” ì‹¤í—˜ì„¤ê³„ì˜ í†µì œì§‘ë‹¨ ì „í›„ë¹„êµì™€ ìœ ì‚¬í•˜ì§€ë§Œ <em>ë¹„êµì§‘ë‹¨ì„ ë¬´ìž‘ìœ„ë¡œ ì„ ì •í•˜ì§€ ì•ŠëŠ”ë‹¤</em> ëŠ” ì°¨ì´ê°€ ìžˆë‹¤. ë¹„ë™ì¼ í†µì œì§‘ë‹¨ ì„¤ê³„ëŠ” ë¬´ìž‘ìœ„ë°°ì¹˜ ì´ì™¸ì˜ ë°©ë²•(ë§¤ì¹­, ê¸°ì¡´ì§‘ë‹¨ì˜ ì„ ì • ë“±)ìœ¼ë¡œ <code>Control Group</code> ë° <code>Treatment Group</code>ì„ ì„ ì •í•œë‹¤.</p><p>ì´ì™¸ì—ë„ <strong>ë³€ìˆ˜ì¡°ìž‘ë²•(IV)</strong> , <strong>ì°¨ì˜ ì°¨ ë¶„ì„(DID)</strong> , <strong>ê²½í–¥ìŠ¤ì½”ì–´ ë§¤ì¹­(PS)</strong> , <strong>íšŒê·€ë¶ˆì—°ì† ë””ìžì¸(RDD)</strong> ë“±ì´ ìžˆë‹¤.</p></li></ul><h3><span id="level-3-ê´€ì°°observation-ë ˆë²¨">Level 3. ê´€ì°°(Observation) ë ˆë²¨</span></h3><ul><li><p><em>ë…ë¦½ë³€ìˆ˜ë¥¼ ì¡°ìž‘í•  ìˆ˜ ì—†ê³ , ì—°êµ¬ëŒ€ìƒì„ ë¬´ìž‘ìœ„í•  ìˆ˜ ì—†ëŠ” ê²½ìš°</em> ì´ë‹¤. ì–´ëŠ í•œ ì‹œì ì—ì„œ ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ëª¨ë‘ë¥¼ ì¸¡ì •í•´ì„œ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ”ë°ì— ê·¸ì¹œë‹¤.</p></li><li><p>ì„ í›„ê´€ê³„ê°€ íŒŒì•…ë˜ì§€ ì•Šì•˜ê³ , ë¬´ìž‘ìœ„í™”ë¥¼ í†µí•´ ë™ì¼í•œ ì§‘ë‹¨ì—ì„œ ë¹„êµí•˜ì§€ ëª»í–ˆìœ¼ë¯€ë¡œ ë¶€ì ì ˆí•œ í•´ì„ì„ í•˜ê²Œ ë  ìœ„í—˜ì„ ê°€ì§€ê³  ìžˆë‹¤.</p></li><li><p><strong>í™•ì¦íŽ¸í–¥</strong> <span class="math inline">\(^{[*1]}\)</span>(confirmation bias) ì´ë‚˜ <strong>ì‚¬í›„í•´ì„íŽ¸í–¥</strong> <span class="math inline">\(^{[*2]}\)</span>(hindsight bias)ì— ì˜í–¥ì„ ë°›ê¸° ì‰½ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‹œì±… ë‹´ë‹¹ìžê°€ ì¢‹ì€ ê²°ê³¼ë§Œì„ ë³´ê³  ì‹¶ë‹¤ê³  í•˜ë©´ ì§‘ê³„ì˜ ë°©ë²•ì„ ìœ ë¦¬í•˜ê²Œ ì„¤ì •í•´ì„œ ìœ ë¦¬í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” ê²ƒì´ ì–¼ë§ˆë“ ì§€ ê°€ëŠ¥í•˜ë¯€ë¡œ ì£¼ì˜ê°€ í•„ìš”í•˜ë‹¤.</p><ul><li><p><code>Level 3</code>ì€ <code>Level 1</code>&amp; <code>Level 2</code>ë¥¼ í•œ í›„ì— ì¶”ê°€ì ìœ¼ë¡œ ê²€í† í•˜ëŠ” ìš©ë„.</p></li><li><p>ë˜í•œ, ì§‘ê³„ì˜ ë°©ë²•ì„ ë¯¸ë¦¬ ì •í•´ë†“ëŠ” ê²ƒì„ í†µí•´, ìžì˜ì ìœ¼ë¡œ ë³€ê²½í•´ì„œ ìž…ë§›ì— ë§žëŠ” í•´ì„ì„ í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.</p></li></ul></li></ul><p><br></p><p>####ã€ì—¬ê¸°ì„œ ê¸°ì–µí•´ì•¼ í•  ê²ƒã€‘ Lv1 <span class="math inline">\(\rightarrow\)</span> Lv2 <span class="math inline">\(\rightarrow\)</span> Lv3 ì˜ ìˆœì„œë¡œ ì‹œì±…ì˜ íš¨ê³¼ë¥¼ ê²€í† í•´ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤!!</p><p><br></p><p>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*1]}:\)</span> ì›í•˜ëŠ” ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ëª¨ìœ¼ëŠ” ë“±ì˜ ê°€ì§€ê³  ìžˆëŠ” ì‹ ë…ì„ í™•ì¸í•˜ë ¤ëŠ” ê²½í–¥ì„±. </p><p>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*2]}:\)</span> ì–´ë–¤ ì‚¬ê±´ì´ ë°œìƒí•œ í›„, ì‚¬ì „ì— ê·¸ëŸ° ì¼ì´ ì¼ì–´ë‚  ê²ƒìœ¼ë¡œ ì˜ˆìƒí–ˆì—ˆë‹¤ëŠ” ì‹ìœ¼ë¡œ ë¬¸ì œë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒ. ì‹¤ì œë¡œëŠ” ë²Œì–´ì§„ ì‚¬ê±´ì— ëŒ€í•´ ì „í˜€ ëŒ€ë¹„ë¥¼ í•˜ì§€ ëª»í•˜ê³ , ê·¸ ì›ì¸ì„ ëƒ‰ì •í•˜ê²Œ ê·œëª…í•´ì•¼ í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  "ì¶©ë¶„ížˆ ì˜ˆì¸¡í–ˆë˜ ì¼"ì´ë¼ë©° ìžê¸° í™•ì‹ ì— ë¹ ì§€ëŠ” ê²ƒ.</p><hr><h1><span id="potential-outcome-framework">* Potential Outcome Framework</span></h1><hr><ul><li><p><strong>ì²˜ì¹˜(Treatment) í˜¹ì€ ê°œìž…(Intervention)ì´ ì´ë¤„ì¡ŒëŠ”ì§€ ì—¬ë¶€</strong></p><p><span class="math inline">\(\begin{equation}Z_i= \left \{\begin{array}{l}1ã€€(Treated) \\0ã€€(Untreated)\end{array}\right.\end{equation}\)</span><br><br></p></li><li><p><strong>ì¢…ì†ë³€ìˆ˜(DV; Dependent Variable) í˜¹ì€ ëª©ì ë³€ìˆ˜(Criterion Variable)</strong> ; ê°œìž…ì„ ë°›ì€ ê²½ìš°ì™€ ë°›ì§€ ì•Šì€ ê²½ìš° ë‘ê°€ì§€ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìžˆë‹¤.<br><em>(ì‹¤ì œë¡œëŠ” ì–´ëŠ í•œìª½ë§Œ ê´€ì°°ê°€ëŠ¥í•˜ì§€ë§Œ)</em></p><p><span class="math inline">\(\begin{equation}Y_i= \left \{\begin{array}{l}Y_i^{(1)}ã€€(Z_i = 1) \\Y_i^{(0)}ã€€(Z_i = 0)\end{array}\right.\end{equation}\)</span></p><p><span class="math inline">\(\Rightarrow Y_i = Y_i^{(0)}(1- Z_i) + Y_i^{(1)}Z_i\)</span></p></li></ul><p><br></p><ul><li><p>ì´ì™€ ê°™ì´, ìƒ˜í”Œ <span class="math inline">\(i\)</span> ì— ëŒ€í•˜ì—¬ ê°œìž…ì„ ë°›ì€ ê²½ìš°ì˜ ê²°ê³¼ <span class="math inline">\(Y^{(1)}\)</span> ì™€ ë°›ì§€ ì•Šì€ ê²½ìš°ì˜ ê²°ê³¼ <span class="math inline">\(Y^{(0)}\)</span> ê°„ì˜ ì°¨ì´ê°€ ê°œìž…ì˜ ì§„ì •í•œ <strong>ì²˜ì¹˜íš¨ê³¼(TE; Treatment Effect)</strong> ë¼ê³  ê°€ì •í•˜ëŠ” ê²ƒì„ <strong>Potential Outcome Framework</strong> ë¼ê³  í•œë‹¤.</p><p><span class="math inline">\(\bf \tau_{TE} = Y^{(1)}-Y^{(0)}\)</span></p></li></ul><p><br></p><ul><li><p>ëª¨ë“  ìƒ˜í”Œ <span class="math inline">\(i\)</span> ì— ëŒ€í•´ ê°ê°ì˜ ì²˜ì¹˜íš¨ê³¼ë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ ê¹Œë‹¤ë¡­ê¸° ë–„ë¬¸ì—, ê·¸ë£¹ê°„ì˜ ë¹„êµë¡œì¨ <strong>í‰ê· ì²˜ì¹˜íš¨ê³¼(ATE; Average Treatment Effect)</strong> ë¥¼ ë‹¤ë£¨ëŠ” ê²½ìš°ë„ ë§Žë‹¤.</p><p><span class="math inline">\(\bf \tau_{ATE}= E[Y^{(1)}]-E[Y^{(0)}]\)</span></p></li></ul><p><br></p><hr><h1><span id="level-1-ì‹¤í—˜ë ˆë²¨-ì¸ê³¼ì¶”ë¡ ì˜-ê¸°ì´ˆ-rct">* Level 1. ì‹¤í—˜ë ˆë²¨ ; ì¸ê³¼ì¶”ë¡ ì˜ ê¸°ì´ˆ, RCT</span></h1><hr><h3><span id="-rctì˜-íŠ¹ì§•">- RCTì˜ íŠ¹ì§•</span></h3><ul><li><p>ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ê´€ì ì—ì„œëŠ” <strong>ABí…ŒìŠ¤íŠ¸</strong> ë¼ê³  í•  ìˆ˜ ìžˆë‹¤.</p></li><li><p>RCT (Randomized Controlled Trial; ë¬´ìž‘ìœ„í™” ë¹„êµ ì‹¤í—˜)ë¥¼ í†µí•´ <code>Control Group</code>ê³¼ <code>Treatment Group</code>ì„ ë¬´ìž‘ìœ„í•˜ê²Œ ë‚˜ëˆ”ìœ¼ë¡œì¨ <strong>ë‘ ê·¸ë£¹ê°„ì˜ ë™ì§ˆì„±</strong> ì„ ê¸°ëŒ€í•  ìˆ˜ ìžˆë‹¤.</p></li><li><p>ì¸¡ì •ëœ êµëž€ì¸ìž(confounding factors)<span class="math inline">\(^{[*1]}\)</span>ëŠ” ë¬¼ë¡ , <strong>ì¸¡ì •ë˜ì§€ ì•Šì€ êµëž€ì¸ìž</strong> ì— ëŒ€í•´ì„œë„ ë¹„êµêµ°ê³¼ ëŒ€ì¡°êµ°ì˜ ê· í˜•ì„ ì´ë£¬ë‹¤.<br>(ì¸¡ì •ë˜ì§€ ì•Šì€ êµëž€ì¸ìž ê¹Œì§€ ì²˜ë¦¬í•  ìˆ˜ ìžˆëŠ” ì‹¤í—˜ë””ìžì¸ì€ <strong>RCT</strong>ì™€ ì™„ë²½í•˜ê²Œ ì„¤ê³„ëœ ì¡°ìž‘ë³€ìˆ˜ë²•(IV), ë¶„í• ì‹œê³„ì—´ë””ìžì¸(ITS) ë°–ì— ì¡´ìž¬í•˜ì§€ ì•ŠëŠ”ë‹¤.)</p></li><li><p>ê·¸ë¡œ ì¸í•´ ëª¨ë“  ì—°êµ¬ ë””ìžì¸ ì¤‘ ê°€ìž¥ ë†’ì€ ë‚´ì íƒ€ë‹¹ì„±<span class="math inline">\(^{[*2]}\)</span>ì„ ê¸°ëŒ€í•  ìˆ˜ ìžˆë‹¤.</p></li><li><p>ì¦‰ RCTì—ì„œëŠ” ì´ë¡ ìƒ, <span class="math inline">\(ATU = ATT = ATE\)</span>ì„ ê¸°ëŒ€í•  ìˆ˜ ìžˆë‹¤.</p><p>( <span class="math inline">\(ATU\)</span> <em>(Average Treatment Effect on the Untreated)</em> <span class="math inline">\(= E[Y^{(1)}|Z=0] - E[Y^{(0)}|Z=0]\)</span> )</p><p>( <span class="math inline">\(ATT\)</span> <em>(Average Treatment Effect on the Treated)</em> <span class="math inline">\(= E[Y^{(1)}|Z=1] - E[Y^{(0)}|Z=1]\)</span> )</p><p>( <span class="math inline">\(\bf ATE\)</span> <strong>(Average Treatment Effect)</strong> <span class="math inline">\(\bf = E[Y^{(1)}] - E[Y^{(0)}]\)</span> )</p></li></ul><p><br></p><p><img src="https://i.imgur.com/waDPtS0.png" width="600px"></p><p>[*] ìœ„ì˜ í‘œì—ì„œ <code>Control Group</code>ì˜ <span class="math inline">\(Y_i^{(1)}\)</span>ê³¼ <code>Treatment Group</code>ì˜ <span class="math inline">\(Y_i^{(0)}\)</span>ì€ ì‹¤ì œë¡œ ê´€ì°° ë¶ˆê°€ëŠ¥í•œ ë°˜ì‚¬ì‹¤ì  <strong>Potential Outcome</strong> ì´ë‹¤.</p><p><br></p><h3><span id="-rctì˜-ì˜ì˜">- RCTì˜ ì˜ì˜</span></h3><ul><li><p><strong>ì„ íƒë°”ì´ì–´ìŠ¤(Selection Bias)</strong> ì˜ ì œê±°</p></li><li><p>ì¡°ìž‘ë³€ìˆ˜ ì´ì™¸ì˜ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ í†µì œí•˜ì§€ ëª»í•œ ì±„ <code>Control Group</code>ê³¼ <code>Treatment Group</code> ì„ íƒí•˜ê²Œ ë˜ë©´, ê·¸ë£¹ê°„ì˜ ë™ì§ˆì„±ì„ í™•ë³´í•˜ì§€ ëª»í•˜ì—¬ <strong>êµëž€ë³€ìˆ˜(confounding factor)</strong> ì— ì˜í•´ íš¨ê³¼ê°€ ì™œê³¡ ë  ìˆ˜ìžˆë‹¤. ì´ëŸ¬í•œ ê²ƒì„ <strong>ì„ íƒ ë°”ì´ì–´ìŠ¤</strong> ë¼ê³  í•œë‹¤.</p></li><li><p>RCTëŠ” ì™„ì „ ë¬´ìž‘ìœ„ë¡œ ì²˜ì¹˜ê·¸ë£¹ì„ ì„ íƒí•˜ê¸° ë•Œë¬¸ì— <strong>ì„ íƒ ë°”ì´ì–´ìŠ¤</strong> ì—ì„œ ìžìœ ë¡œì›Œì§ˆ ìˆ˜ ìžˆë‹¤.</p></li></ul><p><br></p><h3><span id="-rctì˜-ì•½ì ">- RCTì˜ ì•½ì </span></h3><p>(1). ë¹„ìš©(ì˜ˆì‚°, ì‹œê°„ ë“±)ì´ ë§Žì´ ë“ ë‹¤.</p><p>(2). ì™¸ì íƒ€ë‹¹ì„±(ì¼ë°˜í™” ê°€ëŠ¥ì„±)<span class="math inline">\(^{[*3]}\)</span></p><ul><li>RCTì—ì„œëŠ” ë¹„ìš©ì˜ ë¬¸ì œë¡œ ì¸í•´ ì™¸ë¶€ì¡°ê±´ì„ í†µì œí•˜ê²Œ ë˜ê³  ê·¸ë¡œì¸í•´ ì™¸ì íƒ€ë‹¹ì„±ì€ ë‚®ì•„ì§ˆ ìˆ˜ ìžˆë‹¤.</li></ul><p>(3). noncompliance ë¬¸ì œ</p><ul><li>RCTì—ì„œ ë¬´ìž‘ìœ„ë¡œ ê·¸ë£¹ì„ ë°°ë¶„í•´ë„ ê±°ê¸°ì— ë”°ë¥´ì§€ ì•ŠëŠ” ì‚¬ëžŒì´ ìƒê²¨ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì œ</li></ul><p>(4). <em>(íŠ¹ížˆ ê¸°ì—…ì˜ ABí…ŒìŠ¤íŠ¸ì—ì„œ)</em> ë‹¤ë¥¸ RCTë¥¼ ê°™ì€ ëŒ€ìƒìžì— ê²¹ì³ì„œ ì‹¤í–‰í•˜ê²Œ ë  ê²½ìš°, ê·¸ì— ë”°ë¥¸ ë°”ì´ì–´ìŠ¤ê°€ ìƒê¸¸ ìˆ˜ ìžˆë‹¤.</p><ul><li>í†µê³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸°ê°€ ìƒë‹¹ížˆ ë³µìž¡í•´ì§„ë‹¤.</li></ul><p><br><br>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*1]}:\)</span> 'ì›ì¸'ê³¼ 'ê²°ê³¼' ì–‘ìª½ ëª¨ë‘ì—ê²Œ ê³µí†µì˜ ì›ì¸ì´ ë˜ëŠ” ìš”ì¸. Graphical Modelì—ì„œ ê³µí†µë¶€ëª¨, ë¶„ê¸°ë¡œ í‘œí˜„ë˜ëŠ” ë¶€ë¶„. ë‚´ìƒì„±(Endogeneity)ìœ¼ë¡œë„ í‘œí˜„í•œë‹¤. </p><p>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*2]}:\)</span> ë‹¤ë¥¸ ì™¸ìƒë³€ìˆ˜ë“¤ì´ ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šê³  ì§„ì •í•œ ë…ë¦½ë³€ìˆ˜ ì˜ íš¨ê³¼ì¸ê°€ì˜ íƒ€ë‹¹ì„±. </p><p>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*3]}:\)</span> ë‚´ì íƒ€ë‹¹ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì‹¤í—˜ì¡°ê±´ì„ ì—„ê²©ížˆ í†µì œí•œë‹¤ë©´ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì´ ë‚®ì•„ì§ˆ ìˆ˜ ìžˆë‹¤. ì–¼ë§ˆë‚˜ ì¼ë°˜ì  í˜„ì‹¤ì— í™•ìž¥ ê°€ëŠ¥í•œì§€ì˜ íƒ€ë‹¹ì„±. </p><hr><h2><span id="reference">* Reference</span></h2><p>í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” <a href="https://www.youtube.com/watch?v=u8hsTkLg2xc&amp;t=159s">ìœ íŠœë¸Œ ì±„ë„ã€Œãƒ‡ãƒ¼ã‚¿ã®ç§‘å­¦ã®ãƒ¡ã‚½ãƒ‰ãƒ­ã‚¸ãƒ¼ã€ì˜ å±±ç”°å…¸ä¸€ë‹˜ì˜ ê°•ì˜</a>ë¥¼ í‹€ë¡œ ë‚´ìš©ì„ ì •ë¦¬ &amp; ì¶”ê°€ í–ˆìŒì„ ë°íž™ë‹ˆë‹¤.</p><p>ê·¸ ì™¸ ì°¸ì¡°</p><p><a href="https://www.amazon.co.jp/dp/4297111179/ref=cm_sw_r_tw_dp_U_x_-LQxEbJ8JDZ1N">åŠ¹æžœæ¤œè¨¼å…¥é–€ã€œæ­£ã—ã„æ¯”è¼ƒã®ãŸã‚ã®å› æžœæŽ¨è«–/è¨ˆé‡çµŒæ¸ˆå­¦ã®åŸºç¤Ž ï¼ˆå®‰äº•ç¿”å¤ªï¼‰</a></p><p><a href="https://www.rieti.go.jp/jp/publications/dp/19j003.pdf">RCTã‚’ã‚ãã‚‹3ã¤ã®å•é¡Œã¨ãã®è§£æ³•ï¼ˆå±±å£ä¸€ç”·ï¼‰</a></p><p><a href="https://healthpolicyhealthecon.com/2015/02/23/experiment-and-quasi-experiment-1/">å®Ÿé¨“ï¼ˆExperimentï¼‰ã¨ç–‘ä¼¼å®Ÿé¨“ï¼ˆQuasi-experimentï¼‰ã«é–¢ã™ã‚‹è¨˜äº‹(æ´¥å·å‹ä»‹)</a></p><p><a href="http://blog.daum.net/sangrimza/15612241">http://blog.daum.net/sangrimza/15612241</a></p><p><a href="https://m.blog.naver.com/PostView.nhn?blogId=lucifer246&amp;logNo=201407281&amp;proxyReferer=https:%2F%2Fwww.google.com%2F">https://m.blog.naver.com/PostView.nhn?blogId=lucifer246&amp;logNo=201407281&amp;proxyReferer=https:%2F%2Fwww.google.com%2F</a></p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/">Prerequisite</category>
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/Causal-Inference/">Causal Inference</category>
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/Causal-Inference/a-Overall/">a.Overall</category>
      
      
      <category domain="https://jaysung00.github.io/tags/Causal-Inference/">Causal Inference</category>
      
      <category domain="https://jaysung00.github.io/tags/Bayesian-Network/">Bayesian Network</category>
      
      
      <comments>https://jaysung00.github.io/2020/11/30/CI1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ã€ ë„ëŒ€ì²´ ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ê°€ ë­ì•¼? â‘ ã€‘</title>
      <link>https://jaysung00.github.io/2020/11/14/BN1/</link>
      <guid>https://jaysung00.github.io/2020/11/14/BN1/</guid>
      <pubDate>Sat, 14 Nov 2020 14:15:12 GMT</pubDate>
      
      <description>&lt;hr /&gt;</description>
      
      
      
      <content:encoded><![CDATA[<hr><a id="more"></a><h1><span id="ë² ì´ì§€ì•ˆ-ë„¤íŠ¸ì›Œí¬bn-bayesian-network-ëž€">* ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬(BN; Bayesian Network) ëž€?</span></h1><hr><ul><li>í™•ë¥  ë³€ìˆ˜(RV; Random variables)ë“¤ ì‚¬ì´ì˜ ì¡°ê±´ë¶€ ë…ë¦½ ë“±ì˜ ê´€ê³„ë¥¼ ë³´ìž„ìœ¼ë¡œì¨, RVì˜ full joint distributionë“±ì„ ê°„ê²°í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìžˆëŠ” <strong>ê·¸ëž˜í”„ í‘œê¸°ë²• (Graphical Notation)</strong> ì´ë‹¤.</li></ul><p><br></p><ul><li><p>ì—¬ê¸°ì„œ <strong>ê·¸ëž˜í”„(Graph)</strong> ëž€, ìˆ˜í•™ì—ì„œ ì°¨íŠ¸(Chart)ì™€ ëŒ€ì¡°ë˜ì–´ ì •ì˜ëœ <code>node</code>ì™€ <code>edge</code>ì˜ ì§‘í•©</p><ul><li><p><code>edge</code>ê°€ ë°©í–¥ì´ ì§€ì •ë˜ì–´ ìžˆìœ¼ë©´ <code>directed</code>, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ <code>undirected</code></p></li><li><p>ê·¸ëž˜í”„ì˜ ëª¨ë“  <code>edge</code>ê°€ <code>directed</code>ì¼ ë•Œ <code>directed graph</code></p></li><li><p><code>directed edge</code>ì—ì„œ, ì‹œìž‘ë˜ëŠ” ìª½ì˜ ë…¸ë“œë¥¼ <code>parent node</code> ë¼ê³  í•˜ê³  ë°˜ëŒ€ìª½ì€ <code>child node</code>ë¼ê³  í•œë‹¤</p></li><li><p>ë³µìˆ˜ì˜ ì—°ê²°ëœ <code>directed edge</code>ì˜ ë°©í–¥ì´ ê°™ì€ ê²½ìš° ì´ë¥¼ <code>directed path</code>ë¼ê³  í•˜ê³ , <code>directed path</code>ì˜ ì²« ë²ˆì§¸ ë…¸ë“œëŠ” ê²½ë¡œìƒì˜ ëª¨ë“  ë…¸ë“œë“¤ì˜ <code>ancestor node</code>ì´ê³ , ë°˜ëŒ€ë¡œ ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ì€ ì²«ë²ˆì§¸ ë…¸ë“œì˜ <code>descendant node</code>ì´ë‹¤.</p></li><li><p><code>directed path</code>ì˜ ì‹œìž‘ì ê³¼ ëì ì´ ì¼ì¹˜í•  ê²½ìš° ì´ë¥¼ <code>cyclic</code>ì´ë¼ í•˜ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° <code>acyclic</code>ë¼ê³  í•œë‹¤.</p></li></ul></li></ul><p><br></p><ul><li><p><strong>ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬(BN)</strong> ì˜ <strong>Syntax</strong></p><ul><li><p><code>Network</code> ëŠ” <code>Node</code>ì™€ ì´ë“¤ì„ ì—°ê²°ì‹œí‚¤ëŠ” <code>Edge</code>ë¡œ êµ¬ì„±ëœë‹¤</p></li><li><p><code>ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ëž˜í”„(DAG; Directed Acyclic Graph)</code> ê°€ ë˜ì–´ì•¼ í•œë‹¤</p></li><li><p>ê°œë³„ <code>Node</code>ë“¤ì€ RVì¸ <span class="math inline">\(X\)</span>ì— ëŒ€í•´ <span class="math inline">\(\bf P(X | Paranets(X))\)</span>ë¥¼ ì˜ë¯¸í•œë‹¤.</p></li><li><p>ê°œë³„ <code>Edge</code>ë“¤ì€ ë¶€ëª¨ê°€ ìžì‹ì—ê²Œ ì£¼ëŠ” <strong>ì§ì ‘ì ì¸ ì˜í–¥(Direct Influence)</strong> ì„ ì˜ë¯¸í•œë‹¤.</p></li></ul></li></ul><p><br></p><hr><h1><span id="ë¨¼ì €-í™•ë¥ ì—-ëŒ€í•œ-ê°„ë‹¨í•œ-ë³µìŠµë¶€í„°">* ë¨¼ì € í™•ë¥ ì— ëŒ€í•œ ê°„ë‹¨í•œ ë³µìŠµë¶€í„°</span></h1><hr><ul><li><strong>ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬</strong> ë¼ëŠ” ê²ƒì€ ê²°êµ­ <em>í™•ë¥ ë³€ìˆ˜(RV) ê°„ì˜ ê´€ê³„</em> ë¥¼ í‘œí˜„í•œ ê²ƒì´ë‹¤.<br></li><li><strong>í™•ë¥ </strong> ì´ë¼ëŠ” ê²ƒì€ <em>ìƒëŒ€ì ì¸ ë¹ˆë„</em> ì´ë‹¤.<br><br></li></ul><blockquote><p>ë…ë¦½ì„± (Independence)</p></blockquote><ul><li><p><span class="math inline">\(P(A|B) = P(A)\)</span></p><p><span class="math inline">\(\Leftrightarrow P(A,B) = P(A)P(B)\)</span></p><p><span class="math inline">\(\Leftrightarrow P(B|A) = P(B)\)</span>; Aì™€ Bê°€ ë…ë¦½ì´ë©´, BëŠ” Aì™€ ë…ë¦½ì´ë‹¤.</p><ul><li><p>ì‚¬ê±´Bê°€ ë°œìƒí–ˆë‹¤ëŠ” ì •ë³´ëŠ” ì‚¬ê±´Aê°€ ë°œìƒí•  í™•ë¥ ì— ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ëª»í•œë‹¤.</p></li><li><p>ì´ëŠ”, ë°‘ì— ì„œìˆ í•˜ëŠ” Conditional Independence ì™€ ëŒ€ë¦½ë˜ëŠ” ì˜ë¯¸ë¡œ Marginal Independence ë¼ê³  í•  ìˆ˜ ìžˆë‹¤.</p></li></ul></li></ul><p><br></p><blockquote><p>ì¡°ê±´ë¶€ ë…ë¦½ (Conditional Independence)</p></blockquote><ul><li><p><span class="math inline">\(P(A|B,C) = P(A|C)\)</span></p><ul><li>ì‚¬ê±´Cê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë‘ ì‚¬ê±´ Aì™€ Bê°€ ë…ë¦½ì¸ ê²½ìš°, ì´ê²ƒì€ Cë¼ëŠ” ì¡°ê±´í•˜ì—ì„œ <em>ì¡°ê±´ë¶€ ë…ë¦½</em> ì´ë‹¤.</li></ul></li></ul><p><br></p><blockquote><p>ì¡°ê±´ë¶€ í™•ë¥  (Conditional Probability)</p></blockquote><ul><li><p><span class="math inline">\(P(A= true|B=true)\)</span></p><ul><li><p>"Probablity of A given B"</p></li><li><p>Bê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Aì˜ í™•ë¥ <br><br></p></li></ul></li></ul><blockquote><p>ê²°í•© í™•ë¥  (joint Probability)</p></blockquote><ul><li><p><span class="math inline">\(P(A= true, B=true)\)</span></p><ul><li><p>"the probability of A=true <strong>and</strong> B=true"</p></li><li><p>A=trueì™€ B=trueê°€ ë™ì‹œì— ë§Œì¡±í•  í™•ë¥ </p></li><li><p><strong>ì¡°ê±´ë¶€ í™•ë¥ ê³¼ ê²°í•© í™•ë¥ ì˜ ê´€ê³„</strong> ëŠ” ì¼ë°˜ì ìœ¼ë¡œ, <span class="math inline">\(P(X|Y) =\cfrac{P(X,Y)}{P(Y)}\)</span><br><br></p></li></ul></li></ul><blockquote><p>ì´ í™•ë¥  ë²•ì¹™ (Law of Total Probability)</p></blockquote><ul><li><p>"Summing out" or "Marginalization"</p></li><li><p><span class="math inline">\(P(A) = \sum_kP(A,B_k) = \sum_kP(A|B_k)P(B_k)\)</span></p><ul><li><p><span class="math inline">\(P(A) = \sum_kP(A,B_k)\)</span> ëŠ” <span class="math inline">\(B_1,B_2,...,B_n\)</span>ì´ ê°ê° ìƒí˜¸ë°°ë°˜ì ì¸ ì§‘í•©ì´ê³  ì´ë“¤ì˜ í•©ì§‘í•©ì´ ì „ì²´ì§‘í•©ì´ ë˜ë¯€ë¡œ ì„±ë¦½ (marginalize)</p></li><li><p><span class="math inline">\(\sum_kP(A,B_k) = \sum_kP(A|B_k)P(B_k)\)</span>ëŠ” ì¡°ê±´ë¶€í™•ë¥ ê³¼ ê²°í•©í™•ë¥ ì˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ë©´ ìœ ë„ê°€ëŠ¥<br><br></p></li></ul></li><li><p>ì´ë¡œ ì¸í•œ ì´ì ì€, <span class="math inline">\(P(A)\)</span>ë¥¼ ì§ì ‘ êµ¬í•˜ëŠ” ê²ƒë³´ë‹¤, <span class="math inline">\(P(A|B_k)\)</span>ì™€ ê°™ì€ ì¡°ê±´ë¶€í™•ë¥ ì„ êµ¬í•´ì„œ í•©ì¹˜ëŠ” ê²ƒì´ ì¼ë°˜ì ìœ¼ë¡œ ë” ìˆ˜ì›”í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.</p></li><li><p>í˜¹ì€ ê²°í•©í™•ë¥ ì„ ì•Œê³  ìžˆì„ ë•Œ, ì—¬ëŸ¬ê°€ì§€ í™•ë¥ ì„ ê³„ì‚° í•  ìˆ˜ ìžˆë‹¤.</p><ul><li><p>ì˜ˆë¥¼ë“¤ì–´, ê²°í•©í™•ë¥ ì¸ <span class="math inline">\(P(a,b,c,d)\)</span>ë¥¼ ì•Œê³  ìžˆì„ ë•Œ, <span class="math inline">\(P(c|b)\)</span>ëŠ” ì´ë ‡ê²Œ í‘œí˜„í•  ìˆ˜ ìžˆë‹¤</p></li><li><p><span class="math inline">\(P(c|b) = \sum_a \sum_d P(a,c,d|b) = \cfrac{1}{P(b)}\sum_a \sum_d {\bf P(a,b,c,d)}\)</span></p></li><li><p>ê·¸ëŸ¬ë‚˜ jointì˜ ê²½ìš°ì—ëŠ” parameterì˜ ìˆ˜ê°€ exponentialí•˜ê²Œ ëŠ˜ì–´ë‚˜ê²Œ ëœë‹¤! (Chain Ruleì˜ í•„ìš”ì„±)</p></li></ul></li></ul><p><br></p><blockquote><p>í™•ë¥ ì˜ ì—°ì‡„ë²•ì¹™ (Chain Rule for probability)</p></blockquote><ul><li><p>ëª¨ë“  joint distributionì— ëŒ€í•´, ê²°í•©í™•ë¥ ê³¼ ì¡°ê±´ë¶€í™•ë¥ ì˜ ê´€ê³„ì— ì˜í•´ ì–¸ì œë‚˜ ì´í•˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìžˆë‹¤.</p></li><li><p><span class="math inline">\(P(a,b,c,...,z) = P(a|b,c,...,z)P(b,c,....,z)\)</span></p></li><li><p>ì´ê²ƒì„ ë°˜ë³µì ìœ¼ë¡œ í•˜ë©´, <span class="math inline">\(P(a,b,c,...,z) = P(a|b,c,...,z)P(b|c,...,z)P(c|d,...,z)...P(z)\)</span>ë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. (Factorization)</p></li></ul><p><br></p><blockquote><p>ê³± ë¶„í•´ ë²•ì¹™ (Rule of product decomposition)</p></blockquote><ul><li><p>Bayesian Networkì—ì„œëŠ” ê·¸ëž˜í”„ì— ì†í•œ RVì˜ ê²°í•©ë¶„í¬(joint distribution)ëŠ” <code>family</code>ì˜ ëª¨ë“  ì¡°ê±´ë¶€ ë¶„í¬ <span class="math inline">\(P(Child|Parent)\)</span>ì˜ ê³±<span class="math inline">\(^{[*1]}\)</span>ìœ¼ë¡œ í‘œí˜„ í•  ìˆ˜ ìžˆë‹¤. <em>(ì‹œë¦¬ì¦ˆì˜ ë‹¤ìŒ í¬ìŠ¤íŠ¸ì˜ Factorization of Bayes Network ë‚´ìš© ì°¸ì¡°)</em></p></li><li><p><span class="math inline">\(P(x_1,x_2,...,x_n) = \prod _iP(x_i|Parents(x_i))\)</span></p><ul><li><p>ParentsëŠ” ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ì˜í–¥ì„ ë°›ëŠ” ë³€ìˆ˜ë§Œì„ ì˜ë¯¸!</p></li><li><p>ì˜ˆë¥¼ ë“¤ì–´, <span class="math inline">\(X\rightarrow Y \rightarrow Z\)</span> ì¸ ê·¸ëž˜í”„ì—ì„œ <span class="math inline">\(P(X=x, Y=y, Z=z)\)</span>ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ ìƒê°í•´ë³´ìž</p></li><li><p>ì›ëž˜ëŠ” ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•©ì˜ <span class="math inline">\((x, y, z)\)</span>ì— í•´ë‹¹í•˜ëŠ” í™•ë¥  í…Œì´ë¸”ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤</p></li><li><p>ê·¸ëŸ¬ë‚˜, ì´ ë²•ì¹™ì„ ì´ìš©í•˜ë©´ <span class="math inline">\(P(X=x, Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y)\)</span>ë¡œ ê°„ê²°í•˜ê²Œ í‘œí˜„ ê°€ëŠ¥</p></li><li><p>ì´ì²˜ëŸ¼ ê³ ì°¨ì›ì„ ì €ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ì–´ <em>ì°¨ì›ì˜ ì €ì£¼(curse of dimensionality)</em> ì—ì„œë„ ë¹„êµì  ìžìœ ë¡œì›Œ ì§ˆ ìˆ˜ ìžˆë‹¤.</p></li></ul></li></ul><p><br></p><p>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*1]}:\)</span> ì´ë ‡ê²Œ ì •ì˜ë˜ëŠ” ì›ëž˜ëŠ” ë’¤ì—ì„œ ê¸°ìˆ í•˜ëŠ” ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ì˜ Typical Local Structures Rulesì™€ ê´€ë ¨ ë˜ì–´ìžˆë‹¤. </p><hr><h1><span id="ë² ì´ì§€ì•ˆ-ë„¤íŠ¸ì›Œí¬ì˜-rules-of-typical-local-structures">* ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ì˜ Rules of Typical Local Structures</span></h1><hr><p><br></p><blockquote><p>Rule 1. ì‚¬ìŠ¬ í˜¹ì€ í­í¬í˜• (Chain or Cascading)</p></blockquote><p><img src="https://i.imgur.com/IF5m1WL.png"></p><ul><li><p>ë³€ìˆ˜<span class="math inline">\(X\)</span>ì™€ ë³€ìˆ˜<span class="math inline">\(Y\)</span>ì˜ ì‚¬ì´ì— í•˜ë‚˜ì˜ ë°©í–¥ì„± ê²½ë¡œ ë§Œ ìžˆê³  ë³€ìˆ˜<span class="math inline">\(Z\)</span>ê°€ í•´ë‹¹ ê²½ë¡œë¥¼ ê°€ë¡œë§‰ê³  ìžˆëŠ” ê²½ìš°, <strong><span class="math inline">\(Z\)</span>ê°€ ì¡°ê±´ë¶€ë¡œ ì£¼ì–´ì¡Œì„ë•Œ ë‘ ë³€ìˆ˜ <span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span>ëŠ” ì¡°ê±´ë¶€ ë…ë¦½</strong> ì´ë‹¤.</p></li><li><p><span class="math inline">\(X \perp Y|Z\)</span><br><span class="math inline">\(\Leftrightarrow P(Y|X,Z) = P(Y|Z)\)</span></p></li></ul><p><br></p><blockquote><p>Rule 2. ë¶„ê¸° í˜¹ì€ ê³µí†µë¶€ëª¨í˜• (Fork or Common parent)</p></blockquote><p><img src="https://i.imgur.com/mIdGQWD.png"></p><ul><li><p>ë³€ìˆ˜ <span class="math inline">\(Z\)</span>ê°€ <span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span>ì˜ ê³µí†µ ì›ì¸ì´ê³  <span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span>ì‚¬ì´ì— ë‹¨ í•˜ë‚˜ì˜ ê²½ë¡œê°€ ìžˆëŠ” ê²½ìš°, <strong><span class="math inline">\(Z\)</span>ì˜ ì¡°ê±´ì´ ì£¼ì–´ì¡Œì„ ë•Œ <span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span>ëŠ” ì¡°ê±´ë¶€ ë…ë¦½</strong> ì´ë‹¤.</p></li><li><p><span class="math inline">\(X \perp Y | Z\)</span><br><span class="math inline">\(\Leftrightarrow P(X,Y|Z) = P(X|Z)P(Y|Z)\)</span></p></li></ul><p><br></p><blockquote><p>Rule 3. ì¶©ëŒë¶€ í˜¹ì€ V-êµ¬ì¡° (Collider or V-structure)</p></blockquote><p><img src="https://i.imgur.com/9HLO4Ad.png"></p><ul><li><p>ë³€ìˆ˜ <span class="math inline">\(Z\)</span>ê°€ ë‘ ë³€ìˆ˜ <span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span> ì‚¬ì´ì˜ ì¶©ëŒ ë…¸ë“œì´ê³  <span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span> ì‚¬ì´ì— ë‹¨ <em>í•˜ë‚˜ì˜ ê²½ë¡œ</em> ë§Œ ìžˆì„ ê²½ìš°, <strong><span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span>ëŠ” ë¹„ì¡°ê±´ë¶€ ë…ë¦½(underconditionally independent)</strong> ì´ë‹¤. ê·¸ëŸ¬ë‚˜ <strong><span class="math inline">\(Z\)</span> ë˜ëŠ” <span class="math inline">\(Z\)</span>ì˜ <code>descendant</code>ì„ ì¡°ê±´ë¶€ë¡œ í•˜ì˜€ì„ ë•Œ <span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span>ëŠ” ì¢…ì†ì ì¼ ê°€ëŠ¥ì„±</strong> ì´ ìžˆë‹¤.</p></li><li><p><span class="math inline">\(\sim (X \perp Y|Z)\)</span><br><span class="math inline">\(\Leftrightarrow P(X,Y,Z)=P(X)P(Y)P(Z|X,Y)\)</span></p></li><li><p>ì¦‰ <span class="math inline">\(Z\)</span>ê°€ not given ì¼ ë•ŒëŠ” ë…ë¦½ì´ì§€ë§Œ, ë°˜ëŒ€ë¡œ <span class="math inline">\(Z\)</span>ê°€ givenìœ¼ë¡œ ì£¼ì–´ì§€ë©´ <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>ê°€ ì¢…ì†ì ì´ ë  ê°€ëŠ¥ì„±ì´ ìƒê²¨ë²„ë¦°ë‹¤.</p></li></ul><p><br></p><hr><h1><span id="bayes-ball-algorithm">* Bayes Ball Algorithm</span></h1><hr><ul><li><p>ëª©ì  ; <span class="math inline">\(X \perp Y | Z\)</span> (<span class="math inline">\(Z\)</span>ê°€ givenì¼ ë•Œ <span class="math inline">\(X\)</span>ì™€ <span class="math inline">\(Y\)</span>ê°€ ë…ë¦½) ì´ ì„±ë¦½í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒì •í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜</p></li><li><p><span class="math inline">\(X\)</span>ì—ì„œ ê³µì´ ì¶œë°œí•œë‹¤ê³  ê°€ì •í–ˆì„ ë•Œ <span class="math inline">\(Y\)</span>ê¹Œì§€ ê³µì´ ë„ë‹¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ë²•</p></li><li><p>ì—¬ê¸°ì„œ ê³µì€ <code>Information</code>ì„ ì˜ë¯¸í•˜ê³  í™”ì‚´í‘œëŠ” ê³µì˜ ì›€ì§ìž„ì„ ì˜ë¯¸í•œë‹¤. ë…¸ë“œ ê°„ì´ ì§ì ‘ì ì¸ edgeë¡œ ì—°ê²°ë˜ì–´ ìžˆì§€ ì•Šë”ë¼ë„ ê³µì´ êµ´ëŸ¬ê°€ì„œ ë„ë‹¬í•  ìˆ˜ ìžˆë‹¤ë©´ <code>Indirect influence</code>ê°€ ì¡´ìž¬í•˜ê¸°ë•Œë¬¸ì— ë‘ ë³€ìˆ˜ëŠ” <code>depedent</code>í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.</p></li></ul><p><br></p><blockquote><p>Rule 1ì˜ ê²½ìš°</p></blockquote><p>(1). <span class="math inline">\(Z\)</span>ê°€ givenì´ ì•„ë‹ ë•Œ, ê³µì€ ì§€ë‚˜ê°ˆ ìˆ˜ ìžˆë‹¤. (<span class="math inline">\(X, Y\)</span>ëŠ” ì¢…ì†)<br><img src="https://i.imgur.com/A5X39bt.png" width="298px"></p><p>(2). <span class="math inline">\(Z\)</span>ê°€ <strong>given</strong> ì¼ ë•Œ, ê³µì€ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤. (<span class="math inline">\(X \perp Y|Z\)</span>)<br><img src="https://i.imgur.com/k6dl20u.png" width="300px"></p><p><br></p><blockquote><p>Rule 2ì˜ ê²½ìš°</p></blockquote><p>(1). <span class="math inline">\(Z\)</span>ê°€ givenì´ ì•„ë‹ ë•Œ, ê³µì€ ì§€ë‚˜ê°ˆ ìˆ˜ ìžˆë‹¤. (<span class="math inline">\(X, Y\)</span>ëŠ” ì¢…ì†)<br><img src="https://i.imgur.com/8mPvc3A.png" width="300px"></p><p>(2). <span class="math inline">\(Z\)</span>ê°€ <strong>given</strong> ì¼ ë•Œ, ê³µì€ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤. (<span class="math inline">\(X \perp Y|Z\)</span>)</p><p><img src="https://i.imgur.com/hssut55.png" width="300px"></p><p><br></p><blockquote><p>Rule 3ì˜ ê²½ìš°</p></blockquote><p>(1). <span class="math inline">\(Z\)</span>ê°€ <strong>givenì´ ì•„ë‹ ë•Œ, ê³µì€ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤.</strong> (<span class="math inline">\(\bf X \perp Y\)</span>)<br><img src="https://i.imgur.com/yhO2p9I.png" width="300px"></p><p>(2). <span class="math inline">\(X_C\)</span>ê°€ <strong>given</strong> ì¼ ë•Œ, ë°˜ëŒ€ë¡œ pathê°€ ìƒê²¨ì„œ ê³µì´ ì§€ë‚˜ê°ˆ ìˆ˜ ìžˆê²Œ ëœë‹¤. (<span class="math inline">\(X, Y\)</span>ëŠ” <strong>ì¢…ì†</strong> <span class="math inline">\(|Z\)</span>)</p><p><img src="https://i.imgur.com/Y6SAkrl.png" width="300px"></p><p><br></p><blockquote><p>Bayes Ball Algorithm ì—°ìŠµ</p></blockquote><p><img src="https://i.imgur.com/7He2cq7.png" width="350px"></p><p><br></p><ul><li><p><strong>ë¬¸ì œ 1.</strong> <span class="math inline">\(X_1\perp X_4|X_2\)</span></p><p>ë‘ê°€ì§€ ê²½ë¡œë¡œ ê³µì„ êµ´ë¦´ ìˆ˜ ìžˆë‹¤.</p><p>(1). <span class="math inline">\(X_1 \rightarrow {\bf X_2}(given) \rightarrow X_4\)</span> ì˜ ê²½ë¡œëŠ” <span class="math inline">\(X_2\)</span>ê°€ ì‚¬ìŠ¬ì˜ givenìœ¼ë¡œ ë§‰í˜€ìžˆìœ¼ë¯€ë¡œ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤.</p><p>(2). <span class="math inline">\(X_1 \rightarrow X_3 \rightarrow X_5 \rightarrow X_6 \leftarrow {\bf X_2}(given) \rightarrow X_4\)</span> ì˜ ê²½ë¡œëŠ” <span class="math inline">\(X_6\)</span>ê°€ ì¶©ëŒë¶€ì˜ not givenìœ¼ë¡œ ë§‰í˜€ìžˆìœ¼ë¯€ë¡œ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤.</p><p>ë”°ë¼ì„œ ì–´ë– í•œ ê²½ë¡œë¡œë„ ë³¼ì€ ì§€ë‚˜ê°ˆìˆ˜ ì—†ìœ¼ë¯€ë¡œ <strong><span class="math inline">\(X_2\)</span>ê°€ givenì¼ ë•Œ <span class="math inline">\(X_1\)</span>ì™€ <span class="math inline">\(X_4\)</span>ëŠ” ë…ë¦½</strong> ì´ë‹¤.</p></li></ul><p><br></p><ul><li><p><strong>ë¬¸ì œ 2.</strong> <span class="math inline">\(X_2\perp X_5|X_1\)</span></p><p>ë‘ê°€ì§€ ê²½ë¡œë¡œ ê³µì„ êµ´ë¦´ ìˆ˜ ìžˆë‹¤.</p><p>(1). <span class="math inline">\(X_2 \rightarrow X_6 \leftarrow X_5\)</span> ì˜ ê²½ë¡œëŠ” <span class="math inline">\(X_6\)</span>ê°€ ì¶©ëŒë¶€ì˜ not givenìœ¼ë¡œ ë§‰í˜€ìžˆìœ¼ë¯€ë¡œ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤.</p><p>(2). <span class="math inline">\(X_2 \leftarrow {\bf X_1}(given) \rightarrow X_3 \rightarrow X_5\)</span> ì˜ ê²½ë¡œëŠ” <span class="math inline">\(X_1\)</span>ê°€ ë¶„ê¸°ì˜ givenìœ¼ë¡œ ë§‰í˜€ìžˆìœ¼ë¯€ë¡œ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤.</p><p>ë”°ë¼ì„œ ì–´ë– í•œ ê²½ë¡œë¡œë„ ë³¼ì€ ì§€ë‚˜ê°ˆìˆ˜ ì—†ìœ¼ë¯€ë¡œ <strong><span class="math inline">\(X_1\)</span>ê°€ givenì¼ ë•Œ <span class="math inline">\(X_2\)</span>ì™€ <span class="math inline">\(X_5\)</span>ëŠ” ë…ë¦½</strong> ì´ë‹¤.</p></li></ul><p><br></p><ul><li><p><strong>ë¬¸ì œ 3.</strong> <span class="math inline">\(X_1\perp X_6|\{X_2, X_3\}\)</span></p><p>ë‘ê°€ì§€ ê²½ë¡œë¡œ ê³µì„ êµ´ë¦´ ìˆ˜ ìžˆë‹¤.</p><p>(1). <span class="math inline">\(X_1 \rightarrow {\bf X_2}(given) \rightarrow X_6\)</span> ì˜ ê²½ë¡œëŠ” <span class="math inline">\(X_2\)</span>ê°€ ì‚¬ìŠ¬ì˜ givenìœ¼ë¡œ ë§‰í˜€ìžˆìœ¼ë¯€ë¡œ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤.</p><p>(2). <span class="math inline">\(X_1 \rightarrow {\bf X_3}(given) \rightarrow X_5 \rightarrow X_6\)</span> ì˜ ê²½ë¡œëŠ” <span class="math inline">\(X_3\)</span>ê°€ ì‚¬ìŠ¬ì˜ givenìœ¼ë¡œ ë§‰í˜€ìžˆìœ¼ë¯€ë¡œ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤.</p><p>ë”°ë¼ì„œ ì–´ë– í•œ ê²½ë¡œë¡œë„ ë³¼ì€ ì§€ë‚˜ê°ˆìˆ˜ ì—†ìœ¼ë¯€ë¡œ <strong><span class="math inline">\(\{X_2, X_3\}\)</span>ê°€ givenì¼ ë•Œ <span class="math inline">\(X_1\)</span>ì™€ <span class="math inline">\(X_6\)</span>ëŠ” ë…ë¦½</strong> ì´ë‹¤.</p></li></ul><p><br></p><ul><li><p><strong>ë¬¸ì œ 4.</strong> <span class="math inline">\(X_2\perp X_3|\{X_1, X_6\}\)</span></p><p>ë‘ê°€ì§€ ê²½ë¡œë¡œ ê³µì„ êµ´ë¦´ ìˆ˜ ìžˆë‹¤.</p><p>(1). <span class="math inline">\(X_2 \leftarrow {\bf X_1}(given) \rightarrow X_3\)</span> ì˜ ê²½ë¡œëŠ” <span class="math inline">\(X_1\)</span>ê°€ ë¶„ê¸°ì˜ givenìœ¼ë¡œ ë§‰í˜€ìžˆìœ¼ë¯€ë¡œ ì§€ë‚˜ê°ˆ ìˆ˜ ì—†ë‹¤.</p><p>(2). <span class="math inline">\(X_2 \rightarrow {\bf X_6}(given) \leftarrow X_5 \leftarrow X_3\)</span> ì˜ ê²½ë¡œëŠ” <span class="math inline">\(X_6\)</span>ê°€ ì¶©ëŒë¶€ì˜ givenìœ¼ë¡œ ëš«ë ¤ìžˆìœ¼ë¯€ë¡œ ì§€ë‚˜ê°ˆ ìˆ˜ ìžˆë‹¤.</p><p>ë”°ë¼ì„œ ë‘ë²ˆì§¸ ê²½ë¡œë¡œ ë³¼ì€ ì§€ë‚˜ê°ˆ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ <strong><span class="math inline">\(\{X_1, X_6\}\)</span>ê°€ givenì¼ ë•Œ <span class="math inline">\(X_2\)</span>ì™€ <span class="math inline">\(X_3\)</span>ëŠ” ë…ë¦½ì´ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤.</strong></p></li></ul><p><br></p><hr><h1><span id="d-seperationì˜-ì •ì˜">* <span class="math inline">\(d\)</span>-Seperationì˜ ì •ì˜</span></h1><hr><ul><li><p><span class="math inline">\(d\)</span>ëŠ” ë°©í–¥ì„±(directly)ì„ ì˜ë¯¸í•œë‹¤.</p></li><li><p>Bayesian Ball Algorithmìœ¼ë¡œ <span class="math inline">\(d\)</span>-Seperationì„ í™•ì¸í•  ìˆ˜ ìžˆë‹¤.</p></li><li><p>ì •ë¦¬í•˜ìžë©´, ê²½ë¡œpê°€ ì¡°ê±´ë¶€ì§‘í•© <span class="math inline">\(\{W\}\)</span>ì— ì˜í•´ <span class="math inline">\(d\)</span>-Seperateëœë‹¤ëŠ” ëª…ì œëŠ” ì´í•˜ì™€ í•„ìš”ì¶©ë¶„ì¡°ê±´ì´ë‹¤.</p><ol type="1"><li><p>ê²½ë¡œpëŠ” ì¡°ê±´ë¶€ì§‘í•© <span class="math inline">\(\{W\}\)</span>ì— ì†í•˜ëŠ” ì¤‘ê°„ë…¸ë“œ <span class="math inline">\(Z\)</span> ì˜ ì‚¬ìŠ¬ <span class="math inline">\(X \rightarrow Z \rightarrow Y\)</span> ë˜ëŠ” ë¶„ê¸° <span class="math inline">\(X \leftarrow Z \rightarrow Y\)</span> ë¥¼ í¬í•¨í•œë‹¤.</p></li><li><p>ê²½ë¡œpëŠ” ì¡°ê±´ë¶€ì§‘í•© <span class="math inline">\(\{W\}\)</span>ì— ì†í•˜ì§€ ì•ŠëŠ” ì¤‘ê°„ë…¸ë“œ <span class="math inline">\(Z&#39;\)</span> ì˜ ì¶©ëŒë¶€ <span class="math inline">\(X \rightarrow Z&#39; \leftarrow Y\)</span> ë¥¼ í¬í•¨í•œë‹¤.</p></li></ol></li></ul><p><br></p><hr><h2><span id="reference">* Reference</span></h2><p>í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” <a href="https://www.edwith.org/machinelearning2__17/joinLectures/9782">Edwithì— ê°œì„¤ëœ ë¬¸ì¼ì²  êµìˆ˜ë‹˜ì˜ ì¸ê³µì§€ëŠ¥ ë° ê¸°ê³„í•™ìŠµ ê°œë¡  II ê°•ì˜</a>ë¥¼ ì •ë¦¬ &amp; ì¶”ê°€í•œ ë‚´ìš©ìž„ì„ ë°íž™ë‹ˆë‹¤.</p><p>ì¶”ê°€ ë‚´ìš© ì°¸ì¡°</p><p><a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;mallGb=KOR&amp;barcode=9791125102236">ì˜í•™ ë° ì‚¬íšŒê³¼í•™ ì—°êµ¬ë¥¼ ìœ„í•œ í†µê³„ì  ì¸ê³¼ì¶”ë¡  ï¼ˆJudea Pearl, Madelyn Glymour, Nicholas P. Jewellï¼‰</a></p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/">Prerequisite</category>
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/Causal-Inference/">Causal Inference</category>
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/Causal-Inference/b-Bayesian-Network/">b.Bayesian Network</category>
      
      
      <category domain="https://jaysung00.github.io/tags/Causal-Inference/">Causal Inference</category>
      
      <category domain="https://jaysung00.github.io/tags/Bayesian-Network/">Bayesian Network</category>
      
      
      <comments>https://jaysung00.github.io/2020/11/14/BN1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ã€ ë„ëŒ€ì²´ ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ê°€ ë­ì•¼? â‘¡ã€‘</title>
      <link>https://jaysung00.github.io/2020/11/14/BN2/</link>
      <guid>https://jaysung00.github.io/2020/11/14/BN2/</guid>
      <pubDate>Sat, 14 Nov 2020 14:15:12 GMT</pubDate>
      
      <description>&lt;hr /&gt;</description>
      
      
      
      <content:encoded><![CDATA[<hr><a id="more"></a><h1><span id="factorization-of-bayes-network">* Factorization of Bayes Network</span></h1><hr><ul><li><p>ê·¸ëž˜í”„ì— ì†í•œ RVì˜ ê²°í•©ë¶„í¬(joint distribution)ëŠ” <code>family</code>ì˜ ëª¨ë“  ì¡°ê±´ë¶€ ë¶„í¬ <span class="math inline">\(P(Child|Parent)\)</span>ì˜ ê³±ìœ¼ë¡œ í‘œí˜„ í•  ìˆ˜ ìžˆë‹¤.</p></li><li><p><span class="math inline">\(P(X_1,X_2,...,X_n) = \prod _iP(X_i|Parents(X_i))\)</span></p></li><li><p><code>ê³± ë¶„í•´ ë²•ì¹™ (Rule of product decomposition)</code></p></li><li><p>í™•ë¥ ì˜ ì—°ì‡„ë²•ì¹™ <span class="math inline">\(P(a,b,c,...,z) = P(a|b,c,...,z)P(b,c,....,z)\)</span>ì—ì„œ ì‚¬ìŠ¬ê³¼ ë¶„ê¸°ì˜ Ruleì— ë”°ë¥´ë©´ ë¶€ëª¨ë…¸ë“œê°€ givenì´ë©´ ì´ìƒì€ ì¡°ìƒë…¸ë“œëŠ” ì „ë¶€ ë…ë¦½ì´ê²Œ ë˜ë¯€ë¡œ ì„±ë¦½. (ì¶©ëŒë¶€ëŠ” ë¶€ëª¨ë…¸ë“œê°€ ì•„ë‹ˆë‹¤.)</p></li><li><p>ì¦‰, Bayes Networkì˜ ì •ë³´ë¥¼ í†µí•´ joint distributionë¥¼ ê³„ì‚°í•  ë•Œ parameterì˜ ê°¯ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìžˆë‹¤.</p></li></ul><p><br></p><p><span class="math inline">\([ ì˜ˆì‹œ ]\)</span><br><img src="https://i.imgur.com/tl4t2Iz.png" width="350px"></p><p><br></p><ul><li><span class="math inline">\(P(X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8)\)</span>ë¥¼ êµ¬í•œë‹¤ê³  í•˜ìž.</li></ul><ol type="1"><li><p><strong>í™•ë¥ ì˜ ì—°ì‡„ë²•ì¹™ (Chain Rule for probability)</strong> ì— ì˜í•´ ì•„ë¬´ëŸ° Bayesian Networkì˜ ì •ë³´ê°€ ì—†ë‹¤ê³  í•˜ë”ë¼ë„ <span class="math inline">\(P(X_1,X_2,X_3,...,X_8) = P(X_1|X_2,X_3,...,X_8)P(X_2|X_3,...,X_8)P(X_3|X_4,...,X_8)...P(X_8)\)</span> ë¡œ Factorize í•  ìˆ˜ ìžˆë‹¤.</p></li><li><p><strong>ê³± ë¶„í•´ ë²•ì¹™ (Rule of product decomposition)</strong> ì— ì˜í•´ Bayesian Networkì˜ ì •ë³´ë¥¼ í™œìš©í•˜ë©´ <span class="math inline">\(P(X_1,X_2,X_3,...,X_8) = P(X_1)P(X_2)P(X_3|X_1)P(X_4|X_2)P(X_5|X_2)P(X_6|X_3,X_4)P(X_7|X_6)P(X_8|X_5,X_6)\)</span> ë¡œ í›¨ì”¬ ìž‘ì€ parameterë§Œìœ¼ë¡œ Factorize ê°€ëŠ¥í•˜ë‹¤.</p></li></ol><p><br></p><hr><h1><span id="plate-notation">* Plate Notation</span></h1><hr><blockquote><p><span class="math inline">\(\begin{align} P(D|\theta) &amp;= P(X_1,...,X_N|\mu,\sigma) \\ &amp;= \prod_i^N P(X_i|\mu,\sigma) \end{align}\)</span></p></blockquote><p><img src="https://i.imgur.com/QqfT9En.png" width="700px"></p><ul><li>ì´ì²˜ëŸ¼ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ RVë“¤ì— ëŒ€í•´ ìœ„ì™€ ê°™ì´ <strong>Plate Notation</strong> ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.</li></ul><p><br></p><hr><h1><span id="ë² ì´ì§€ì•ˆ-ë„¤íŠ¸ì›Œí¬ì—ì„œì˜-í™•ë¥ ì¶”ë¡ ">* ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ì—ì„œì˜ í™•ë¥ ì¶”ë¡ </span></h1><hr><ul><li><p>BNì— ìžˆëŠ” ëª¨ë“  random variables ;<br><span class="math inline">\(X = \{X1 ... X_N\}\)</span></p></li><li><p>ì£¼ì–´ì§„ ì¦ê±° ë³€ìˆ˜ (given evidence variables) ;<br><span class="math inline">\(X_V =\{X_{k+1}...X_N\}\)</span><br><span class="math inline">\(x_V\)</span>ëŠ” evidence values</p></li><li><p>ëª…ì‹œì ìœ¼ë¡œ ë‹¤ë£¨ì§€ëŠ” ì•Šì§€ë§Œ ê´€ê³„ê°€ ìžˆì–´ì„œ ê°ì•ˆí•  í•„ìš”ê°€ ìžˆëŠ” ë³€ìˆ˜ (hidden variables) ;<br><span class="math inline">\(X_H = X-X_V = \{X_1...X_k\}\)</span></p></li><li><p>hidden variables ; <span class="math inline">\(X_H = \{Y,Z\}\)</span></p><ul><li><span class="math inline">\(Y\)</span> : query variable (interested hidden variables)<br></li><li><span class="math inline">\(Z\)</span> : uninterested hidden variables</li></ul></li></ul><p><br></p><h3><span id="1-1-ì£¼ë³€í™•ë¥ -marginal-probability">1-1 ì£¼ë³€í™•ë¥  (Marginal Probability)</span></h3><blockquote><p>ì¦ê±° ë³€ìˆ˜ <span class="math inline">\(X_V\)</span> ì˜ <strong>ì£¼ë³€í™•ë¥  (Marginal Probability)</strong> <span class="math inline">\(P(x_V)\)</span> ëŠ”?</p></blockquote><p>Â Â Â Â Â Â <span class="math inline">\(\begin{align} P(x_V) &amp;=\sum_{X_H}P(X)=\sum_{X_H}P(X_H,X_V) \space\space\space\space\space\space\dots(1)\\ &amp;= \sum_{x_1}...\sum_{x_k}P(x_1...x_k,x_V)\space\space\space\space\space\space\space\space\space\dots(2) \end{align}\)</span></p><ul><li><p>(1). ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´ <strong>full joint</strong> ëœ ê²ƒì„ <span class="math inline">\(X_H\)</span>ë¡œ marginalize outí•œ ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤.</p></li><li><p>(2). ê°ê°ì˜ Hidden variableì— ëŒ€í•´ marginalize outí•œ ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤.</p></li></ul><p><br></p><h3><span id="1-2-ì¡°ê±´ë¶€-í™•ë¥ -conditional-probability">1-2. ì¡°ê±´ë¶€ í™•ë¥  (Conditional Probability)</span></h3><blockquote><p>ì£¼ì–´ì§„ ì¦ê±°(evidence)ì˜ ì§‘í•©<span class="math inline">\(x_V\)</span>ì´ ìžˆì„ë•Œ, <strong>query variable(ì£¼ì–´ì§€ì§€ ì•Šì•˜ì§€ë§Œ ê´€ì‹¬ìžˆëŠ” ë³€ìˆ˜)</strong> ì˜ <strong>ì¡°ê±´ë¶€ í™•ë¥ </strong> <span class="math inline">\(P(Y|x_V)\)</span>ì€?</p></blockquote><p>Â Â Â Â Â Â <span class="math inline">\(\begin{align} P(Y|X_V) &amp;= \sum_ZP(Y,Z = z|x_V) \space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\dots(1)\\ &amp;= \sum_Z\cfrac{P(Y,Z,x_V)}{P(x_V)} = \sum_Z\alpha P(X) \space\space\space\space\space\space\space\space\space\space\dots(2)\\ &amp;= \sum_Z \cfrac{P(Y,Z,x_V)}{\sum_{y,z}P(Y=y, Z=z, x_V)}\space\space\space\space\space\space\space\space\space\dots(3) \end{align}\)</span><br><br></p><ul><li><p>(1). <span class="math inline">\(Z\)</span>ë¥¼ jointë¡œ ë„£ì–´ì£¼ë©´ì„œ <span class="math inline">\(Z\)</span>ì— ëŒ€í•´ marginalize out í•œë‹¤.</p></li><li><p>(2). ì¡°ê±´ë¶€ í™•ë¥ ì˜ ì •ì˜ë¥¼ ì´ìš©í•´, <span class="math inline">\(x_V\)</span>ë¥¼ í¬í•¨í•œ <strong>full joint</strong> ë¥¼ <span class="math inline">\(P(x_V)\)</span> (Marginal Probability)ë¡œ ë‚˜ëˆˆë‹¤.<br>(<span class="math inline">\(\cfrac{1}{P(x_V)} = \alpha\)</span>ë¼ëŠ” ì •ê·œí™” ìƒìˆ˜(normalization constant)ì˜ ê³±ìœ¼ë¡œ ìƒê°í• ìˆ˜ë„ ìžˆë‹¤.)</p></li><li><p>(3). ë¶„ëª¨ì˜ ì£¼ë³€í™•ë¥ ì€ Inference Question1ì²˜ëŸ¼ <strong>full joint</strong> ë¥¼ ëª¨ë“  Hidden variableì— ëŒ€í•´ marginalize í•´ì„œ êµ¬í•  ìˆ˜ ìžˆë‹¤.</p></li></ul><p><br></p><hr><h1><span id="ë³€ìˆ˜ì œê±°-ì•Œê³ ë¦¬ì¦˜-variable-eliminatation-algorithm">* ë³€ìˆ˜ì œê±° ì•Œê³ ë¦¬ì¦˜ (Variable Eliminatation Algorithm)</span></h1><hr><p><img src="https://i.imgur.com/suXGsdJ.png" width="750px"></p><blockquote><p>ìœ„ì™€ ê°™ì´ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ ë³€ìˆ˜ì œê±° ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ <span class="math inline">\(P(J=j)\)</span>ë¥¼ êµ¬í•´ë³´ìž</p></blockquote><h3><span id="step1">* Step1</span></h3><ul><li>ìœ„ì˜ ì¤€ë¹„ë¥¼ í†µí•´ ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ ìƒì—ì„œ ê´€ì‹¬ìžˆëŠ” í™•ë¥ ì˜ ì¶”ë¡ ì„ ìœ„í•´ì„œ, <strong>full joint</strong> ë¥¼ êµ¬í•˜ê³  uninterested hidden variableì— ëŒ€í•´ <strong>Marginalize</strong> í•œë‹¤.</li></ul><p><br></p><ul><li><span class="math inline">\(\sum_{A,E,B,M} P(J= j,A,E,B,M)\)</span></li></ul><p><br></p><h3><span id="step2">* Step2</span></h3><ul><li><p><strong>full joint</strong> ë¥¼ Bayesina Networkì˜ ì •ë³´ë¥¼ ì´ìš©í•´ <em>ê³±ë¶„í•´ ë²•ì¹™</em>ìœ¼ë¡œ ë°”ê¿” ì“´ë‹¤.</p></li><li><p>ë¶„í•´í•œ ê³±ì˜ ë‚˜ì—´ìˆœì„œëŠ” <strong>topological order</strong> <span class="math inline">\(^{[*1]}\)</span> ë¥¼ ë”°ë¥¸ë‹¤.</p></li><li><p><strong>topological order</strong> ; B, E, A, J, M</p></li></ul><p><br></p><ul><li><span class="math inline">\(\sum_{B,E,A,M} P(B)P(E)P(A|B,E)P(J=j|A)P(M|A)\)</span></li></ul><p><br></p><p>&lt;span style="font-size: 85%;&gt; <span class="math inline">\(^{[*1]}:\)</span> ë“¤ì–´ì˜¤ëŠ” í™”ì‚´í‘œê°€ ì—†ëŠ” ë…¸ë“œ ë¶€í„° í•˜ë‚˜ì”© ì„ íƒí•˜ë©° ì§€ìš°ëŠ” ê²ƒì„ ë°˜ë³µí• ë•Œ ê²°ì •ë˜ëŠ” ìˆœì„œ </p><h3><span id="step3">* Step3</span></h3><ul><li><p>ìˆœì„œë¥¼ ìœ ì§€í•œ ì±„ ê° <span class="math inline">\(\sum\)</span>ê°€ ê´€ë ¨ì—†ëŠ” ê²ƒì„ ë°–ìœ¼ë¡œ ë¹¼ë‚¸ë‹¤.</p></li><li><p>ì œê±°í•  ë³€ìˆ˜ì˜ ìˆœì„œëŠ” ë’¤ì—ì„œë¶€í„° ì •í•´ì§„ë‹¤.</p></li></ul><p><br></p><ul><li><span class="math inline">\(\space\space\space\sum_B P(B)\sum_EP(E)\sum_AP(A|B,E)P(J=j|A)\sum_MP(M|A)\)</span></li></ul><p><br></p><h3><span id="step4">* Step4</span></h3><ul><li>ë’¤ì—ì„œ ë¶€í„° <strong>function notation</strong>ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ì„œ ë³€ìˆ˜ë¥¼ ì§€ì›Œë‚˜ê°„ë‹¤.</li></ul><p><br></p><ol type="1"><li><span class="math inline">\(\space\space\space\sum_B P(B)\sum_EP(E)\sum_AP(A|B,E)P(J=j|A) \underline {\sum_MP(M|A)}\)</span></li></ol><ul><li><p><span class="math inline">\(=\sum_B P(B)\sum_EP(E)\sum_AP(A|B,E)P(J=j|A) \underline {\bf f_1(A)}\)</span></p><ul><li>ë°‘ì¤„ì¹œ ë¶€ë¶„ì€ Jì™€ <span class="math inline">\(d\)</span>-seperateì´ê¸° ë•Œë¬¸ì— ê³ ë ¤í•  í•„ìš”ê°€ ì—†ë‹¤. ì¦‰, Aì˜ ê°’ê³¼ ìƒê´€ì—†ì´ <span class="math inline">\(f_1(A)\)</span>ëŠ” 1ì„ ê°–ëŠ”ë‹¤.</li></ul></li></ul><p><img src="https://i.imgur.com/EnN5hP3.png" width="220px"></p><p><br></p><ol start="2" type="1"><li><span class="math inline">\(=\sum_B P(B)\sum_EP(E)\underline{\sum_AP(A|B,E)P(J=j|A)}\)</span></li></ol><ul><li><p><span class="math inline">\(=\sum_B P(B)\sum_EP(E)\underline {\bf f_2(E,B)}\)</span></p><ul><li><span class="math inline">\(f_2(E,B)\)</span>ëŠ” ì´í•˜ì™€ ê°™ë‹¤.</li></ul></li></ul><p><img src="https://i.imgur.com/BOhvHDZ.png" width="750px"></p><p><br></p><ol start="3" type="1"><li><span class="math inline">\(=\sum_B P(B)\underline {\sum_EP(E)f_2(B,E)}\)</span></li></ol><ul><li><p><span class="math inline">\(=\sum_B P(B) \underline {\bf f_3(B)}\)</span></p><ul><li><span class="math inline">\(f_3(B)\)</span>ëŠ” ì´í•˜ì™€ ê°™ë‹¤.</li></ul></li></ul><p><img src="https://i.imgur.com/A3u5hM9.png" width="750px"></p><p><br></p><ol start="4" type="1"><li><span class="math inline">\(= \sum_BP(B)f_3(B)\)</span></li></ol><ul><li><p><span class="math inline">\(= P(B=b)f_3(B=b) + P(B= \sim b)f_3(B=\sim b)\)</span></p></li><li><p><span class="math inline">\(=0.001 * 0.849017 + 0.999 * 0.0513413 \fallingdotseq 0.052139\)</span></p></li><li><p>ë”°ë¼ì„œ, <span class="math inline">\(P(J=j) = 0.052139\)</span> ê°€ ëœë‹¤.</p></li></ul><p><br></p><hr><h2><span id="reference">* Reference</span></h2><p>í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” <a href="https://www.edwith.org/machinelearning2__17/joinLectures/9782">Edwithì— ê°œì„¤ëœ ë¬¸ì¼ì²  êµìˆ˜ë‹˜ì˜ ì¸ê³µì§€ëŠ¥ ë° ê¸°ê³„í•™ìŠµ ê°œë¡  II ê°•ì˜</a>ë¥¼ ì •ë¦¬ &amp; ì¶”ê°€í•œ ë‚´ìš©ìž„ì„ ë°íž™ë‹ˆë‹¤.</p><p>ì¶”ê°€ ë‚´ìš© ì°¸ì¡°</p><p><a href="https://www.youtube.com/watch?v=TZnEJ4wvLPY">https://www.youtube.com/watch?v=TZnEJ4wvLPY</a></p><p><a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;mallGb=KOR&amp;barcode=9791125102236">ì˜í•™ ë° ì‚¬íšŒê³¼í•™ ì—°êµ¬ë¥¼ ìœ„í•œ í†µê³„ì  ì¸ê³¼ì¶”ë¡  ï¼ˆJudea Pearl, Madelyn Glymour, Nicholas P. Jewellï¼‰</a></p>]]></content:encoded>
      
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/">Prerequisite</category>
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/Causal-Inference/">Causal Inference</category>
      
      <category domain="https://jaysung00.github.io/categories/Prerequisite/Causal-Inference/b-Bayesian-Network/">b.Bayesian Network</category>
      
      
      <category domain="https://jaysung00.github.io/tags/Causal-Inference/">Causal Inference</category>
      
      <category domain="https://jaysung00.github.io/tags/Bayesian-Network/">Bayesian Network</category>
      
      
      <comments>https://jaysung00.github.io/2020/11/14/BN2/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
